U I M A
From Extraction to Reasoning
Chris Welty
IBM Research
U I M A Acknowledgements
•Richard Fikes
(Stanford)
•Selene Makarios
(Stanford)
•Rob McCool 
(Stanford)•IBM Unstructured 
Information 
Management 
Architecture (UIMA) 
Project
•David Ferrucci (IBM)
•Bill Murdock (IBM)
•Mary Neff (IBM)
U I M A Why extraction?
Who was in Paris on 
Feb. 27, 2003?Person Place
Date
U I M A Why reasoning?
Former Iraqi Ambassa dor to Canada  
Hisham Al Shawi, who def ected to the UK in
August 1993, w as a key figur e in launching Iraq 's nuclear 
program. F rom 197 2 to
1974, Al Shawi was chairman of Iraq's Atomic Energy 
Commission and w as Iraq's rep resentative on the boa rd of the 
IAEA. Al Shawi helped co ordinate Iraq's
efforts to train Iraqi students and send them a broad to 
universities and intern ational rese arch facilit ies. Howev er, there 
is no evidence that Al Shawi knew of effo rts to use th ese 
trainees in a milit ary nuclear programme.
During the week of Feb. 26, 2003,  Al Shawitravelledto France 
and proceeded up the Sein e by bo at.  He spent three days in 
the latin quarter, and retu rned on Mar. 1 .
The pu rpose of his trip was not k nown.
Al Shawi and a fellow defector, former ambassador to Tunisia 
Hamid al Jabbouri, denied having  any detail ed knowledge of 
Iraq's nuclear program or  its procurement n etwork.Who was in Paris on 
Feb. 27, 2003?
U I M A Why reasoning?
Former Iraqi Ambassa dor to Canada  
Hisham Al Shawi , who def ected to th e UK in
August 19 93, was a key figure in launching Iraq 's 
nuclear program. From 19 72 to
1974, Al Shawi was chairman of Iraq's Atomic Energy 
Commission and w as Iraq's rep resentative on the boa rd 
of the IAEA. Al Shawi helped coo rdinate Iraq's
efforts to train Iraqi students and send them a broad to 
universities and intern ational research facilit ies. 
Howev er, there is no evid ence th at Al Shaw iknew of 
efforts to use th ese train ees in a military nu clear 
programme.
During the week of Feb. 26, 2003, Al 
Shawi travelle dto France and 
proceeded up the Seine by boat.  He 
spent three days in the latin quarter, 
and returned on Mar. 1.
The pu rpose of his trip was not k nown.
Al Shawi and a fellow defector, former ambassador to  
Tunisia Ha midal Jabbouri, denied hav ing any detailed 
knowledge of Iraq's nuclear program or its procurement 
network.Who was in Paris on 
Feb. 27, 2003?
U I M A What is needed?
Former Iraqi Ambassa dor to Canada  
Hisham Al Shawi , who def ected to th e UK in
August 19 93, was a key figure in launching Iraq 's 
nuclear program. From 19 72 to
1974, Al Shawi was chairman of Iraq's Atomic Energy 
Commission and w as Iraq's rep resentative on the boa rd 
of the IAEA. Al Shawi helped coo rdinate Iraq's
efforts to train Iraqi students and send them a broad to 
universities and intern ational research facilit ies. 
Howev er, there is no evid ence th at Al Shaw iknew of 
efforts to use th ese train ees in a military nu clear 
programme.
During the week of Feb. 26, 2003, Al 
Shawi travelle dto France and 
proceeded up the Seine by boat.  He 
spent three days in the latin quarter, 
and returned on Mar. 1.
The pu rpose of his trip was not known.
Al Shawi and a fellow defector, former ambassador to  
Tunisia Ha midal Jabbouri, denied hav ing any detailed 
knowledge of Iraq's nuclear program or its procurement 
network.•Reasoning
–Temporal containment
–Spatial containment
•Knowledge
–Geography
–Basic Ontology
•Coreference
–Multiple mentions of same 
entities, relations
–Find time that something 
happened
•Extraction
–Recognize people, places
–Recognize times, relations
U I M A Real World Reasoning?
•The acquisition bottleneck
•Reasoning is hard
–Complexity
NP < Exp < Non-Elementary < Undecidable
–More than 1,000,000 RDF  triples
–Can’t hold results of 1000 documents in memory
•Reasoning is sound
–Not tolerant of errors
<Person>Bush International</Person>
–Not just precis ion
Iraqi press agency says <event>the war is ended</event>
•Reasoning can be inscrutable
–If I have no friends then all my friends are doctorsHow much 
reasoning when
Bounded 
reasoning
ExplanationText Analysis
U I M A General Problem
•Given
–an ontology in OWL
–A background knowledge base in RDF
–Inference procedures
–A collection of existing IE components
•Use the results of IE to populate the KB
–Map IE semantics to KB semantics
–Map extracted entities to possibly existing KB 
instances 
U I M A
Simple Toke n and 
Sentence Annotator
Annotates tokens and 
sentences.
EAnnotator
IBM EAnnotator -A 
statistical ent ity, 
relation, and event 
annotat or.Cross Annotator 
Coreference
Resolves corref erence
disputes across 
annotat orsXsgParsing
Annotator
Performs deep parse 
using slot grammar 
parsers
KS Relation Annotator
Knowledge Structures 
Relation Annotator (pattern-
based relat ion detector)Simple Phone Call Relation 
Annotator
Finds mentions of phone 
calls in text and annotates 
them as MadePhoneCallTo
relations.Simple Phone Number 
Annotator
Finds phone numbers in textStarting Point
Ace Annotator
A stat istical ent ity and 
relation annotator that 
performs w ithin-
document coreference
resolut ion.KS Relation Detector 
Aggre gate
Aggregate that  includes 
KS Relation Detector 
and TAE's that provide 
its inputs
Phrase Finder Annotator
Creates  annotations for 
phrases to be passed t o ESG.  
Phrase sources are Wo rdNet 
and pre-annotated Resporator
phrases.
U I M A
Simple Toke n and 
Sentence Annotator
Annotates tokens and 
sentences.
EAnnotator
IBM EAnnotator -A 
statistical ent ity, 
relation, and event 
annotat or.Cross Annotator 
Coreference
Resolves corref erence
disputes across 
annotat orsXsgParsing
Annotator
Performs deep parse 
using slot grammar 
parsers
KS Relation Annotator
Knowledge Structures 
Relation Annotator (pattern-
based relat ion detector)Simple Phone Call Relation 
Annotator
Finds mentions of phone 
calls in text and annotates 
them as MadePhoneCallTo
relations.Simple Phone Number 
Annotator
Finds phone numbers in textStarting Point
Ace Annotator
A stat istical ent ity and 
relation annotator that 
performs w ithin-
document coreference
resolut ion.KS Relation Detector 
Aggre gate
Aggregate that  includes 
KS Relation Detector 
and TAE's that provide 
its inputs
Phrase Finder Annotator
Creates  annotations for 
phrases to be passed t o ESG.  
Phrase sources are Wo rdNet 
and pre-annotated Resporator
phrases.GlossO nt
Finds taxonomic and 
other definitional 
relations from 
gloassaries
KDDAnnotator
A stat istical ent ity and 
relation annotator that 
performs w ithin-
document coreference
resolut ion.Nominator
Finds proper names 
and other clues.
TAFF/TALENT
Finds a number of 
standard named 
entit ies.
TFST
Finds a number of 
standard named 
entit ies.TFSTAddr
Finds addresses and 
extracts the subplace
relation.TFSTTime
Extracts TimeEx3 
entit ies.HoldsDuring
Extracts Relat ions 
between TimeEx3 
entit ies and relat ions.
JResporator
Extracts over 80 
classes of  entities.TFSTOnBoard
Finds onBoard
relations between 
people and vehicles.TFSTVe hicle
Finds vehicles.Cross DocumentCorefe rence
Merges coreference chains acr oss 
documents.
U I M A Issues in the combination
•Components have overlapping semantics
–Common type system, but…
–Different meanings, precision, recall
–ACE, Resporator
•Multiple annotations on a single span
–Disagree 20% of the time
•Multiple overlapping co-reference chains
–U.S. subplace of Russia
U I M A Knowledge Integration
Analysis Phase
Query PhaseDocsAnalysis
Semantic
Search IndexExtracted
Knowledge
DBKnowledge
Integration
RDF Sto reOnto logy
(OWL)
KW
SearchSemantic
SearchFact 
SearchDeductive
Search
U I M A Semantic Integration Goals
•Process results of IE into a form suitable for reasoning
–i.e. by advanced reasoning co mponents (time, space, etc.)
•Map linguistic structures into knowledge-base
–Different ontologies
–Different semantics
•Explore utility of reasoning
–Clean up the IE results using ontology semantics
–Improve precision and recall
–Propose candidate contexts
–Experiment with differ ent kinds of reasoning
•Scale the results along so me dimension of “massive”
•Evaluate the quality of the results-Moving targets
-Declarative
U I M ASemantic Integration
“13 delegates from Turkey arrived today.”
IEIE
“13 delegates from <country>Turkey</country> arrived today.”
FormatFormat
conversionconversion
<OWL:country rdf:ID=“Turkey” />
Easy!!!
U I M A IE ↔KR
ExplainabilityBrittleness
Recall
ScalePrecisionRelationsEntities vs. Mentions
U I M A IE ↔KR
ExplainabilityBrittleness
Recall
ScalePrecisionRelationsEntities vs. Mentions
U I M A IE ↔KR
ExplainabilityBrittleness
Recall
ScalePrecisionRelationsEntities vs. Mentions
U I M A IE ↔KR
ExplainabilityBrittleness
Recall
ScalePrecisionRelationsEntities vs. Mentions
U I M A IE ↔KR
ExplainabilityBrittleness
Recall
ScalePrecisionRelationsEntities vs. Mentions
U I M A IE ↔KR
ExplainabilityBrittleness
Recall
ScalePrecisionRelationsEntities vs. Mentions
U I M A IE ↔KR
Co-reference
Improve relation extraction
Make KR more 
error tolerant
Explain IEUse ontology semantics 
to improve precision & 
recall
Scalable 
inferenceExplainabilityBrittleness
Recall
ScalePrecisionRelationsEntities vs. Mentions
U I M AMapping from Extraction to Knowledge
•Type-Class Mapping
–Person →Person
–Country →Location & Political Entity
•Entity Mapping
–Person(Abduhl Ramazi) →kani:person-101
•Relations
–IE:At(Person,Place) →KANI:At
–IE:At( Place,Place) →KANI:subPlace
•Complex Mappings
–HoldsDuring(At(Person, Place), TimeInterval) →
At(Person, fv) & fvValue( fv, Place) & fvTimeInterval( fv, TimeInterval)
–Uses(BioTerrorism, Diseases) →…
U I M A Complex mappings
fvTimeIntervalfvTimeIntervalatatfvValuefvValue
ReifiedFluentReifiedFluent --0404“Joe arrived at Bush Intercontinental Airport at 12:00.”
Person: JoePerson: Joe Facility: BIAFacility: BIA
Relation: Relation: at(Joeat(Joe , BIA), BIA)Relation: Relation: holdsDuring(at(JoeholdsDuring(at(Joe , BIA), 12:00), BIA), 12:00)
PersonPerson --0101
Name: Name: joejoeFacilityFacility --0202
Name: BIAName: BIATimePointTimePoint --0101
Value: 1200Value: 1200TimeExTimeEx : 12:00: 12:00
Knowledge IntegratorKnowledge IntegratorAnnotate Text EntitiesAnnotate Text Entities
Integrate EntitiesIntegrate EntitiesAnnotate Text RelationsAnnotate Text Relations
Integrate Text RelationsIntegrate Text RelationsAnnotate Time RelationsAnnotate Time Relations
Integrate Time RelationsIntegrate Time RelationsKBKBTextText
Ontology Ontology
U I M A Broader Contributions
•Reuse&Adapt existing work
–Chimaera, OntoMerge, Prompt
•Develop catalog of integration inferences
–Extension to Prompt
–Moving towards semi-automation for linguistic 
ontologies
•Mapping TimeML ↔DAML-Time
•Expose deeper semantic requirements
•Improving P&R of IE
U I M ACatalog of Ontology Merging 
Operations
•Simple mappings
–Class1→Existing Class2
–Class1→New Class2
–Class1+Class2Subclass new Class2
–…
•Complex mappings
–Class1→Set2of Classes
–Set1of Classes →Class2
–…
•Language Expressiv ity
–DL-expressible [Halevy] [Calvanese & DeGiacamo]
–Function-free FOL expressible [McDermott & Smith]
–FOL expressible [Chalupsky & MacGregor]
–Non-FOL
U I M AMapping TimeML to OWL-Time
•TimeML
–Markup language with linguistic-based semantics
–time expressions (Timex)
–Events
–Links (before, after, …)
•DAML-Time
–Ontology-based specification of  time points and intervals
–Based on Allen calculus
–No events
•High level correspondence [Pustejovsky&Hobbs]
•Generate complete OWL-Time RDF for TimeBank 1.1 
corpus
U I M ADeeper Semantic Requirements
•IE focused on surface semantics
•Surface semantics appear obvious
–Person(Chris)
–onBoard(Chris, train)
•Requirement for reasoning exposes problems
“Chris was in his office on April 22, 2003.”
Holds(in(Chris, o ffice), t1)
What is t1:
•April 22, 2003 ?
•A time interval during April 22, 2003 ? 
•A time interval that includes April 22, 2003 ?
•A time interval that overlaps with April 22, 2003 ?
•A time interval that intersects with April 22, 2003 ?
U I M AUsing Semantics to boost 
precision/recall
•“A man was arrested, his name was given as Chris”
Cannot find a link from “his ” to “Chris” – the relation is not lexical, it’s semantic
During integration, the semantics of name relation are pro cessed and Chris assigned as name
•“He arrived at Bush Intercontinental at 12:00”
Entity extraction tags “Bush Inter continental”  with Person and Facility
Relation extraction finds at( person, Bush Intercontinental)
Semantics of atrelation requires range be a facility or  place
Semantics of Person and Facilit y are of disjo int cla sses
Person annotation thrown awayCoCo--reference chainreference chain Relation: Relation: nameOf(hisnameOf(his , Chris), Chris)
Type: Person, FacilityType: Person, Facility Relation: Relation: at(heat(he , Bush Intercontinental), Bush Intercontinental)
U I M AKnowledge for Improving Precision
•Range/Domain Constraints
–At xPerson ×Place
•Background Knowledge
–“Kisumu is the AIDS capital of Kenya.” 
–Capital of Kenya is Nairobi
•Logical Consistency
–Vehicle in two places at the same time
•Temporal Consistency
–A time point both before and after the same event 
U I M AKnowledge for Improving Recall
•Semantic Relations
–Has-alias
•Containment axioms
–Boston in Massachusetts in US
–April 23, 2003 in April, 2003, in 2003
•Classification axioms
–Author of news paper article a journalist
•Spatial axioms
–Passenger on vehicle loc ated where vehic le is located
•Temporal ordering axioms
–Transitivity of before, after
–Full Allen calculus
•Requires different notion of evaluation
U I M A Trials and Tribulations
•Reasoning helps information processing
–Increase in recall through deductive inference
–Increase in precision through constraint processing
•General-purpose reasoning algorithms are complex
–OWL-lite is Co-NP
–OWL-DL is EXPTIME
–OWL, General First-order reasoning is undecidable
–Allen Calculus undecidable
•In practice this means reasoning must be bounded
–A tradeoff between scale and effectiveness of reasoning
•Research agenda – explore this tradeoff
–Techniques for bounding data (e.g. partitioning)
–Techniques for “hiding” data 
–Incrementally apply more sophisticated reasoning (staged reasoning)
–Measure KB and Ontology complexity (what is massive?)
U I M A Hiding Data - Motivation
•IE generates a lot of data
•Need to reduce the burden on reasoning-
based processes
–RDF-based reasoners limited to ~500K nodes
•Can we reduce the amount of data?
•What if it is needed later?
U I M AData Volume Analysis
……Rick Wagoner is Rick Wagoner is 
CEO of General CEO of General 
Motors .  He is  an Motors .  He is  an 
alumnus of Duke alumnus of Duke 
UniversityUniversity ……
Mr. Wagoner  (Person)Rick Wagone r
(Person)CEO of
Genera l Motors
(Organizati on)
He(Person)Duke Univ ersity
(Organizati on)
alumnus ofcorefP1 type PersonP1 type Person
P2 type PersonP2 type Person
O1 type OrgO1 type Org
O2 type OrgO2 type Org
P1 P1 ceoOfceoOf O1O1
P2 P2 alumnusOfalumnusOf O2O2
P1 P1 sameAssameAs P2P2
P3 type PersonP3 type Person
P3 P3 sameAssameAs P2P2
P3 P3 authorOfauthorOf O3O3
O3 type BookO3 type Bookcoref
Two triples for every coTwo triples for every co --referencereference••2.6 mentions per instance2.6 mentions per instance
••20 instances per document20 instances per document
••136 triples per document136 triples per document
••64 triples per docum ent for representing 64 triples per docum ent for representing coreferencecoreference ..IEIE SISI
……Mr. Wagoner, Mr. Wagoner, 
the author of the author of 
Sleeping in the Sleeping in the 
WoodsWoods ……Sleeping  in the 
Woods (Book)autho rOf
U I M AHiding Data
Mr. Wagoner  (Person)
He(Person)
Mr. Wagoner  (Person)……Rick Wagoner is Rick Wagoner is 
CEO of General CEO of General 
Motors .  He is  an Motors .  He is  an 
alumnus of Duke alumnus of Duke 
UniversityUniversity ……Rick Wagone r
(Person)CEO of
Genera l Motors
(Organizati on)
Duke Univ ersity
(Organizati on)
alumnus ofcorefP1 type PersonP1 type Person
P2 type PersonP2 type Person
O1 type OrgO1 type Org
O2 type OrgO2 type Org
P1 P1 ceoOfceoOf O1O1
P2 P2 alumnusOfalumnusOf O2O2
P1 P1 sameAssameAs P2P2
P3 type PersonP3 type Person
P3 P3 sameAssameAs P2P2
P3 P3 authorOfauthorOf O3O3
O3 type BookO3 type BookP1 type PersonP1 type Person
O1 type OrgO1 type Org
O2 type OrgO2 type Org
P1 P1 ceoOfceoOf O1O1
P1 P1 alumnusOfalumnusOf O2O2
P1 P1 authorOfauthorOf O3O3
O3 type BookO3 type BookcorefIEIE SISI
……Mr. Wagoner, Mr. Wagoner, 
the author of the author of 
Sleeping in the Sleeping in the 
WoodsWoods ……Sleeping  in the 
Woods (Book)autho rOf
••47% reduction in the number of RDF triples47% reduction in the number of RDF triples
••62% reduction in the number of RDF nodes62% reduction in the number of RDF nodes
••But what if itBut what if it ’’s wrongs wrong ……Mr. Wagoner  (Person)coref
U I M AHiding Data
Mr. Wagoner  (Person)……Rick Wagoner is Rick Wagoner is 
CEO of General CEO of General 
Motors .  He is  an Motors .  He is  an 
alumnus of Duke alumnus of Duke 
UniversityUniversity ……Rick Wagone r
(Person)CEO of
Genera l Motors
(Organizati on)
Duke Univ ersity
(Organizati on)
alumnus ofP1 type PersonP1 type Person
P2 type PersonP2 type Person
O1 type OrgO1 type Org
O2 type OrgO2 type Org
P1 P1 ceoOfceoOf O1O1
P2 P2 alumnusOfalumnusOf O2O2
P1 P1 sameAssameAs P2P2
P3 type PersonP3 type Person
P3 P3 sameAssameAs P2P2
P3 P3 authorOfauthorOf O3O3
O3 type BookO3 type BookIEIE SISI
……Mr. Wagoner, Mr. Wagoner, 
the author of the author of 
Sleeping in the Sleeping in the 
WoodsWoods ……Sleeping  in the 
Woods (Book)autho rOfP1 type PersonP1 type Person
O1 type OrgO1 type Org
O2 type OrgO2 type Org
P1 P1 ceoOfceoOf O1O1
P1 P1 alumnusOfalumnusOf O2O2
P3 type PersonP3 type Person
P3 P3 sameAssameAs P1P1
P3 P3 authorOfauthorOf O3O3
O3 type BookO3 type Book
••47% reduction in the number of RDF triples47% reduction in the number of RDF triples
••62% reduction in the number of RDF nodes62% reduction in the number of RDF nodes
••But what if itBut what if it ’’s wrongs wrong ……
••““HideHide ””IE inferences in provenanceIE inferences in provenance
••Expose when problem arisesExpose when problem arisesMr. Wagoner  (Person)coref
U I M A Key Challenges
•How much reasoning when
•Artifacts of establis hed IE
–Imprecision
–Co-reference
–Multiple annotations
•Confidence, Trustworthiness
•Maintaining provenance through mapping
•Limits of Automated Reasoning for integration
–Limits of description logic reas oning (OWL/DL) [Lenzerini, 2002], 
[Halevy,  2001]
–Limits of first order reasoning (FOL) [McDermott, et al, 2002]
•Axiomatic semantics of information extraction
–Further clarify semantics [ACE, KDD]
–Boost pr ecision
•Adaptability
•Evaluation
U I M AHow much reasoning when?
•Incremental value of increased processing
•Many dimensions of complexity
–Representational
•Worst-case complexity
•Special-purpose reasoning
•Optimizations
–Instance level
•Number of instances, relations
•Connectedness
•Precision
–Ontology level
•Number of classes, properties
•Number of axioms, constraints
