This is the author’s version of a work that was submitted/accepted for pub-
lication in the following source:
De Bruin, Tonia , Freeze, Ronald, Kaulkarni, Uday, & Rosemann, Michael
(2005) Understanding the Main Phases of Developing a Maturity Assess-
ment Model. In Campbell, B, Underwood, J, & Bunker, D (Eds.) Aus-
tralasian Conference on Information Systems (ACIS) , November 30 - De-
cember 2 2005, Australia, New South Wales, Sydney.
This ﬁle was downloaded from: http://eprints.qut.edu.au/25152/
Notice :Changes introduced as a result of publishing processes such as
copy-editing and formatting may not be reﬂected in this document. For a
deﬁnitive version of this work, please refer to the published source:
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 Understanding the Main Phases of Developing a Matur ity Assessment Model 
Tonia de Bruin (PhD Candidate) and Prof Michael Ros emann 
Queensland University of Technology 
Ronald Freeze (PhD Candidate) and Prof Uday Kulkarn i 
Arizona State University  
 
Faculty of Information Technology 
Queensland University of Technology 
Brisbane, Queensland 
Email: t.debruin@qut.edu.au ; m.rosemann@qut.edu.au  
W.P. Carey School of Business 
Arizona State University 
Tempe, Arizona 
Email: ronald.freeze@asu.edu ; uday.kulkarni@asu.edu  
Abstract  
Practitioners and academics have developed numerous  maturity models for many domains in order to measu re 
competency. These initiatives have often been influ enced by the Capability Maturity Model.  However, a n 
accumulative effort has not been made to generalize  the phases of developing a maturity model in any d omain. 
This paper proposes such a methodology and outlines  the main phases of generic model development.  The  
proposed methodology is illustrated with the help o f examples from two advanced maturity models in the  
domains of Business Process Management and Knowledg e Management.   
Keywords  
Business Process Management, Knowledge Management, Maturity Model, Design Methodology, CMM 
INTRODUCTION  
As organizations continually face pressures to gain  and retain competitive advantage, identifying ways  of cutting 
costs, improving quality, reducing time to market a nd so on, become increasingly important.  Maturity models 
have been developed to assist organizations in this  endeavour.  These models are used as an evaluative  and 
comparative basis for improvement (Fisher 2004; Har mon 2004; Spanyi 2004) and in order to derive an 
informed approach for increasing the capability of a specific area within an organization (Ahern et al . 2004, 
Hakes 1996; Paulk et al. 1993).  Maturity models ha ve been designed to assess the maturity (i.e. compe tency, 
capability, level of sophistication) of a selected domain based on a more or less comprehensive set of  criteria.  
The most popular way of evaluating maturity is a fi ve-point Likert scale with ‘5’ representing the hig hest level 
of maturity.  Maturity models have proliferated acr oss a multitude of domains since the concept of mea suring 
maturity was introduced with the Capability Maturit y Model (CMM) from the Software Engineering Institu te 
(SEI) – Carnegie Mellon.  Some examples of existing  management models are included in Table 1. 
 
Model  Domain Key Reference Developer Developed 
Capability Maturity Model 
Integration CMMI Management http://www.sei.cmu.edu/cm 
mi/cmmi.html   Carnegie Mellon 
University Early 00’s 
Enterprise Architecture 
Maturity Model IT Management https://www.nascio.org/hotIs 
sues/EA/EAMM.pdf   National Association 
of State CIO’s Early 00’s 
European Foundation for 
Quality Management 
(EFQM) Excellence Model  Business 
Management http://www.efqm.org/Default 
.aspx?tabid=35   EFQM Early 90’s 
Process Maturity Model Process 
Management http://www.rummler-
brache.com/   Rummler-Brache 
Group Early 90’s 
Project Management 
Maturity Model Project 
Management http://www.ogc.gov.uk/sdtoo 
lkit/reference/tools/PMMM_ 
release_v5.pdf   Office of 
Government 
Commerce, UK Early 90’s 
Table 1: Examples of Management Maturity Models  
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 The CMM has gained such global acceptance that high  maturity scores are one of the requirements for ac cepting 
off-shoring partners.  The SEI has created six matu rity models in total and has recently incorporated three legacy 
CMMs into one maturity model now named the Capabili ty Maturity Model Integration – CMMI (Ahern et al. 
2004). Two other stand alone models include the Peo ple Capability Maturity Model and the Software 
Acquisition Capability Maturity Model. However, the  SEI is not the only developer of methods to assess  
maturity. More than 150 maturity models have been d eveloped to measure, among others, the maturity of IT 
Service Capability, Strategic Alignment, Innovation  Management, Program Management, Enterprise 
Architecture and Knowledge Management Maturity. Unl ike CMM which has reached the level of a compliance  
standard (Mutafelija and Stromberg 2003), most of t hese models simply provide a means for positioning the 
selected unit of analysis on a pre-defined scale.   
Whilst maturity models are high in number and broad  in application, there is little documentation on h ow to 
develop a maturity model that is theoretically soun d, rigorously tested and widely accepted.  This pap er seeks to 
address this issue, by presenting a model developme nt framework applicable across a range of domains.  Support 
for this framework is provided through the presenta tion of the consolidated methodological approaches,  
including testing, undertaken by two universities w hile independently developing maturity models in th e 
domains of Business Process Management (BPM) and Kn owledge Management (KM) respectively.  Throughout 
this paper, these models will be referred to as the  Business Process Management Maturity (BPMM) model and 
the Knowledge Management Capability Assessment (KMC A) model.  This paper is structured so that the gen eric 
phases required for development of a general maturi ty model are identified first.  Next, each phase is  discussed 
in detail using the two selected maturity models as  examples. Finally, conclusions are drawn regarding  the 
potential benefits from utilisation of such a model  and limitations and future research are identified .    
DEVELOPMENT FRAMEWORK 
The importance of a standard development framework is emphasised when considering the purpose for whic h a 
model may be applied including whether the resultin g maturity assessment is descriptive, prescriptive or 
comparative in nature.  If a model is purely descri ptive, the application of the model would be seen a s single 
point encounters with no provision for improving ma turity or providing relationships to performance.  This type 
of model is good for assessing the here-and-now i.e . the as-is situation.  A prescriptive model provid es emphasis 
on the domain relationships to business performance  and indicates how to approach maturity improvement  in 
order to positively affect business value i.e. enab les the development of a road-map for improvement.  A 
comparative model enables benchmarking across indus tries or regions.  A model of this nature would be able to 
compare similar practices across organizations in o rder to benchmark maturity within disparate industr ies.  A 
comparative model would recognize that similar leve ls of maturity across industries may not translate to similar 
levels of business value.  It is argued that, whils t these model types can be seen as distinct, they a ctually 
represent evolutionary phases of a model’s lifecycl e.  First, a model is descriptive so that a deeper understanding 
of the as-is domain situation is achieved.  A model  can then be evolved into being prescriptive as it is only 
through a sound understanding of the current situat ion that substantial, repeatable improvements can b e made.  
Finally, for a model to be used comparatively it mu st be applied in a wide range of organizations in o rder to 
attain sufficient data to enable valid comparison.  The proposed standard development framework forms a sound 
basis to guide the development of a model through f irst the descriptive phase, and then to enable the evolution of 
the model through both the prescriptive and compara tive phases within a given domain.  Furthermore, we  
propose that, whilst decisions within the phases of  this framework may vary, the phases themselves can  be 
reflected in a consistent methodology that is able to be applied across multiple disciplines.  Figure 1 summarises 
the phases included in the generic framework. 
Design    Populate Test Deploy Scope Maintain 
 
Figure 1: Model Development Phases 
Whilst these phases are generic, their order is imp ortant. For example, decisions made when scoping th e model 
will impact on the research methods selected to pop ulate the model or the manner in which the model ca n be 
tested.  In addition, progression through some phas es may be iterative, for example it may be a case o f ‘design’, 
‘populate’ and ‘test’ and dependent upon the ‘test’  results, necessary to re-visit and adjust decision s made in 
earlier phases. The usefulness of this lifecycle mo del is best reflected by showing how it has been ap plied for the 
independent development of the BPMM and KMCA models .   
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 Phase 1 - Scope 
The first phase in developing a maturity model is t o determine the scope of the desired model.  The co mbination 
of scoping decisions will influence all remaining p hases in the proposed generic model development fra mework.  
Determining the scope of the desired model will set  the outer boundaries for model application and use .  The 
major decisions to be addressed in Phase 1 - Scope are reflected in Table 2.   
 
Criterion Characteristic 
Focus of Model Domain Specific General 
Development Stakeholders Academia Practitioners Government Combination 
Table 2: Decisions when Scoping a Maturity Model 
The most significant decision made in this phase in volves the focus of the model.  Focus refers to whi ch domain 
the maturity model would be targeted and applied.  Focusing the domain will distinguish the proposed m odel 
from other existing models.  Focusing the model wit hin a domain will also determine the specificity an d 
extensibility of the model.  Examples of general mo dels include management models, like EFQM (which 
considers business excellence) and Total Quality Ma nagement (which considers the quality of processes) .  An 
example of a popular, more specific model was the C MM which applies to the single process of software 
development.  With the initial focus of the model i dentified, stakeholders from academia, industry, no n-profits 
and government can be identified to assist in the d evelopment of the model.  The importance of initial ly scoping 
for the development of a maturity model is confirme d by examples from the independent development of t he 
BPMM and KMCA models.  Scoping decisions were simil ar for both the BPMM and KMCA models.  The 
general focus of both models was identified by the selection of specific domains BPM and KM respective ly.  An 
extensive review of existing literature in each dom ain, related domains and maturity models was conduc ted.  
Such a review can provide a deep understanding of h istorical and contemporary domain issues.  The revi ew by 
Rosemann et al. (2004) identified existing models a nd provided support for developing a more comprehen sive 
model specific to the BPM domain.  Existing models either did not adequately capture domain specific i ssues, 
complexities and/or had not been rigorously tested.   The review also confirmed that academics and prac titioners 
shared a strong interest in the development of a mo del to fill this gap.  Therefore the goal was to de velop a model 
specific to the BPM domain that would assist organi zations in better understanding BPM complexities an d 
further to enable the improvement of domain capabil ities.  The model was initially viewed as a diagnos tic tool 
that would first enable assessment or description o f the ‘as-is’ domain position of an entity.  A furt her aim was to 
develop the model so that it could be used to assis t in the determination of the desired ‘to-be’ posit ion and enable 
the development of a roadmap for improving the doma in position from ‘as-is’ to ‘to-be’.  Furthermore, BPMM 
researchers were interested in developing a model t hat would become widely accepted and enable compara tive 
benchmarking with the potential to forming the basi s of a global BPM standard. The KMCA research team 
shared similar experiences when scoping their model .  Finally, determining which stakeholders will ass ist in the 
model development process is influenced by the mode l’s purpose.  For the BPMM, a consortium of academi a 
and practitioners was assembled to provide input fr om multiple domain perspectives.  For the KMCA, a l arge 
multi-national firm that comprises multiple indepen dent business units was engaged.  Input was obtaine d from 
different business units of this firm to insure the  broad acceptability of the model.   
Phase 2 - Design 
The second phase of the proposed framework is to de termine a design or architecture for the model that  forms 
the basis for further development and application.  Table 3 shows major Phase 2 decisions. 
 
Criterion Characteristic 
Audience Internal External 
Executives, Management Auditors, Partners 
Method of Application Self Assessment Third Party Assisted Certified Prac titioner 
Driver of Application Internal Requirement External Requirement Both 
Respondents Management Staff Business Partners 
Application 1 entity / 1 region Multiple entities / single 
region Multiple entities /  
multiple region 
Table 3: Decisions when Designing a Maturity Model 
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 In particular, the design of the model incorporates  the needs of the intended audience and how these n eeds will 
be met.  The needs of the intended audience are ref lected in why  they seek to apply the model, how  the model 
can be applied to varying organizational structures , who  needs to be involved in applying the model and what 
can be achieved through application of the model.  To meet audience needs, the model design therefore needs to 
strike an appropriate balance between an often comp lex reality and model simplicity.  A model that is 
oversimplified may not adequately reflect the compl exities of the domain and may not provide sufficien t 
meaningful information for the audience.  Whilst a model that appears too complicated may limit intere st or 
create confusion.  In addition, a model that is too  complicated raises the potential for incorrect app lication 
resulting in misleading outcomes.  Within existing maturity models a common design principle is to rep resent 
maturity as a number of cumulative stages where hig her stages build on the requirements of lower stage s with 5 
representing high maturity and 1 low.  This practic e was made popular by the CMM and appears to have w ide 
practical acceptance.  The number of stages may var y from model to model, but what is important is tha t the 
final stages are distinct and well-defined, and tha t there is a logical progression through stages.  S tages should 
also be named with short labels that give a clear i ndication of the intent of the stage.  Stage defini tions should be 
developed to expand stage names and provide a summa ry of the major requirements and measures of the st age, 
especially those aspects that are new to the stage and not included as elements of lower stages.  When  defining 
maturity stages either a top-down or bottom-up appr oach can be used.  With a top-down approach definit ions are 
written first and then measures are developed to fi t the definitions.  With a bottom-up approach the r equirements 
and measures are determined first and then definiti ons are written to reflect these.  A top-down appro ach works 
well if the domain is relatively naïve and there is  little evidence of what is thought to represent ma turity.  The 
emphasis in this instance is firstly on what represents maturity and then how  can this be measured.  In a more 
developed domain where there is existing evidence o n what represents maturity, the focus moves first to how  this 
can be measured and then builds definitions on this  basis.  An example of maturity stages defined for the KMCA 
model is provided in Table 4. 
 
KMCA Stages  Definitions  
Level-0: Not Possible Knowledge sharing is discoura ged. There is general unwillingness to share 
knowledge. People do not seem to value knowledge sh aring. 
Level-1: Possible Knowledge sharing is not discoura ged. There is a general willingness to 
share. People who understand the value of sharing d o it. Meaning of 
knowledge assets is understood. 
Level-2: Encouraged Culture encourages sharing of k nowledge assets. Value of knowledge 
assets is recognized. Knowledge assets are stored /  tracked in some fashion. 
Level-3: Enabled/ Practiced Sharing of knowledge as sets is practiced. Systems / tools to enable KM 
activities exist. Rewards / incentives promote know ledge sharing. 
Level-4: Managed Employees expect to locate knowled ge. Training is available. KM related 
activities are part of workflow.  Systems / tools f or supporting KM 
activities are easy to use. KM capabilities and ben efits are assessed. 
Leadership exhibits commitment to KM and provides K M strategy. 
Level-5:  
Continuous Improvement KM processes are reviewed / improved. KM systems / tools are widely 
accepted, monitored / updated.  KM assessment gener ates realistic 
improvement.  
Table 4: Example Maturity Stages of KMCA model 
A further consideration when designing a model is h ow maturity stages can be reported to the audience.       
Representation of maturity as a series of one-dimen sional linear stages is widely-accepted and has for med the 
basis for assessment in many existing tools.  This form of assessment results in an ‘average’ maturity  stage being 
provided for the entity. Whilst this form of assess ment provides a simple means of comparing maturity stages, it 
does not adequately represent maturity within compl ex domains, providing little guidance to an organiz ation 
wishing to improve the ‘as-is’ position.  Alternati vely, a ‘stage-gate’ approach enables the provision  of more 
differentiated maturity assessments within complex domains.  A stage-gate approach is achieved by prov iding 
additional layers of detail that enable separate ma turity assessments for a number of discrete areas, in addition to 
an overall assessment for the entity.  These layers  can be represented by the domain, domain component s and 
sub-components.  The results obtained from a layere d model enable an organization to gain a deeper 
understanding of their relative strengths and weakn esses in the domain and to target specific improvem ent 
strategies thereby enabling more efficient resource  allocation.  The ability to drill-down through the  maturity 
assessment enables model assessment reports to be t ailored to varying needs of multiple audiences.     
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 Figure 2 illustrates how these layers can be 
matched to the needs of varying audiences 
within an organization. A domain component 
(layer 2) is a major, independent aspect of a 
given domain that is important to domain 
maturity e.g. critical success factors.  Domain 
components are reflected in general stage 
definitions and enable clustering of results to 
model audience.  Domain sub-components 
(layer 3) are specific capability areas within 
the domain components that provide further 
detail enabling targeted maturity level 
improvements.  When designing a model the 
number of domain components and sub-
components should be kept low to minimise 
perceived complexity in the model and ensure 
the independence of the components.  The 
experience of BPMM researchers indicates 6 
components and 5 sub-components are 
adequate.  KMCA researchers used 4 domain 
components and 6 sub-components.   Figure 2: Exampl e of Maturity Model Layers 
Phase 3 - Populate  
Once the scope and design of the model are agreed t he content of the model must be decided.  In this p hase it is 
necessary to identify what needs to be measured in the maturity assessment an d how  this can be measured.  
Identification of domain components is critical for  complex domains as this enables a deeper understan ding of 
maturity, without which the identification of speci fic improvement strategies is difficult.  The goal is to attain 
domain components and sub-components that are mutua lly exclusive and collectively exhaustive.  In a ma ture 
domain the identification of domain components can be achieved through an extensive literature review.   In 
particular critical success factors and barriers to  entry provide great insights into domain component s as 
evidenced by Rosemann and de Bruin (2004).  The pre sence of a rich stream of literature and tested mod els 
reduces concerns of whether components are mutually  exclusive and collectively exhaustive.  Once an in itial list 
has been developed interviews are used to further v alidate the a priori constructs and increase the al ready 
established mutually exclusive and collectively exh austive list of critical success factors.  Confirma tion of 
components selected from multiple evidentiary sourc es improves the extensibility of the findings of th e final 
maturity model.  In a relatively new domain (e.g. K nowledge Management), it may not be possible to gat her 
sufficient evidence through existing literature to derive a comprehensive list of domain components.  In this 
instance, a literature review is considered only su fficient in providing a theoretical starting point and other means 
of identification is necessary. Figure 3 shows the domain components identified for the BPMM and KMCA 
models. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Concept Diagrams for the BPMM and KMCA mo dels  Management 
& Staff Layer 1 
Layer 2 
Layer 3  Domain 
Component 
Domain 
Sub-Component Executives CEO Domain 
Lessons Learned 
Knowledge 
Documents 
Data 
Expertise User Satisfaction Performance & 
Productivity Culture KM Capability 
Lessons Learned 
Knowledge 
Documents 
Data 
Expertise User Satisfaction Performance & 
Productivity Culture KM Capability Strategic Alignment
Governance 
Methods 
Information 
Technology 
People 
Culture Process Success Business Success ContextBPM Success Factors 
Strategic Alignment
Governance 
Methods 
Information 
Technology 
People 
Culture Process Success Business Success ContextBPM Success Factors 
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 Furthermore, identification of domain sub-component s is recommended for complex domains (e.g. Business  
Process Management and Knowledge Management).  This  additional layer of detail assists in the developm ent of 
assessment questions, enables richer analysis of ma turity results and improves the ability to present maturity 
results in a manner that meets the needs of the tar get audience.  It is unlikely that a literature rev iew (no matter 
how comprehensive) will identify sufficient informa tion to populate this layer of detail.  It is recom mended that 
exploratory research methods such as Delphi techniq ue, Nominal Group technique, case study interviews and 
focus groups be considered.  Selection of the most appropriate technique/s will depend on the stakehol ders 
involved in the model development and the resources  available to the development team.  This is shown by way 
of example where the BPMM researchers utilized the Delphi technique and KMCA researchers utilized focu s 
groups in the definition and identification of doma in components and sub-components.  
Although the lapsed time and resources when using t he Delphi technique were significant, this method w as 
considered most appropriate for the BPMM model for a number of reasons including: (1) it provided the 
opportunity to access a broad range of global domai n experts; (2) it met existing budget and resource constraints; 
and (3) it enabled the identification of contempora ry issues.  The Delphi technique includes the ident ification and 
selection of a panel of experts from whom informati on about a specific topic is solicited through the iterative 
completion of a number of surveys.  Delphi studies are considered beneficial when: (1) dealing with co mplex 
issues (Okoli and Pawlowski 2004; Ono and Wedemeyer  1994); (2) seeking to combine views to improve 
decision making (Bass 1983); (3) in order to contri bute to an incomplete state of knowledge (Delbecq e t al. 
1975); and (4) where there is a lack of empirical e vidence (Murphy et al. 1998).  The development of a  maturity 
model in a complex domain involves all four of thes e issues.  Furthermore, Okoli and Pawlowski (2004) indicate 
that the two major areas for Delphi study applicati ons are the traditional forecasting and more recent ly 
concept/framework development where studies typical ly involve a two step process being: (1) identifyin g and 
elaborating a set of concepts and (2) classificatio n/taxonomy development.  Further insights into the use of the 
Delphi technique in the development of a maturity m odel are provided by Rosemann and de Bruin (2005).    
For the KMCA model, additional capability areas wer e identified in association with knowledge life cyc le 
stages.  KMCA researchers were working with a singl e large organization so had a ready supply of avail able 
knowledge workers. Population was primarily based o n brainstorming sessions, focus groups and pilot/pr e 
testing of the survey instrument.  The domain resea rch provided numerous views of these stages of know ledge 
(Birkinshaw et al. 2002; Satyadas et al. 2001; Zack  1999b).  Each domain component was mapped to a fou r 
stage acquire/store/present/transfer knowledge life  cycle model.  Brainstorming sessions were assemble d with 
the client organization in order to provide clarity  for each domain component and develop scale items that would 
capture the stage of the knowledge life cycle.  Onc e it was felt that sufficient coverage was achieved  within each 
domain component, a focus group was assembled in or der to determine if the scale items were understand able 
for the target audience.  The focus group consisted  of individuals selected from several business unit s for which 
the maturity assessment was to be administered.   
Each of the methods employed, the Delphi technique and Focus Groups, has differing advantages and 
disadvantages with the relative importance of these  dependent on the context of the scope that has pre viously 
been defined.  For example, the advantages of Delph i studies include: (1) Anonymity leads to more crea tive 
outcomes and adds richness to data (van de Ven and Delbecq 1974; Okoli and Pawlowski 2004); (2) issues  
inherent in face-to-face groups such as dominant pe rsonalities, conflict and group pressures are virtu ally 
eliminated (Loo 2002; Murphy et al. 1998); (3) geog raphic boundaries and associated travel and co-ordi nation 
factors are essentially removed (Loo 2002; Okoli an d Pawlowski 2004) and (4) duration and cost of stud y can be 
minimised (Powell 2003).  Whilst these advantages w ere important to BPMM researchers due to the desire  to 
create a global standard, the KMCA model was develo ped in conjunction with a single large organization  and 
the engagement required certain deliverables at cer tain times.  As a result, some of the advantages of  the Delphi 
studies were unobtainable or not of particular bene fit.  Anonymity was not considered important due to  the time 
constraints of the project. While all focus groups and brainstorming sessions were face-to-face, the c ulture of the 
organization promoted input by all participating in dividuals, but inherent issues may have still exist ed.  
Geographic boundaries, associated travel, coordinat ion factors and the cost of the KMCA study were min imized 
due to a single organization engagement.  Similarly , there are a number of criticism of Delphi studies  including; 
(1) the flexible nature of Delphi study design (Erf fmeyer et al. 1986; Schmidt 1997; Turoff 1970; van de Ven 
and Delbecq 1974); (2) the discussion course is det ermined by the researchers (Dalkey and Helmer 1963;  
Richards and Curran 2002); (3) accuracy and validit y of outcomes (Ono and Wedemeyer 1994; Woudenberg 
1991).  Whilst the global nature of BPMM model deve lopment meant the use of Delphi studies was preferr ed, 
researchers worked to minimise inherent disadvantag es.  Similar criticisms exist for the conduct of fo cus groups 
and KMCA researchers also took action to minimise i mpact.   
The important issue when populating the model is to  select the combination of research methods that is  most 
appropriate for model development in the context of  earlier scoping decisions and desired model outcom es.  By 
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 way of example, domain components for the BPMM mode l were identified through an extensive literature 
review with the resultant list validated through in terviews and case studies in two organizations.  Ro semann and 
de Bruin (2004) provide insights into both the use of an extensive literature review and the subsequen t 
application of the BPMM model in two case studies.  Rosemann and de Bruin (2005) explains how the Delp hi 
technique was then used to further define sub-compo nents by seeking input from domain experts from var ious 
domain perspectives.  This process identified a ran ge of contemporary global BPM issues and contribute s to 
developing a model that has wide practical appeal a nd potential for a global standard.   Figure 4 depi cts the 
outcomes achieved with respect to BPMM model develo pment.     
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4: Domain, Components and Sub-Components of the BPMM model  
Next it is necessary to determine how maturity meas urement can occur i.e. the instrument/s used in con ducting 
an assessment and the inclusion of appropriate ques tions and measures within this instrument.  When se lecting 
an instrument for conducting an assessment consider ation needs to be given to the model generalisabili ty 
together with resources available for conducting as sessments.  A quantitative method such as a survey that can 
be made available through electronic means is recom mended.   Use of a survey that incorporates quantit ative 
measures enables collection of results that enable consistent statistical analysis and improves compar ability of 
results.  Delivery and collection of survey by elec tronic means makes the survey easily distributable to a wide 
range of respondents across geographic boundaries.  Furthermore, electronic delivery and collation aid s in 
reducing the costs associated with survey distribut ion and increases the reliability of responses by r emoving the 
need for re-keying.    With respect to determining the questions, the domain components and sub-compon ents 
provide good guidance.  A review of existing litera ture can result in a comprehensive list of question s.  Another 
alternative is to use questions that have previousl y been determined and used in another form.  The us e of Likert 
scales (or similar) can improve reliability and con sistency of response and enables results to be easi ly mapped to 
maturity stages.  It is important that questions an d responses are valid i.e. that they measure what i t is they are 
intended to measure.  In addition, a balance in the  number of questions is important.  Sufficient ques tions are 
required to ensure complete measurement but too man y questions may reduce reliability of data by resul ting in a 
reduction in total survey responses or an increase in incomplete surveys.  The structure of the survey  can assist in 
this endeavour.  Depending on the intended responde nt it is possible to structure the survey for diffe rent results.  
For example, the BPMM survey was designed in discre te sections with each section being completed by an  
expert in the domain component.  In this case it is  possible to increase the number of questions asked  resulting in 
greater insights into the domain component but limi ting the number of respondents (and therefore the a bility to 
generalize) within the unit of analysis.  Alternati vely, the KMCA survey was designed to be completed by a 
large number of individuals (thereby increasing the  ability to generalize) within the unit of analysis  but resulting 
in a less comprehensive understanding of the domain  component as fewer questions were asked of a more 
general respondent.   
Whilst the methods employed for populating the mode l may vary from case to case, the use of complement ary 
research methods in the identification of independe nt and relevant content assists in developing a sou ndly 
constructed model that can be further tested.     Governance 
Process 
Management 
Standards Process Metrics & 
Performance 
Linkage Process Roles and 
Responsibilities Business Process 
Management Maturity 
Methods 
Process 
Management 
Decision Making Strategic 
Alignment 
Process Output 
MeasurementEnterprise Process 
Architecture Strategy & 
Process Capability 
Linkage Process 
Improvement Plan Information 
Technology People Culture 
Process 
Customers & 
Stakeholders Process Control & 
MeasurementProcess 
Implementation & 
Execution Process Design & 
Modeling 
Process Control & 
MeasurementProcess 
Implementation & 
Execution Process Design & 
Modeling 
Process Education 
& Learning Process 
Management 
Knowledge Process Skills & 
Expertise 
Process Attitudes 
& Behaviors Process Values & 
Beliefs Responsiveness to 
Process Change 
Process Project & 
Program MgmtProcess 
Improvement & 
Innovation 
Process Project & 
Program MgmtProcess 
Improvement & 
Innovation 
Process 
Management 
Leaders Process 
Collaboration & 
Communication Leadership 
Attention to 
Process 
Process 
Management 
Social Networks Process 
Management 
Controls Governance 
Process 
Management 
Standards Process Metrics & 
Performance 
Linkage Process Roles and 
Responsibilities Business Process 
Management Maturity 
Methods 
Process 
Management 
Decision Making Strategic 
Alignment 
Process Output 
MeasurementEnterprise Process 
Architecture Strategy & 
Process Capability 
Linkage Process 
Improvement Plan Information 
Technology People Culture 
Process 
Customers & 
Stakeholders Process Control & 
MeasurementProcess 
Implementation & 
Execution Process Design & 
Modeling 
Process Control & 
MeasurementProcess 
Implementation & 
Execution Process Design & 
Modeling 
Process Education 
& Learning Process 
Management 
Knowledge Process Skills & 
Expertise 
Process Attitudes 
& Behaviors Process Values & 
Beliefs Responsiveness to 
Process Change 
Process Project & 
Program MgmtProcess 
Improvement & 
Innovation 
Process Project & 
Program MgmtProcess 
Improvement & 
Innovation 
Process 
Management 
Leaders Process 
Collaboration & 
Communication Leadership 
Attention to 
Process 
Process 
Management 
Social Networks Governance 
Process 
Management 
Standards Process Metrics & 
Performance 
Linkage Process Roles and 
Responsibilities Business Process 
Management Maturity 
Methods 
Process 
Management 
Decision Making Strategic 
Alignment 
Process Output 
MeasurementEnterprise Process 
Architecture Strategy & 
Process Capability 
Linkage Process 
Improvement Plan Information 
Technology People Culture 
Process 
Customers & 
Stakeholders Process Control & 
MeasurementProcess 
Implementation & 
Execution Process Design & 
Modeling 
Process Control & 
MeasurementProcess 
Implementation & 
Execution Process Design & 
Modeling 
Process Education 
& Learning Process 
Management 
Knowledge Process Skills & 
Expertise 
Process Attitudes 
& Behaviors Process Values & 
Beliefs Responsiveness to 
Process Change 
Process Project & 
Program MgmtProcess 
Improvement & 
Innovation 
Process Project & 
Program MgmtProcess 
Improvement & 
Innovation 
Process 
Management 
Leaders Process 
Collaboration & 
Communication Leadership 
Attention to 
Process 
Process 
Management 
Social Networks Process 
Management 
Controls 
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 Phase 4 - Test 
Once a model is populated, it must be tested for re levance and rigor.    It is important to test both the construct of 
the model and the model instruments for validity, r eliability and generalisability.   
Construct validity is represented by both face and content validity.  Face validity is assessed by whe ther good 
translations of the constructs have been achieved. Such validation is assessed during the population o f the model 
using such tools as focus groups and interviews. Th e maturity model should be considered complete and accurate 
with respect to the identified scope of the model.  Selecting complementary methods for populating the  model 
will assist in achieving face validity.  Content va lidity is assessed as to how completely the domain has been 
represented. The extent of the literature review an d breadth of the domain covered provides a measure of content 
validity. Once the initial maturity model has been judged complete, an inter-rater reliability pilot t est can be 
initiated in order to improve the convergence of op inions that desired design objectives have been ach ieved. 
These initial steps for construct validity are crit ical to insuring that the theoretical basis of the model is sound.  
The construct of the BPMM was tested by application  in case studies, incorporating surveys and intervi ews, 
conducted with two organizations in different indus tries.  Tools such as case study protocols, the inc lusion of 
quantitative measures within surveys and the use of  the same researchers throughout the studies were u sed to 
improve reliability of data gathered.  For the KMCA , initial construct testing was accomplished by way  of pilot 
testing undertaken with a portion of one organizati onal business unit to determine model acceptability .  
Subsequent brainstorming sessions and a pre-test of  the model identified the need to assemble a focus group for 
each business unit to further assess model instrume nts.  Focus groups for each organizational business  unit were 
assembled in which the model was to be applied.  Th e purpose and design of these groups was to insure the 
understandability and relevance of the subsequent i nstrument.  
In addition to testing the model construct, it is n ecessary to test any assessment instruments for validity to ensure 
they measure what it was intended they measure and reliability  to ensure results obtained are accurate and 
repeatable.  In both models, the assessment instrum ent was a maturity assessment survey.  BPMM survey 
questions were validated by referencing existing li terature and by seeking agreement within a selected  group of 
domain experts.  The small number of survey respond ents within a given unit of analysis made pilot-tes ting 
difficult, resulting in questions being testing thr ough application of the survey in two organizations .  
Respondents were asked to comment on survey structu re, ease of survey completion, time for completion and 
perceived completeness of the questions.  KMCA ques tions were validated by utilizing portions of a pre viously 
validated instrument.  Additionally, due to the lar ge population available for survey administration, factor 
analysis was able to be utilized to insure converge nt and divergent validity (Freeze and Kulkarni 2005 ).  Also, as 
the KMCA survey was being administered to a large p opulation, a pilot group for testing the survey was  
considered critical.  Pilot groups were used to pre -test the survey instrument with the goal of insuri ng the 
relevance of the survey instrument and providing ap propriate examples within the organization or busin ess unit 
that relate to the domain components. The pilot gro up was selected to include individuals from the pop ulation to 
be assessed.   
Whilst the manner in which testing is undertaken ca n vary between models, inclusion of this phase in a  generic 
framework is supported by the vital role testing pl ayed in the development of the BPMM and KMCA models . 
Phase 5 - Deploy 
Following population and testing, the model must be  made available for use and to verify the extent of  the 
model’s generalisability.  To whom it is made avail able and in what manner can be addressed in two ste ps that 
will provide wider acceptance and improve the stand ardization of the model.  Deployment includes issue s such 
as initial organizational application and can consi der the design collaborators as primary respondents .  Where the 
model has been developed and tested utilising the r esources of an involved stakeholder (i.e. an indust ry, non-
profit or government entity) it is likely that the initial application of the model will be with this stakeholder.  This 
is the first step in determining the critical issue  of model generalisability and can lead to general acceptance of 
the model. However, until the model has been deploy ed to entities independent of the development and t esting 
activities, generalisability will continue to be an  open issue irrespective of whether the model has b een 
developed for a specific domain or for general appl ication.  Moving to the second step in deployment, it is 
necessary to apply the model within entities that a re independent of the model development.  For model s that 
were developed for specific domains where single or ganizational stakeholders were involved, the identi fication 
of similar firms in different markets may supply th e list of potential “next” administrations.  For mo dels 
developed in general domains where multiple organiz ational stakeholders existed, the use of consortium s for 
further application may be appropriate.  Depending on the original scope of model application, selecti on of a 
range of entities on the basis of industry, region,  sector, financial resources and employee numbers w ill assist in 
improving the generalisability of the model.  The i dentification of organizations that may benefit fro m future 
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 application of the maturity model and the ability t o apply the model to multiple entities provides the  final steps 
towards standardisation and global acceptance of th e developed model.     
Phase 6 - Maintain 
The goal of the maturity model impacts greatly the resources necessary to maintain the model’s growth and use.  
Success in establishing the generalisability of the  model requires that provisions be made to handle a  high 
volume of model applications.  This will necessitat e some form of repository in order to track model e volution 
and development.  Evolution of the model will occur  as the domain knowledge and model understanding 
broadens and deepens.  A model that provides prescr iptive actions to improve maturity must have the re sources 
available to track interventions longitudinally.  T his capability will further support the model’s sta ndardization 
and global acceptance.  The availability of resourc es to undertake such maintenance will also be deter mined to 
some degree by initial scoping.  For example, if a model is made available via a web-interface, resour ces will be 
required over time to ensure the interface is updat ed to reflect changes in the underlying assessment tools.  If 
software is developed to make the model available t o consultants for third-party assisted application,  software 
developers will need to update versions to reflect changes in the domain and technology.  If globalisa tion of the 
model is achieved and certification of model applie rs is required issues such as training material, ce rtification 
processes, and so on will need to be considered.     
The continued relevance of a model will be ensured only by maintaining the model over time. 
CONCLUSIONS  
This paper has proposed a generic methodology for t he development of maturity models in various domain s.  
The value in a generic methodology lies in the abil ity to develop a model that is highly generalisabil ity and 
enables standardization.  Use of a standard methodo logy enables a stable state of model development to  be 
reached and for incremental improvements to be made  over time.  The benefits to a domain of a model th at is 
both well-founded and well-based include: having an  ongoing source and accumulation of domain knowledg e, 
improving sustainability as organizations are bette r equipped for domain success and better understand ing 
relationships and influences that impact the domain .  The value to organizations of applying such a mo del lies in 
the ability to measure and assess domain capabiliti es at a given point in time.  This provides an orga nization with 
a better understanding of existing domain capabilit ies, enables benchmarking against a range of compet itors, 
enables greater efficiency in the utilization of re sources in improving domain capabilities and presen ts an 
opportunity for improved success in the domain.   
Limitations & Future Research 
The proposed development framework is limited in th at it is supported by the experiences of researcher s of only 
two models in the domains of Business Process Manag ement and Knowledge Management respectively.  
Application and support of the development framewor k by researchers in other domains will provide addi tional 
insight into the relevance and usefulness of the pr oposed framework.  The BPMM and the KMCA models are  
subject to ongoing research.  Future experiences fr om this research will be incorporated into the prop osed model 
development framework.   
REFERENCES 
Ahern, D. M., Clouse, A., & Turner, R. (2004). CMMI distilled: a practical introduction to integra ted process 
improvement  (2nd ed. ed.). Boston; London: Addison-Wesley. 
Bass, B. M. (1970). When Planning for Others. Journal of Applied Behavioral Science , 6(2), 151-171. 
Birkinshaw, J., & Sheehan, T. (2002). Managing the knowledge life cycle. MIT Sloan Management Review, 
44 (1), 75-83. 
Dalkey, N., & Helmer, O. (1963). An Experimental Ap plication of the Delphi Method to the Use of Expert s. 
Management Science, 9 (3), 458-467. 
Delbecq, A.L., Van de Ven, A. H., & Gustafson, D. H . (1975). Group techniques for program planning: A guide 
to nominal group and Delphi process . Glenview, IL: Scott-Foresman. 
Erffmeyer, R. C., Erffmeyer, E. S., & Lane, I. M. ( 1986). The Delphi Technique: An Empirical Evaluatio n of the 
Optimal Number of Rounds. Group and Organization Studies, 11 (1-2), 120-128.  
Fisher, D.M. (2004). The Business Process Maturity Model. A Practical Approach for Identifying Opportu nities 
for Optimization, URL http://www.bptrends.com/resou rces_publications.cfm, Accessed September 2005. 
16 th  Australasian Conference on Information Systems  Ma turity Assessment Model 
29 Nov – 2 Dec 2005, Sydney  de Bruin 
 
 Freeze, R. D., & Kulkarni, U. (2005). Knowledge man agement capability assessment: Validating a knowled ge 
assets measurement instrument. Proceedings of the Hawaii International Conference on System Sciences, 
HICCS-38 , Hawaii. 
Hakes, C. (1996) The Corporate Self Assessment Handbook, 3 rd  Edn , Chapman & Hall, London. 
Harmon, P. (2004). Evaluating an Organization's Bus iness Process Maturity, URL 
http://www.bptrends.com/resources_publications.cfm,  Accessed 30 September 2005. 
Loo, R. (2002). The Delphi method: a powerful tool for strategic management. Policing an International Journal 
of Police Strategies & Management, 25 (4), 762-769. 
Mutafelija, B., & Stromberg, H. (2003). Systematic process improvement using ISO 9001:2000 and CMMI . 
Boston: Artech House 
Murphy, M. K., Black, N. A., Lamping, D. L., McKee,  C. M., Sanderson, C. F. B, Askham, J., Marteau, T.  
(1998). Consensus development methods, and their us e in clinical guideline development. Health 
Technology Assessment, 2 (3). 
Okoli, C. & Pawlowski, S. D. (2004). The Delphi met hod as a research tool: an example, design consider ations 
and applications. Information  & Management, 42  , 15-29. 
Ono, R. & Wedemeyer, D. J. (1994). Assessing the Va lidity of the Delphi Technique. Futures 26 (3), 289-304. 
Paulk, M. C., Curtis, B., Chrissis, M. B., & Weber,  C. V. (1993). The Capability Maturity Model for Software, 
Version 1.1  (No. CMU/SEI-93-TR-24): Software Engineering Insti tute. 
Powell, C. (2003). The Delphi technique: myths and realities. Journal of Advanced Nursing, 41 (4), 376-382. 
Richards, J. I. & Curran, C. M. (2002). Oracles on "Advertising": Searching for a Definition. Journal of 
Advertising, 31 (2), 63-76. 
Rosemann, M. & de Bruin, T. (2004). Application of a Holistic Model for Determining BPM Maturity.  
Proceedings of the AIM Pre-ICIS Workshop on Process  Management and Information Systems, 
Washington, D.C., December, 46-60. 
Rosemann, M. & de Bruin, T. (2005). Towards a Busin ess Process Management Maturity Model. Proceedings of 
the 13 th  European Conference on Information Systems (ECIS 2 005). Regensburg, Germany, May.  
Rosemann, M., de Bruin, T. and Hueffner, T (2004). A Model for Business Process Management Maturity. 
Proceedings of the Australasian Conference on Infor mation Systems (ACIS 2004), Hobart, December.  
Satyadas, A., Harigopal, U., & Cassaigne, N. P. (20 01). Knowledge management tutorial: An editorial ov erview. 
IEEE Transactions on Systems Man and Cybernetics Pa rt C-Applications and Reviews, 31 (4), 429-437. 
Schmidt, R. C. (1997). Managing Delphi Surveys Usin g Nonparametric Statistical Techniques. Decision 
Science, 28 (3), 763-774. 
Spanyi, A. (2004). Towards Process Competence, URL http://www.bptrends.com/resources _publications.cfm , 
Accessed 30 September 2005. 
Turoff, M. (1970). The Design of a Policy Delphi. Technological Forecasting and Social Change, 2 , 149-171. 
Van De Ven, A. H. & Delbecq, A. L., (1974). The Eff ectiveness of Nominal, Delphi, and Interacting Grou p 
Decision Making Processes. Academy of Management Journal, 17 (4), 605-621. 
 Woudenberg, F. (1991). An Evaluation of Delphi. Technological Forecasting and Social Change, 40 , 131-150. 
Zack, M. H. (1999). Developing a knowledge strategy . California Management Review, 41 (3), 125-145. 
COPYRIGHT  
de Bruin, T., Rosemann, M, Freeze, R., and Kulkarni , U., and © 2005. The authors assign to ACIS and 
educational and non-profit institutions a non-exclu sive licence to use this document for personal use and in 
courses of instruction provided that the article is  used in full and this copyright statement is repro duced. The 
authors also grant a non-exclusive licence to ACIS to publish this document in full in the Conference Papers and 
Proceedings. Those documents may be published on th e World Wide Web, CD-ROM, in printed form, and on 
mirror sites on the World Wide Web. Any other usage  is prohibited without the express permission of th e 
authors.   
