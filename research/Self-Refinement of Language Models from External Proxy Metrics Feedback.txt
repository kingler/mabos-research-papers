Self-Refinement of Language Models from
External Proxy Metrics Feedback
Keshav Ramji1∗Young-Suk Lee2Ramón Fernandez Astudillo2Md Arafat Sultan2
Tahira Naseem2Asim Munawar2Radu Florian2Salim Roukos2
1University of Pennsylvania2IBM Research AI
keshavr@seas.upenn.edu
{ysuklee, tnaseem, raduf, roukos}@us.ibm.com
{ramon.astudillo, arafat.sultan, asim}@ibm.com
Abstract
It is often desirable for Large Language Mod-
els (LLMs) to capture multiple objectives
when providing a response. In document-
grounded response generation, for example,
agent responses are expected to be relevant
to a user’s query while also being grounded
in a given document. In this paper, we in-
troduce ProxyMetric-based Self-Refinement
(ProMiSe ), which enables an LLM to refine
its own initial response along key dimensions
of quality guided by external metrics feed-
back, yielding an overall better final response.
ProMiSe leverages feedback on response qual-
ity through principle-specific proxy metrics,
and iteratively refines its response one prin-
ciple at a time. We apply ProMiSe to open
source language models FLAN -T5-XXL and
LLAMA -2-13 B-CHAT , to evaluate its perfor-
mance on document-grounded question an-
swering datasets, MultiDoc2Dial and QuAC,
demonstrating that self-refinement improves
response quality. We further show that fine-
tuning LLAMA -2-13 B-CHAT on the synthetic
dialogue data generated by ProMiSe yields sig-
nificant performance improvements over the
zero-shot baseline as well as a supervised fine-
tuned model on human annotated data.
1 Introduction
The state-of-the-art large language models (LLMs)
have demonstrated to be effective in generating new
synthetic data, useful in improving zero-shot task
generalization through fine-tuning without requir-
ing vast amounts of human annotations. Various
approaches have been proposed to show the abil-
ity of models to evaluate and critique responses
(Saunders et al., 2022; Scheurer et al., 2023; Shinn
et al., 2023; Ye et al., 2023), as well as their poten-
tial to refine: given feedback, correct their outputs
(Welleck et al., 2022; Peng et al., 2023; Madaan
et al., 2023; Huang et al., 2023; Wang et al., 2023b).
These explorations have studied various feedbackmechanisms (human-in-the-loop, reward models
to capture human preferences, model-generated
feedback) and forms (pairwise comparisons, scalar
scores, natural language descriptions), as well as
refinement techniques (separate supervised refiners,
domain-specific refinement).
Of particular note are recent works exploring
the self-refinement phenomenon (Madaan et al.,
2023; Wang et al., 2023b; Shinn et al., 2023), lever-
aging the same LLM to perform critique and/or
refinement on top of generating responses. The
observations of these works unveil shortcomings:
smaller instruction-tuned models fail to replicate
the results of systems such as GPT-3.5 and GPT-4
in refinement, and in the absence of well-designed
stopping mechanisms, self-refinement applied to
high-quality responses can make the results worse
(Huang et al., 2023). When humans correct them-
selves, they do it often with one or more objec-
tives in mind, i.e. principles. Such principles may
include faithfulness, specificity, safety (i.e. non-
toxic), relevance to a question posed, etc. and may
vary across tasks — we seek to imbue these as-
pects into conversational agents, to ensure they are
reflected in the agent’s responses.
To this effect, we introduce an iterative,
principle-guided approach to self-refinement in rel-
atively smaller language models where refinement
has previously proven unsuccessful. Our algo-
rithm, termed ProxyMetric-based Self-Refinement
(ProMiSe ), combines proxy metric thresholding
for different principles with independent principle-
specific few-shot refinement and best-of-N rejec-
tion sampling. This allows for the deliberate se-
lection of task-appropriate metrics with calibrated
sufficiency thresholds, and specific prompts bet-
ter designed to match the instruction-following ca-
pabilities of smaller models. In this manner, we
perform multi-aspect self-refinement via iterative
∗Work done during internship at IBM Research AI.
1arXiv:2403.00827v1  [cs.CL]  27 Feb 2024
Figure 1: A high-level overview of our proposed self-refinement algorithm for content-grounded question answering,
with both initial response generation and iterative refinement performed with the same Large Language Model M.
single-aspect improvement queries, as opposed to
simultaneous refinement on many dimensions.
We apply this method to content-grounded ques-
tion answering, demonstrating consistent improve-
ments on a diverse set of evaluation metrics for
single-turn response generation. We then extend
ProMiSe to multi-turn dialogue data generation
to generate user queries in addition to agent re-
sponses. We fine-tune LLAMA -2-13 B-CHAT on the
synthetic data, yielding significant improvement
over the zero-shot baseline and supervised models
solely fined-tuned on human annotations.
Crucially, this approach is built on open-source
models and does not rely on propietary models
with black-box API access; we note, however, that
the proposed algorithm can be directly applied to
closed-source models as well. Furthermore, it can
be extended to other tasks, provided that proxy met-
rics can be defined and a few in-context exemplars
can be created for the relevant principles. Our key
contributions are:
•We introduce a novel domain-agnostic al-
gorithm, ProMiSe , to perform multi-aspect
self-refinement on desirable principles for a
response through in-context learning, using
proxy metrics as external quality feedback.
•ProMiSe is applied to both content-grounded
single-turn question answering and multi-turndialogue generation. Extensive evaluations
on MultiDoc2Dial and QuAC datasets with 5
automatic evaluation metrics (RougeL, Bert-
Recall, Bert-K-Precision, Recall, K-Precision)
as well as LLM-as-judge with GPT-4, demon-
strate its effectivenss both in few-shot and
fine-tuning settings. We will release both the
software and the synthetic dialogue data.
•We analyze the relationship between the
change in proxy metric scores and the down-
stream evaluation metrics, revealing an unsu-
pervised correlation and reinforcing the effi-
cacy of our method.
2 Algorithm
Given an input, e.g. a document and conversa-
tion history, ProMiSe executes three main steps:
(i) Generate an initial response, (ii) Obtain exter-
nal feedback via proxy metrics, and (iii) Refine
the response with respect to each principle, if the
response is deemed inadequate by the feedback
mechanism. The last two steps are run iteratively
until the response meets a quality threshold. We
present a detailed description of these steps below.
2.1 Initial Response Generation
For an input instance, we perform Best-of-N sam-
pling to yield a set of responses, Y0, from Lan-
2
Algorithm 1 ProMiSe Self-Refinement
Inputs: Model M;
x: Task Input (document, context, . . .);
P: User-defined set of principles;
T: Set of metrics corresponding to P;
i: Initial generation prompt;
rp: Refinement prompt for principle p;
τ= [τ1, τ2, . . . , τ |T |]: Metric i∈ T has threshold τi;
w= [w1, w2, . . . , w |T |]: Metric i∈ T has weight wi;
λ: Improvement threshold for weighted metric sum;
N: Number of initial responses generated per turn
Begin:
1:Y0={yn}N
n=1withyn∼pM(y|x, i)
2:y0= arg max
y∈Y0(
|T |P
t=11(arg max
y′∈Y0{mt(y′, x)}=y))
3:if|T |P
t=11(mt(y0, x)≥τt) =|T |then
4: return y0
5:else
6: foriteration j= 0,1, . . . J :do
7: Yj+1={yp}|P|
p=1withyp∼pM(y|yj, x, r p)
8: yj+1= arg max
y∈Yj+1(
|T |P
t=11(arg max
y′∈Yj+1{mt(y′, x)}=y))
9: if|T |P
t=11(mt(yj+1, x)≥τp) =|T |then
10: return yj+1
11: else if|T |P
t=1wt· 1(mt(yj+1, x)≥mt(yj, x))then
12: yj+1=yj
13: end if
14: end for
15: return yJ
16:end if
guage Model M, given the input and an initial
generation prompt. The initial generation prompt
consists of an instruction and optional in-context
demonstrations. The instruction explicitly suggests
that a response be generated which reflects desir-
able principles, the set of which is contained in P.
We determine the quality of the sampled responses
based on a set of proxy metrics determined a priori,
designated as T. We note that the selected metrics
should be designed by the user to improve align-
ment by reflecting the principle set for the response,
P, with respect to the current task. As such, each
metric mtis also predicated on the inputs provided
which may be used as means to assess candidate
responses – a text passage or document, conversa-
tion history, etc. (thus lending itself to the content-
grounded setting). Each responses in Y0is scored
with each metric mtinT, and the response with
the highest scores on the greatest number of metrics
is chosen as the best initial response, y0.
Next, we determine the global sufficiency of
y0as an acceptable response, by comparing theproxy scores element-wise against a threshold τ,
consisting of scalar values τ1, τ2, . . . , τ |T |.τiis
the minimum value such that a response is deemed
sufficient, for each metric iinT. If the scores of
y0exceeds their respective thresholds, for all T
components, we return it as the final response. If
not (i.e. y0fails to clear the threshold on at least
one metric), we proceed to the refinement module.
2.2 Response Refinement
Our approach to response refinement is predicated
on in-context exemplars of principle-specific refine-
ment for the given task. The refinement prompt also
contains the previous best response, denoted yj—
in the first iteration, this is equal to y0. Aligning re-
sponses with multiple principles (i.e. where P>1)
induces a multi-objective problem; rather than ex-
plicitly optimizing across the set simultaneously,
we propose deliberate refinement with respect to
one principle at a time, selecting an optimal candi-
date at each iteration based on the proxy metrics.
For each iteration in the self-refinement phase, we
loop through the set of principles Pand generate a
set of new responses, with the goal of each resulting
response reflecting improvement on its respective
principle. In each such query to Language Model
M, we introduce a principle-specific refinement
prompt, consisting of in-context demonstrations of
refinement and an instruction to improve the cur-
rent best response, both with respect to the current
principle. Examples of such prompts are contained
in Appendix C.
Determining Improvement We perform rejec-
tion sampling, this time on the set of refinement
candidates, scoring with each metric in Tand se-
lecting the response, yj+1, with the highest scores
on the majority of metrics. The scores of yj+1,
the best refinement candidate, are then compared
against the threshold τ. If the scores of yj+1exceed
the threshold on all |T |metrics, then we stop refine-
ment and accept it as the final response. Otherwise,
we now compare against the scores of the previ-
ous best response, yj. The user assigns weights
w= [w1, w2, . . . , w |T |]for the respective metrics
inT; these importances should likely be informed
by the principles in Pwhich each metric corre-
sponds to, and the user’s design goals. Then, given
the scores for yj+1andyj, we compute:
|T |X
t=1wt· 1(mt(yj+1, x)≥mt(yj, x))
3
For each metric in T, the indicator takes on a value
of 1 if the new response is an improvement on the
previous best response, with respect to that metric,
or 0 otherwise, and is weighted by the elements
inw. If this sum fails to exceed a user-defined
threshold of λ, we do not update the best refinement
response for this iteration (i.e. set yj+1=yj); else,
we proceed to the next refinement iteration, until
termination.
3 Evidence: Question Answering
We apply ProMiSe to content-grounded question
answering: given a document and a conversation
history, which may consist of a single user utter-
ance (a question posed to the conversational agent)
or a multi-turn dialogue between the user and the
agent, we seek for the LLM to produce a response
to the most recent user query.
3.1 Set of Principles
We first identify an appropriate set of principles for
the task, which define key characteristics of a good
agent response. They are as follows:
1.Specificity. If an initial response is too vague,
this would likely lead to more user interac-
tions asking the agent to make its response
more specific.
2.Faithfulness. We suggest that accurate, fac-
tual responses are those grounded in the doc-
ument, and thus should have high (semantic
and lexical) overlap with the document.
3.Relevance and Consistency. The conversa-
tional agent response should be relevant to the
most recent user query, and by induction to
the entire conversation history.
3.2 In-Context Demonstration Selection
We explore our algorithm through the generation
of both a single agent response and an entire multi-
turn dialogue. We include the algorithm for multi-
turn dialog generation in Appendix A.
Response and Query Generation. For the gen-
eration of an initial response consistent with the
content-grounded QA setting, we extract 3 in-
stances from the MultiDoc2Dial (Feng et al., 2021)
training data as in-context exemplars; the prompt
template is included in Appendix C. This includes
the document, conversation history, and the gold re-
sponse provided by the annotators. The in-contextexemplars for query generation work similarly,
with 3 demonstrations consisting of different con-
versation lengths (in number of utterances), but
where the last utterance is the final user query.
Principle Refinement. To perform in-context re-
finement on a particular principle, we similarly
take 3 in-context demonstrations from the train-
ing dataset, but seek to contrast between a better
and worse response, with respect to the principle.
To accomplish this, we manually annotate a worse
response for each instance relative to the gold re-
sponse. In the prompt, we model this as 3 separate
utterances: the worse agent response, a user turn
probing the agent to improve its response to update
along the principle, and another agent utterance
containing the better response (i.e. the gold re-
sponse). To more explicitly suggest the presence of
a response quality difference, we include the tags
“not {principle}” and “more {principle}”, for the
two agent turns, respectively, where {principle} is
either ‘specific’, ‘relevant’, or ‘accurate’.
3.3 External Proxy Metrics
To capture the aforementioned principles, we de-
fine relevant proxy metrics. The proxy metrics
should be reflective of response quality improve-
ment along our chosen dimensions and should not
directly optimize the final evaluation metrics.
ROUGE Metrics. We select three ROUGE met-
rics intended to correspond to each of the three
principles. ROUGE-1 recall between the response
and the document mostly represents specificity as
more specific answers contain more details from
the document. Next, we use ROUGE-L between
the response and the document — this primarily
addresses faithfulness, as a greater score would
suggest a more extractive answer, which is clearly
preferable to hallucinated facts. Finally, we com-
pute ROUGE-L between the response and the con-
versation history to capture consistency between
the user query and the response and relevance of
the response to the query history.
WeCheck: Factual Consistency Checker.
Given a candidate response and the grounding doc-
ument, WeCheck (Wu et al., 2022) addresses the
faithfulness principle.
Our experiments evaluate each model in three
thresholding settings: solely using the three afore-
mentioned ROUGE metrics, solely using the
WeCheck model, and using a combination of both.
If we use only WeCheck, rejection sampling is
4
PROXY METRICS STAGE ROUGE -L BERT-R ECALL BERT K-P REC. R ECALL K-P REC.
MD2D FLAN-T5-XXL (11B)
ONLY ROU-L + R OU-1 I NITIAL 21.55 28.11 40.42 32.34 76.77
FINAL 21.72 29.29 42.74 34.14 79.29
ONLY RM I NITIAL 22.33 28.91 44.58 33.61 81.29
FINAL 22.43 29.17 45.60 34.20 82.25
0-S HOT / ROU+ RM I NITIAL 22.30 28.94 44.55 33.68 81.56
FINAL 22.38 30.10 46.60 35.58 83.13
MD2D LLAMA -2-13B-C HAT
ONLY ROU-L + R OU-1 I NITIAL 19.31 28.92 34.44 38.45 70.33
FINAL 18.95 29.67 36.04 40.43 71.76
ONLY RM I NITIAL 19.97 29.65 34.33 38.07 70.08
FINAL 19.95 29.89 40.68 38.59 76.73
ROU+ RM I NITIAL 20.36 30.17 40.68 38.59 76.84
FINAL 20.06 30.64 41.46 40.00 77.43
QUACFLAN-T5-XXL (11B)
ONLY ROU-L + R OU-1 I NITIAL 41.57 40.70 43.31 44.87 87.57
FINAL 40.00 41.47 46.06 45.77 88.26
ONLY RM I NITIAL 37.01 36.58 48.82 40.21 91.51
FINAL 34.99 35.00 49.69 38.61 91.58
ROU+ RM I NITIAL 38.20 37.28 48.44 40.78 91.38
FINAL 35.85 37.13 50.91 41.21 91.89
QUACLLAMA -2-13B-C HAT
ONLY ROU-L + R OU-1 I NITIAL 31.36 35.12 40.79 42.86 83.08
FINAL 29.23 35.28 42.87 43.63 82.96
ONLY RM I NITIAL 29.83 33.11 46.64 39.22 87.78
FINAL 28.70 31.83 47.79 37.62 87.24
ROU+ RM I NITIAL 28.85 32.29 46.59 38.54 88.04
FINAL 26.76 32.39 48.05 39.64 88.11
Table 1: Experimental Results on the MultiDoc2Dial (MD2D) and QuAC test sets, containing 10,204 and 1,000
instances, respectively. Experiments are reported with the Flan-T5-XXL (11B) and Llama-2-13B-Chat models,
using 3 Rouge (ROU) measures, the WeCheck reward model (RM), and both in tandem for thresholding. "Initial"
refers to scoring generations after rejection sampling, while "Final" includes both "sufficient" initial responses and
post-refinement responses. In proxy metrics, Rouge-L includes computing between the candidate response with
both the grounding document and the given user query, and Rouge-1 is with respect to the document. Highest scores
are boldfaced for each model. We decode with sampling method by setting temperature=0.7, top-k=50 and top-p=1.
performed to yield the highest scoring response ac-
cording to WeCheck and we determine whether a
refined response constitutes an improvement solely
using the WeCheck scores. If using both ROUGE
and WeCheck, a sufficient response must clear the
threshold on all four metrics. During refinement,
we yield a reward indicator with each category
(Rouge and reward model, i.e. WeCheck) which is
1 if deemed to have improved during the present
iteration and 0 otherwise, and compute a weighted
sum using a user-defined weight vector w. If this
sum is greater than 0.5, we update the best response
to be the new one, else retain the previous best.
4 Experimental Results and Discussion
We use two widely-used open-source language
models to evaluate our algorithm for content-
grounded question answering, FLAN-T5-XXL(Chung et al., 2022) and LLAMA -2-13B- CHAT
(Touvron et al., 2023)
Evaluation Datasets. We evaluate the technique
on the test dataset of MultiDoc2Dial (Feng et al.,
2021) (https://doc2dial.github.io/multidoc2dial/),
content-grounded dialogues, and the valida-
tion dataset of QuAC (Choi et al., 2018)
(https://quac.ai/), short form question-answering.
Both datasets feature conversations wherein an-
swers to queries posed by the user are expected to
come from a document.1
Evaluation Metrics We use five automatic evalu-
ation metrics: ROUGE-L (Lin, 2004), BERTScore
1We use a sub-document split on MultiDoc2Dial, to re-
move the information retrieval (IR) component such that we
only have the most relevant document as opposed to the entire
set of candidate documents. We use the validation dataset of
QuAC as the test data since the testset is not publicly available.
5
FINE-TUNING DATA SYNSET SIZE ROUGE -L BERT-R ECALL RECALL BERT-K-P REC. K-P REC.
NONE (BASELINE ) 0 21.11 30.95 38.62 40.05 76.89
Synset 1-INITIAL 8K 24.63 28.51 34.21 41.91 78.26
Synset 1-FINAL 8K 26.13 33.65 39.49 46.98 82.74
Synset 2-INITIAL 10K 24.51 27.82 33.81 41.01 76.71
Synset 2-FINAL 10K 26.06 33.83 40.56 48.67 84.46
Synset 3-INITIAL 14K 26.18 30.43 34.81 41.73 79.08
Synset 3-FINAL 14K 26.78 33.57 38.26 46.32 83.68
HUMAN (BASELINE ) 55.32 56.43 56.75 31.38 75.22
HUMAN +Synset 1-FINAL 8K 55.40 57.24 57.79 32.53 76.13
HUMAN +Synset 2-FINAL 10K 55.58 57.18 57.63 31.79 75.46
HUMAN +Synset 3-FINAL 14K 55.00 56.92 57.56 32.78 75.87
Table 2: Effectiveness of the proposed refinement algorithm measured by the synthetic data qualities. We QLoRA
fine-tune LLAMA -2-13B-C HAT model on the two sets of synthetic multi-turn dialogues, one generated with the
refinement algorithm denoted by Synset x-FINAL , and the other generated without the refinement algorithm denoted
bySynset x-INITIAL .Synset 1includes 8k and Synset 2, 10k samples of 2 turn dialogues. Synset 3includes 10k
samples of 2 turn, 2k samples of 4 turn, and 2k samples of 6 turn dialogues. The upper portion of the table compares
the performance of the model fine-tuned on the synthetic data with the highest-scoring baseline without fine-tuning.
The lower portion of the table compares the performance of the model fine-tuned on the combination of human
annotated and synthetic data with the model fine-tuned on human annotated data only.
Recall, BERTScore K-Precision (K-Prec. here-
after), (Zhang et al., 2020), Recall, and K-Precision.
ROUGE-L, BERTScore Recall and Recall mea-
sure the agreement between the candidate response
and the provided gold response. BERTScore K-
Prec. and K-Prec. measure the agreement between
the candidate response and the grounding docu-
ment. We chose Recall and K-Prec. metrics due to
their strong correlation with human assessments of
instruction-following models in content-grounded
QA tasks, (Adlakah et al., 2023).
4.1 Single-Turn QA Results
Table 1 presents the results across the three possible
metric sets (ROUGE metrics, the WeCheck reward
model, and both) as defined in Section 3.3. It can
be observed that designating Tto be the combina-
tion of the three ROUGE metrics and the WeCheck
reward model yields improved results across the
majority of the metrics. We find that using the
WeCheck reward model as the sole sufficiency met-
ric yields less consistent improvement across the
set of evaluation metrics, yet boosts performance
when applied in tandem with ROUGE metrics.
To identify the appropriate sufficiency thresh-
old for the proxy metrics, we perform a rigorous
study of various settings, included in Appendix
B. Experiments containing ROUGE metrics main-
tain a sufficiency threshold of 0.02 for response-
document Rouge-1 Recall, 0.05 for response-
document Rouge-L and 0.05 for response-query
Rouge-L. Results involving the WeCheck rewardmodel use a threshold of 0.5 between the response
and document.
Initial Final
MultiDoc2Dial (Avg. Gold: 15.55)
Flan-T5-XXL ROU-Only 32.06 35.88
Flan-T5-XXL RM-Only 33.16 33.70
Flan-T5-XXL ROU + RM 33.00 36.09
Llama-2-13B-Chat ROU-Only 39.30 44.40
Llama-2-13B-Chat RM-Only 38.45 39.51
Llama-2-13B-Chat ROUGE + RM 38.56 42.59
QuAC (Avg. Gold: 12.57)
Flan-T5-XXL ROU-Only 17.40 20.46
Flan-T5-XXL RM-Only 18.19 19.07
Flan-T5-XXL ROU + RM 17.79 21.49
Llama-2-13B-Chat ROU-Only 29.61 33.41
Llama-2-13B-Chat RM-Only 27.73 27.39
Llama-2-13B-Chat ROU + RM 28.74 32.58
Table 3: Average word token counts for initial and final
generations with ProMiSe. Statistics are computed for
the three different settings of the proxy metric set, T;
RM is the WeCheck reward model.
In Table 3, we compare the average length (word
count) of the initial and final responses, for the Mul-
tiDoc2Dial and QuAC datasets. It can be observed
that the length of final responses is marginally
greater than the average initial response length.
This suggests that our performance improvements
exhibited in Table 1 are unlikely to be solely a re-
6
sult of longer responses (e.g. reproducing large sec-
tions of the document). Simultaneously, our model
producing longer responses relative to the gold re-
sponse likely explains slight declines in Rouge-L
scores with both models and both datasets; in par-
ticular, Llama-2’s responses are much longer than
Flan-T5’s and the gold response.
Analysis with Proxy Metrics We explore the
relationship between the improvement in the fi-
nal evaluation metrics and the direction of change
on the proxy metrics in ProMiSe. That is, is im-
provement on the proxy sufficiency metrics during
the execution of the algorithm correlated with the
downstream evaluation metric improvement from
initial to final response?
Count ROU-L Diff. BERT-R Diff.
ROU-Only
ROU 3 ↑ 376 +2.31% +8.26%
ROU 2 ↑ 128 +2.41% +6.81%
ROU 1 ↑ 69 +0.64% -1.74%
RM-only
RM↑ 169 +0.68% +1.68%
ROU + RM
ROU 3 ↑, RM↑ 110 +2.21% +8.23%
ROU 2 ↑, RM↑ 45 +0.25% +1.57%
ROU 1 ↑, RM↑ 59 +1.18% -0.66%
ROU 3 ↑, RM↓ 195 -1.40% +5.67%
ROU 2 ↑, RM↓ 50 +0.07% +4.25%
ROU 1 ↑, RM↓ 32 +3.37% +2.16%
Table 4: Analysis of the correlation between improve-
ment on proxy ROUGE (ROU) and WeCheck reward
model (RM) metrics with change in the final evaluation
ROUGE-L and BERT-Recall with the gold response.
Performed with Flan-T5-XXL on a 2,038 sample Mul-
tiDoc2Dial development set. Proxy metric scores are
computed between the candidate and either the provided
document or context. ↑and↓represents improvement
and decline, respectively. "2 ↑" means that two of the
proxy ROUGE metric set improved. The differences
reported are averaged across the sample count.
The relationship between the proxy metric scores
and the Rouge-L and BERTScore-Recall evalua-
tion metrics is shown in Table 4. We find that the
chosen proxy metrics appear to serve as an unsu-
pervised link to the final evaluation metrics. The
number of samples that improve for each proxy
metric change are roughly similar, a trend notice-
able across settings. Furthermore, a greater degree
of improvement on proxy metrics (e.g. improvingon all three ROUGE metrics) generally corresponds
to a larger average improvement (or less negative
change) for Rouge-L and BERTScore-Recall with
respect to the gold response. This highlights the
value of our external metric feedback technique:
by optimizing on a scoring scheme while simulta-
neously preserving the integrity of the downstream
evaluation metrics, we can capture a similar notion
of response quality and sufficiency.
4.2 Multi-Turn Synthetic Dialogues
We generate synthetic dialogues of varying lengths
from Flan-T5-XXL, containing refinement in-
stances: the initial response, a user query to im-
prove the response along a principle, and the re-
fined response. The dialogues are generated from
scratch, bootstrapping solely on the grounding doc-
uments in MultiDoc2Dial training data. They alter-
nate between user and agent utterances, and consist
of 1-3 agent responses (thus containing total 2, 4,
or 6 turns). We sampled 10k dialogues with 2 turns,
2k with 4 turns and another 2k with 6 turns. We
QLoRA fine-tune (Dettmers et al., 2023) Llama-
2-13B-Chat model on these synthetic data. See
Section D for details.
The results are shown in Table 2. We observe siz-
able improvements across all metrics when compar-
ing the performance without refinement, denoted
INITIAL , as opposed to with refinement, denoted
FINAL . Notably, these improvements are present
on both lexical and semantic similarity measures;
+6-6.75% for both BERT-Recall and Recall, and
+7.5-8% for BERT K-Precision and K-Precision.
Furthermore, merging the synthetic data with 38k
samples of human annotated data from the Multi-
Doc2Dial train set yields improvements over solely
training on human annotated data. These results
suggest the value of response quality refinement in
generating high-quality synthetic data and yielding
downstream improvements on evaluation metrics.
4.3 LLM-as-a-Judge Evaluation
We also perform automated evaluation with GPT-
4 as a judge, (Zheng et al., 2023), which has
been shown strongly correlate to human evalua-
tion. Given the initial and final generations, we
prompt the model to impartially assess which re-
sponse is better. We largely adapt the prompts used
for MT-bench evaluation in Zheng et al. (2023),
which we show in Appendix F. For MultiDoc2Dial,
we randomly sample 2,551 of the indices of the
test set responses (exactly one quarter), and only
7
Figure 2: GPT-4-as-a-Judge results on Flan-T5-XXL
for MultiDoc2Dial (MD2D) and QuAC. With 2551 ran-
domly sampled instances from the MultiDoc2Dial test
set, we examine those for which the initial and final
response differ: 495 samples for ROUGE-only, 131
samples for RM-only (WeCheck), and 504 samples for
ROUGE + RM. We perform a similar analysis with all
1000 QuAC test set instances; the respective counts are:
193 samples for ROUGE-only, 65 samples for RM-only,
and 224 samples for ROUGE + RM.
perform evaluation on samples for which the initial
and final responses differ as a result of refinement.
With the QuAC dataset, analyze all 1,000 test set in-
stances, likewise evaluating where initial and final
responses differ. The results are shown in Figure 2,
where the numbers in percentage are the win rate
of each response.
We find that GPT-4 deems the final response
to be better than the initial response on all condi-
tions for MultiDoc2Dial. The relative outlier is the
QuAC dataset with RM-only; this is likely because
WeCheck measures entailment rather than agree-
ment. Often the correct short response is less likely
to be entailed than an incorrect longer response by
the grounding document. However, a higher win
rate with the ROUGE + RM combination validates
the complementary nature of our proxy metrics.
Furthermore, the strong correlation between the
automatic evaluation metrics in Table 1 with the
GPT-4 evaluation results in Figure 2 evidences the
efficacy of our algorithm.
5 Other Related Work
Various work on self-refinement may be distin-
guished according to the source of feedback, (Pan
et al., 2023; Huang et al., 2023). Internal feed-back relies on the model’s inherent knowledge and
parameters to reassess its outputs. External feed-
back incorporates inputs from humans, other mod-
els. Our work is inspired by (Madaan et al., 2023).
Unlike Madaan et al. (2023), however, who rely
on very large LLMs (GPT-3.5, ChatGPT, GPT-4)
as the source of internal feedback, we introduce
external feedback with proxy metrics and enable
self-refinement technique to work with relatively
small LLMs including Flan-T5-XXL and Llama-2-
13B-Chat in content-grounded setups.
Regarding internal feedback, (Bai et al., 2022)
experiment with method for training a harmless AI
assistant through self-improvement. (Wang et al.,
2023a) propose Shepherd, a language model tuned
to critique its own responses and suggest refine-
ments. As for external feedback, (Paul et al., 2023)
propose REFINER , a framework for finetuning LMs
to explicitly generate intermediate reasoning steps
while interacting with a critic model that provides
automated feedback on the reasoning. (Gou et al.,
2023) propose CRITIC that interacts with appropri-
ate tools, e.g. calculator, search engine, wikipedia,
etc., to evaluate certain aspects of the text and
then revise the output based on the feedback ob-
tained during the validation process. (Olausson
et al., 2024) critically examines the LLM’s ability
to perform self-repair on problems taken from Hu-
manEval and APPS and concludes that self-repair
still lags behind what can be achieved with human-
level debugging. (Gao et al., 2023) propose RARR
(Retrofit Attribution using Research and Revision)
that revises a generated text on the basis of the
relevant evidence retrieved by re-search.
6 Conclusion
We present a novel algorithm, ProMiSe, for self-
refinement of language models. ProMiSe uses ex-
ternal multi-aspect feedback via proxy metrics cap-
turing desirable principles for a high-quality re-
sponse. ProMiSe is applied to content-grounded
single-turn question answering and multi-turn di-
alogue generation. Extensive evaluations on Mul-
tiDoc2Dial and QuAC datasets with 5 automatic
evaluation metrics as well as LLM-as-a-judge with
GPT-4, demonstrate its effectiveness in both few-
shot learning and supervised fine-tuning setups.
This approach crucially enables relatively small
LMs like Flan-T5-XXL and Llama-2-13B-Chat to
successfully perform self-refinement.
8
7 Limitations
Our work employs two open-source LMs: FLAN -
T5-XXL and LLAMA -2-13 B-CHAT . Therefore, the
generated data, including the synthetic multi-turn
dialogues, can be susceptible to the limitations
of such LMs, particularly the biases inherent in
the training data which may be harmful with hate,
abuse and social stereotypes. We have tested the
algorithm ProMiSe on English only although it
would have been more desirable to verify the value
of the algorithm in multi-lingual setups. We have
conducted extensive evaluations including 5 well-
known automtic evaluation metrics and LLM-as-a-
judge with GPT-4, which has been shown to cor-
relate well with human evaluations. Nonetheless,
inclusion of human evaluation would have strength-
ened our position further.
8 Ethics and Impact
Our technique can be used to guide generations to-
wards user-specified targets; however, this could be
applied to generate toxic or malicious content, by
way of an adversarial principle selection. Nonethe-
less, we note that ProMiSe does present meaningful
implications in enabling alignment to human pref-
erences (where preferences, in this setting, refer to
the user-defined principles). We will release the
software for the ProMiSe algorithm, enabling oth-
ers in the community to consider other principles
of interest, or applications to other tasks.
References
Vaibhav Adlakah, Parishad BehnamGhader, Xing Han
Lu, Nicholas Meade, and Siva Reddy. 2023. Eval-
uating correctness and faithfulness of instruction-
following models for question answering.
Yuntao Bai, Saurav Kadavath, Sadipan Kundu, Amanda
Askell, Jackson Kernion, Andy Jones, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Cameron McK-
innon, Carol Chen, Catherine Olsson, Christopher
Olah, Danny Hernandez, Dawn Drain, Deep Ganguli,
Dustin Li, Eli Tran-Johnson, Ethan Perrez, Jamie
Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau,
Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,
Michael Sellitto, Nelson Elhage, Nicholas Schiefer,
Noemi Mercado, Nova DasSarma, Robert Lasenby,
Robin Larson, Sam Ringer, Scott Johnston, Shauna
Kravec, Sheer El Showk, Stanislav Fort, Tamera Lan-
ham, Timothy Telleen-Lawton, Tom Conerly, Tom
Henighan, Tristan Hume, Samuel R. Bowman, Zac
Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas
Joseph, Sam McCandlish, Tom Brown, and Jared Ka-
plan. 2022. Constitutional ai: Harmlessness from ai
feedback.Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen
tau Yih, Yejin Choi, Percy Liang, and Luke Zettle-
moyer. 2018. Quac : Question answering in context.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
bert Webson, Shixiang Shane Gu, Zhuyun Dai,
Mirac Suzgun, Xinyun Chen, Aakanksha Chowdh-
ery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,
Dasha Valter, Sharan Narang, Gaurav Mishra, Adams
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-
cob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,
and Jason Wei. 2022. Scaling instruction-finetuned
language models.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms.
Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra
Joshi. 2021. MultiDoc2Dial: Modeling dialogues
grounded in multiple documents. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing , pages 6162–6176, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent
Zhao, Ni Lao, Hongrae Lee, and Da-Cheng Juan.
2023. Rarr: Researching and revising what language
models say, using language models. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics, Volume 1: Long Papers ,
pages 16477–16508.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong
Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
2023. Critic: Large language models can self-correct
with tool-interactive critiquing. arXiv preprint
arXiv:2305.11738 .
Jie Huang, Xinyun Chen, Swaroop Mishra,
Huaixiu Steven Zheng, Adams Wei Yu, Xiny-
ing Song, and Denny Zhou. 2023. Large language
models cannot self-correct reasoning yet.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Text Summarization
Branches Out , pages 74–81.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Sean Welleck, Bodhisattwa Prasad Majumder,
Shashank Gupta, Amir Yazdanbakhsh, and Peter
Clark. 2023. Self-refine: Iterative refinement with
self-feedback.
Theo X Olausson, Jeevana Priya Inala, Chenglong
Wang, Jianfeng Gao, and Armando Solar-Lezama.
2024. Is self-repair a silver bullet for code genera-
tion? In Proceedings of ICLR 2024 .
9
Liangming Pan, Michael Saxon, Wenda Xu, Deepak
Nathani, Xinyi Wang, and William Yang Wang. 2023.
Automatically correcting large language models: Sur-
veying the landscape of diverse self correction strate-
gies. arXiv preprint arXiv:2308.03188 .
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-
riz Borges, Antoine Bosselut, Robert West, and
Boi Faltings. 2023. Refiner: Reasoning feedback
on intermediate representations. arXiv preprint
arXiv:2304.01904 .
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check
your facts and try again: Improving large language
models with external knowledge and automated feed-
back.
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
Long Ouyang, Jonathan Ward, and Jan Leike. 2022.
Self-critiquing models for assisting human evalua-
tors.
Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak,
Jun Shern Chan, Angelica Chen, Kyunghyun Cho,
and Ethan Perez. 2023. Training language models
with language feedback at scale.
Noah Shinn, Federico Cassano, Edward Berman, Ash-
win Gopinath, Karthik Narasimhan, and Shunyu Yao.
2023. Reflexion: Language agents with verbal rein-
forcement learning.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models.
Tianlu Wang, Ping Ya, Xiaoqing Ellen Tan, Sean
O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu,
Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-
Zarandi, and Asli Celikyilmaz. 2023a. Shepherd: A
critic for language model generation. arXiv preprint
arXiv:2308.04592 .Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan
Li, Hongkun Yu, and Heng Ji. 2023b. Enable lan-
guage models to implicitly learn self-improvement
from data.
Sean Welleck, Ximing Lu, Peter West, Faeze Brah-
man, Tianxiao Shen, Daniel Khashabi, and Yejin
Choi. 2022. Generating sequences by learning to
self-correct.
Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian
Li, and Yajuan Lv. 2022. Wecheck: Strong factual
consistency checker via weakly supervised learning.
arXiv preprint arXiv:2212.10057 .
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong
Kim, Hyeonbin Hwang, and Minjoon Seo. 2023.
Selfee: Iterative self-revising llm empowered by self-
feedback generation. Blog post.
Chen Zhang, Luis Fernando D’Haro, Yiming Chen,
Malu Zhang, and Haizhou Li. 2024. A comprehen-
sive analysis of the effectiveness of large language
models as automatic dialogue evaluators. In AAAI-
2024 .
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-
ating text generation with bert. In ICLR 2020 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena.
10
A Self-Refinement Algorithm for Synthetic Dialogue Generation
Algorithm 2 Synthetic Dialogue Generation with ProMiSe
Inputs: Model M;
d: Input document;
c=∅: conversation history
P: User-defined set of principles;
T: Set of metrics corresponding to P;
i: Initial generation prompt;
rp: Refinement prompt for principle p;
q: Query generation prompt;
u: User utterance to make the response more {principle };
τ= [τ1, τ2, . . . , τ k]: Quality metric ihas threshold τi;
w= [w1, w2, . . . , w k]: Quality metric ihas weight wi;
λ: Improvement threshold for weighted metric sum;
N: Number of initial responses generated per turn
Until Max Turns Reached:
1:Q ∼pM(y|d, c, q )
2:c=c∪ {Q}
3:Y0={yn}N
n=1withyn∼pM(y|d, c, i )
4:y0= arg max
y∈Y0(
|T |P
t=11(arg max
y′∈Y0{mt(y′, d, c)}=y))
5:if|T |P
t=11(mt(y0, x)≥τt) =|T |then
6:c=c∪ {y0}
7:else
8: foriteration j= 0,1, . . . J :do
9: Yj+1={yp}|P|
p=1withyp∼pM(y|yj, d, c, r p)
10: yj+1= arg max
y∈Y j+1(
|T |P
t=11(arg max
y′∈Y j+1{mt(y′, x)}=y))
11: if|T |P
t=11(mt(yj+1, d, c)≥τp) =|T |then
12: c=c∪ {yj} ∪ {u} ∪ {yj+1}
13: break
14: else if|T |P
t=1wt· 1(mt(yj+1, d, c)≥mt(yj, d, c))then
15: yj+1=yj
16: end if
17: end for
18: c=c∪ {u} ∪ {yJ}
19:end if
We include a complete version of Algorithm 1 adapted for synthetic dialogue generation, leveraged
in our fine-tuning experiments. At first, we sample a new user query from large language model M,
bootstrapping only off of the document. As the total number of turns (utterances) to be modeled is
user-defined, we append each utterance to the end of the conversation history. For example, given the
last user query, we append an agent response to it, which is either an initial (no refinement necessary) or
final (post-refinement) response. If refinement did occur, then we first append the previous best agent
response ( yj), then a user turn uof "User: Please make this response more {principle }", and then the
improved and sufficient agent response yj+1. Once an agent response has been procured and appended
for the current turn, we continue back to line 1 and generate a new user query, this time conditioning on
the conversation history as well; this repeats until the user-specified max turn limit is reached.
11
B Metric Selection and Threshold Calibration
THRESHOLDING STAGE ROUGE -L BERT-R ECALL BERT K-P REC. R ECALL K-P REC.
ROUGE -1 K-P REC.≥0.7 INITIAL 22.06 27.60 39.45 34.1 76.08
FINAL 22.21 28.19 40.29 34.57 77.23
ROUGE -1 K-P REC.≥0.8 INITIAL 23.06 28.65 39.65 35.42 76.49
FINAL 23.26 29.36 41.00 36.34 76.68
ROUGE -1 K-P REC.≥0.9 INITIAL 22.71 29.08 40.79 35.62 75.58
FINAL 22.79 29.67 42.40 36.59 78.63
WECHECK≥0.4 INITIAL 24.00 29.87 44.02 36.45 81.51
FINAL 23.93 30.02 44.53 36.67 82.08
WECHECK≥0.5 INITIAL 23.84 30.37 44.18 36.45 81.58
FINAL 23.89 30.51 44.99 36.69 82.40
WECHECK≥0.6 INITIAL 24.37 30.19 44.27 36.48 81.31
FINAL 24.23 30.17 45.08 36.72 81.95
WECHECK≥0.7 INITIAL 23.85 30.05 44.48 36.27 81.75
FINAL 23.82 30.38 45.64 37.00 82.63
WECHECK≥0.8 INITIAL 24.08 29.93 44.13 36.03 81.75
FINAL 24.07 30.31 45.41 36.74 83.00
ALL3 ROUGE + W ECHECK≥0.4INITIAL 23.97 30.12 44.41 36.45 81.96
FINAL 24.17 31.57 46.62 38.67 83.32
ALL3 ROUGE + W ECHECK≥0.5INITIAL 24.02 30.08 44.28 36.86 81.29
FINAL 24.05 31.29 46.40 38.74 82.81
ALL3 ROUGE + W ECHECK≥0.6INITIAL 24.01 29.91 44.55 36.07 81.50
FINAL 23.90 31.11 46.80 38.05 83.26
ALL3 ROUGE + W ECHECK≥0.7INITIAL 23.90 30.06 44.02 36.49 81.49
FINAL 24.17 31.62 46.22 38.88 83.00
ALL3 ROUGE + W ECHECK≥0.8INITIAL 23.79 29.94 44.47 36.49 81.84
FINAL 23.35 31.08 47.00 38.37 83.57
Table 5: Threshold calibration and metric selection was performed on a development (validation) set split of the
MultiDoc2Dial dataset, consisting of 2,038 samples. Experiments are reported with the Flan-T5-XXL (Chung et al.,
2022) model. Note that Rouge-L between response and document (Rouge-L-Doc) as well as between response
and the user query (Rouge-L-Query) are maintained constant, while we experiment with changing the third metric
between Rouge-1 F1, Rouge-1 K-Precision, and Rouge-1 Recall. We also vary the threshold for the WeCheck
reward model (Wu et al., 2022), in isolation and in tandem with the best performing Rouge metric combination.
ROUGE Metric Thresholding. We explore a plethora of different thresholding settings to calibrate
sufficiency and select the metric set Taccordingly. Note that "Rouge-1 K-Prec. ≥0.7", refers to using
Rouge-1 K-Precision with a threshold of 0.7, and Rouge-L F1 of the response with respect to both the
document and the user query with a threshold of 0.05. Then, maintaining the latter two with the same
configuration, we vary the first metric, exploring the use of a Rouge-1 K-Precision measure in place of
recall and experimenting with thresholds of 0.7, 0.8, and 0.9.
WeCheck and Combo Thresholding. We also vary the threshold for the WeCheck reward model
when applied as a standalone sufficiency metric, from 0.4 to 0.8 in increments of size 0.1. Finally, using
the chosen combination of the three ROUGE metrics (i.e. Rouge-1 Recall with 0.02, Rouge-L F1 with
0.05 for both response-document and response-query comparison), we vary the WeCheck threshold,
yielding a very interesting set of results. Across the majority of metrics, our self-refinement is effective
for all thresholding methods applied, although it manifests to different degrees depending on the set
of proxy metrics. Notably, the similarity in performance across threshold levels (i.e. not exhibiting a
clear trend correlating to an increase in threshold) allows our algorithm to more effectively serve as a
means of user-defined risk control with respect to a target refinement rate α. As noted in Section 2, a
12
greater threshold results in a higher standard for the initial response to meet, thus yielding a higher rate of
refinement as more responses are deemed inadequate.
C Initial Response Generation, Query Generation, and Principle Refinement Prompts
C.1 Initial Generation Prompt
Provided is a dialog between two speakers, User and Agent. Generate a response
that is coherent with the dialog history and the provided document. Desired traits
for responses are: 1) Specific - The response contains specific content, and
2) Accurate - The response is correct and factual with respect to the document.
document: DIAL-IN search accounts#3_0Log On to DIAL - IN [1 ] \n\nWhat business
records must I keep to document the searches I perform? \nThe business records you
keep must exist prior to the search you perform and must establish the business
purpose of the search.
...
To verify your browser is compatible to continue using any of the state 's government
websites, please visit https://encryption.ny.gov/ [6]. If your browser is not
currently compatible , please update it to the newest version.
context: User: I need to know how to pay the dial-in search account fees.
Agent: The custoers must pay a deposit with the application, and it should be enough
to pay for two months of searches. Was your application accepted?
User: Yes, it was.
Agent: then, your deposit will be added to your new account
balance.
User: Can you tell me some of the organizations that are exempt from the search fees?
Agent: Some of the exempted organizations are any public organization, its officers,
a volunteer fire company, volunteer ambulance service, etc. These organizations are
exempt from the search fee.
User: What to do in case none of the users performed a search that the DMV contacted
me for?
Agent: You should contact the DMV immediately.
User: Why would the DMV contact me about a search?
Agent: The DMV may contact you to ask you about a search to make sure you comply with
the Dial-In Terms of Service.
###
...
13
...
document: \n\nBenefits Planner: Family Benefits \nWhen you start receiving
disability benefits , certain members of your family may also qualify for benefits
on your record. Benefits may be paid to your : spouse; divorced spouse ; children;
disabled child ; and adult child disabled before age 22.
...
Find out more about Benefits For A Disabled Child. \n\nPublications \nDisability
Benefits Benefits For Children What You Need To Know When You Get Social Security
Disability Benefits Information for Government Employees Benefits For Children
With Disabilities
context: User: Oh, hi. Please, i 'm looking for some info about family benefits.
could you help me out?
Agent: Are you currently receiving any disability benefits?
User: Yeah, i started to receive it recently.
Agent: Well, in that case, i can tell you that some members of your family may also
qualify to get benefits on your record.
###
document: \n\nExposure through Project 112 or Project SHAD \nIf you were a part of
chemical and biological warfare testing through Project 112 or Project Shipboard
Hazard and Defense SHAD , you may be at risk for certain illnesses. The Department
of Defense s Deseret Test Center in Fort Douglas, Utah, conducted this testing,
which took place aboard ships and on land in various locations from 1962 to 1974.
Find out if you can get disability compensation or benefits. \n\nCan I get
disability benefits from VA? \nYou may be able to get disability benefits if you
meet both of the requirements listed below.
...
Get declassified Department of Defense fact sheets If you have a question about the
tests , if you have any information that can help show you were part of them
including whether you may have been part of them or contact the Department of
Defense at 800 - 497 - 6261.
context: User: I wanted more information on VA benefits and project 112
Agent: Were you part of chemical and biological warfare testing through Project 112
or Project Shipboard Hazard and Defense SHAD?
Figure 3: Above is the initial generation prompt, containing the instruction and the three in-context exemplars drawn
from the train set of MultiDoc2Dial (Feng et al., 2021), omitting the current sample inputs (document and context).
The exemplars demonstrate question answering given the conversation history, and are separated by "###".
14
C.2 Query Generation Prompt for Synthetic Dialogue Generation
Provided is a dialog between two speakers, User and Agent. Generate a new question,
posed by the user, that is coherent with the dialog history and contains
specfic content.
document: \n\nBenefits Planner: Family Benefits \nWhen you start receiving
disability benefits , certain members of your family may also qualify for benefits
on your record. Benefits may be paid to your : spouse; divorced spouse ; children;
disabled child ; and adult child disabled before age 22.
...
Find out more about Benefits For A Disabled Child. \n\nPublications \nDisability
Benefits Benefits For Children What You Need To Know When You Get Social Security
Disability Benefits Information for Government Employees Benefits For Children
With Disabilities
context: User: Oh, hi. Please, i 'm looking for some info about family benefits.
could you help me out?
Agent: Are you currently receiving any disability benefits?
User: Yeah, i started to receive it recently.
###
document: NY State Adventure License FAQs#3_0\n\n7. Is there an additional fee to
have icons added to my DMV photo document? \nThere are no additional fees if you
request the icons be added at the time of your photo document renewal.
...
For Boating Safety Certificate and Empire Passport holders , contact Parks via their
website: www.parks.ny.gov [3]. For Lifetime Sportsman, Small / Big Game, Bow
Hunting, Trapping, Muzzle Loading, or Fishing, contact DEC via their website :
www.dec.ny.gov [4].
...
15
...
context: User: I have a restricted use license issued in NJ and need information
about driving in NY.
Agent: Do you meet NY requirements for obtaining a restricted license?
User: Yes, I do.
Agent: Great. You can receive a restricted license to drive in NY. The restrictions
will be the same as the same as the restrictions for a driver with a NY driver license.
User: Where can I apply for the restricted driver license?
###
document: \n\nFamily Servicemembers Group Life Insurance (FSGLI) \nFamily SGLI, also
known as Family Servicemembers Group Life Insurance FSGLI, offers coverage for the
spouse and dependent children of service members covered under full - time SGLI.
...
If your service member is part of the Public Health Service , you ll need to fill out the
Spouse Coverage Election and Certificate SGLV 8286A and have them turn it in to their
unit s personnel officer. Download the Spouse Coverage Election and Certificate PDF
context: User: How much will my service member pay for dependent coverage?
Agent: Nothing.s We provide dependent coverage at no cost until the child is
18 years old , or sometimes longer if the child meets one of the requirements
listed below
User: To continue receiving dependent coverage after age 18, what are the requirements?
Figure 4: Query generation prompt (q in Appendix A’s algorithm), containing an instruction and three in-context
demonstrations of user queries given a document (separated by "###"), omitting the current instance inputs.
C.3 Principle Refinement Prompt Template
document: {document}
context: {context}
Agent response 1 (not {principle}): {less_principle_response}
Let's make this response more {principle}.
Agent response 2 (more {principle}): {more_principle_response}
16
C.4 Specificity Principle Refinement Prompt with In-Context Exemplars
We want to improve the previous response to make it more specific. To aid in this
process, we provide examples of incremental improvement on specific, where Agent
response 2 is more specific than Agent response 1.
document: DIAL-IN search accounts#3_0Log On to DIAL - IN [1 ] \n\nWhat business
records must I keep to document the searches I perform? \nThe business records you
keep must exist prior to the search you perform and must establish the business
purpose of the search.
...
To verify your browser is compatible to continue using any of the state 's government
websites , please visit https://encryption.ny.gov/ [6]. If your browser is not
currently compatible , please update it to the newest version.
context: User: I need to know how to pay the dial-in search account fees.
Agent: The custoers must pay a deposit with the application, and it should be enough
to pay for two months of searches.
Agent: Was your application accepted?
User: Yes, it was.
Agent: then, your deposit will be added to your new account balance.
User: Can you tell me some of the organizations that are exempt from the search fees?
Agent: Some of the exempted organizations are any public organization, its officers,
a volunteer fire company, volunteer ambulance service, etc. These organizations are
exempt from the search fee.
User: What to do in case none of the users performed a search that the DMV contacted
me for?
Agent: You should contact the DMV immediately.
User: Why would the DMV contact me about a search?
Agent response 1 (not specific): The DMV may contact you about a search to ensure
compliance.
Let's make this response more specific.
Agent response 2 (more specific): The DMV may contact you to ask you about a search to
make sure you comply with the Dial-In Terms of Service.
###
...
17
...
document: \n\nBenefits Planner: Family Benefits \nWhen you start receiving
disability benefits , certain members of your family may also qualify for benefits
on your record. Benefits may be paid to your : spouse; divorced spouse ; children;
disabled child ; and adult child disabled before age 22.
context: User: Oh, hi. Please, i 'm looking for some info about family benefits.
could you help me out?
Agent: Are you currently receiving any disability benefits?
User: Yeah, i started to receive it recently.
Agent response 1 (not specific): Then, some others may qualify for benefits.
Let's make this response more specific.
Agent response 2 (more specific): Well, in that case, i can tell you that some
members of your family may also qualify to get benefits on your record.
###
document: \n\nExposure through Project 112 or Project SHAD \nIf you were a part of
chemical and biological warfare testing through Project 112 or Project Shipboard
Hazard and Defense SHAD , you may be at risk for certain illnesses.
...
Get declassified Department of Defense fact sheets If you have a question about the
tests , if you have any information that can help show you were part of them including
whether you may have been part of them or contact the Department of Defense at
800 - 497 - 6261.
context: User: I wanted more information on VA benefits and project 112
Agent response 1 (not specific): Were you part of Project 112 or Project SHAD?
Let's make this response more specific.
Agent response 2 (more specific): Were you part of chemical and biological warfare
testing through Project 112 or Project Shipboard Hazard and Defense SHAD?
Figure 5: Refinement prompt ( rp) with respect to the specificity principle, with an instruction and three in-context
demonstrations. Our instruction explicitly suggests to improve on specificity, and that in the provided in-context
exemplars, the latter response (Agent response 2) is a specificity improvement over the former response (Agent
response 1). Notably, we demonstrate to the model that "Let’s make this response more specific" is an utterance in
between the worse and better responses. Each exemplar and the "more specific" (gold) response is derived from the
MultiDoc2Dial train set, while the "not specific" response is developed by a human annotator, bootstrapping off the
gold response. The three exemplars are separated by "###", and the above prompt omits the current instance inputs.
18
D Fine-tuning Experimental Setup
We QLoRA fine-tune LLAMA -2-13 B-CHAT on both human annotated and synthetically generated Multi-
Doc2Dial datasets. We set the learning rate to 1e-5, LoRA rank to 8 and LoRA alpha to 32. We apply 4bit
quantization for both model training and inferencing. Unlike baseline model inferencing with few-shot
learning for which we use sampling method, we use greedy decoding for fine-tuned models.
We train the models with 4 A100 (80GB memory) GPUs up to 10 epochs. Training takes between 5
hours for 8k training samples and 24 hours for about 50k samples. We select the best checkpoint on the
basis of the 5 evaluation metrics (RougeL, BERTScore Recall, BERTScore K-Prec., Recall and K-Prec.)
scores on the development test data.
E Zero-Shot vs Few-Shot Comparison
INITIAL EXEMPLARS + M ETRICS STAGE ROUGE -L BERT-R ECALL BERT K-P REC. R ECALL K-P REC.
FLAN-T5-XXL (11B) R ESULTS
3-S HOT / ONLY ROU-L + R OU-1 I NITIAL 21.50 27.27 38.08 31.19 75.58
FINAL 21.84 28.96 41.41 33.96 78.78
0-S HOT / ONLY ROU-L + R OU-1 I NITIAL 21.55 28.11 40.42 32.34 76.77
FINAL 21.72 29.29 42.74 34.14 79.29
3-S HOT / ONLY RM I NITIAL 22.67 28.80 42.83 33.00 80.42
FINAL 22.65 28.91 43.55 33.31 81.14
0-S HOT / ONLY RM I NITIAL 22.33 28.91 44.58 33.61 81.29
FINAL 22.43 29.17 45.60 34.20 82.25
3-S HOT / ROU+ RM I NITIAL 22.61 28.75 42.60 32.75 80.36
FINAL 22.71 29.89 44.86 34.76 81.98
0-S HOT / ROU+ RM I NITIAL 22.30 28.94 44.55 33.68 81.56
FINAL 22.38 30.10 46.60 35.58 83.13
LLAMA -2-13B-C HAT RESULTS
3-S HOT / ONLY ROU-L + R OU-1 I NITIAL 20.63 29.35 34.32 36.49 71.03
FINAL 20.23 30.22 36.07 38.82 72.74
0-S HOT / ONLY ROU-L + R OU-1 I NITIAL 19.31 28.92 34.44 38.45 70.33
FINAL 18.95 29.67 36.04 40.43 71.76
3-S HOT / ONLY RM I NITIAL 21.50 30.35 39.20 36.89 76.29
FINAL 21.25 30.11 39.51 36.85 76.53
0-S HOT / ONLY RM I NITIAL 19.97 29.65 34.33 38.07 70.08
FINAL 19.95 29.89 40.68 38.59 76.73
3-S HOT / ROU+ RM I NITIAL 21.48 30.29 39.15 36.79 76.34
FINAL 21.11 30.95 40.05 38.62 76.89
0-S HOT / ROU+ RM I NITIAL 20.36 30.17 40.68 38.59 76.84
FINAL 20.06 30.64 41.46 40.00 77.43
Table 6: Experimental Results on MultiDoc2Dial test set, containing 10,204 instances. Experiments are reported
with the Flan-T5-XXL and Llama-2-13B-Chat models, using 3 Rouge (ROU) measures, the WeCheck reward model
(abbreviated as RM), and both in tandem for sufficiency thresholding. Highest scores are boldfaced for each model.
The zero-shot results are the same as those included in Table 1; this table presents a comparison between 0-shot and
3-shot performance, for each metric set.
In Table 6, we also present a comparison based on the number of few-shot exemplars employed in initial
response generation. Notably, we observe that the 0-shot performance of initial responses are, in fact,
higher than the 3-shot results for the same phase. This suggests that instruction-tuned LMs such as
Flan-T5-XXL are already fairly adept at dialogue response generation without in-context exemplars.
Furthermore, we find that the zero-shot setting achieves higher initial scores relative to 3-shot for Flan-T5-
XXL, but such improvement is less consistent for Llama-2-13B-Chat. Simultaneously, we found that the
3-shot results with refinement constitute an improvement over the 0-shot performance. This indicates that
in-context exemplars are necessary to improve performance during the refinement phase, although only
three demonstrations are sufficient to illustrate the notion of the target principle to the LM. That is, despite
fairly coherent initial responses, there is still room for improvement, achieved when using three in-context
exemplars per principle. Thus, the results reported above and in Table 1 hold 3 exemplars constant for the
refinement phase.
19
F LLM-as-a-Judge Evaluation Setup
Recent literature (Zheng et al., 2023; Zhang et al., 2024) evidences the ability of using language models
as discriminators, judging generation quality in lieu of (or as a supplement to) human evaluation feedback.
The line of work has also been a strong motivator in influencing self-feedback and refinement approaches;
it demonstrates the ability of powerful models to reflect human preferences and provide meaningful
critiques. In pursuing such an approach, we deliberately choose to explicitly model certain properties in
the instruction: for example, we seek permutation-invariance (while knowing that models are susceptible
to position bias when given a set of multiple choice options and mitigating their preference for longer
answers.
F.1 Judge Prompt Template
Please act as an impartial judge and evaluate the quality of the responses provided by
the two AI assistants to the user question displayed below. Your evaluation should
consider correctness and helpfulness. You will be given a reference document, a user
conversation, assistant A 's answer, and assistant B 's answer. Your job is to evaluate
which assistant 's answer is better based on the information in the reference document
and the user conversation so far. Begin your evaluation by comparing both assistants '
answers with the document and the user conversation so far. Identify and correct any
mistakes. Avoid any position biases and ensure that the order in which the responses
were presented does not influence your decision. Do not allow the length of the
responses to influence your evaluation. Do not favor certain names of the assistants.
Be as objective as possible. After providing your explanation, output your final verdict
by strictly following this format: "[[A]]" if assistant A is better, "[[B]]" if
assistant B is better, and "[[C]]" for a tie.
[User Document]
{document}
[User Conversation]
{conversation}
[The Start of Assistant A 's Answer]
{answer_a}
[The End of Assistant A 's Answer]
[The Start of Assistant B 's Answer]
{answer_b}
[The End of Assistant B 's Answer]
Figure 6: LLM-as-a-Judge prompt template for automated response evaluation between initial and final generations
in the content-grounded question answering setting. A document and conversation are given as an input for each
sample, and we compare two possible agent responses to the most recent user query posed in the conversation.
20
