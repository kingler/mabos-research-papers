Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework

Ruochen Zhao1, Xingxuan Li1,2y, Shaﬁq Joty1,3z, Chengwei Qin1, Lidong Bing2
1Nanyang Technological University, Singapore
2DAMO Academy, Alibaba Group
3Salesforce AI
{ruochen002, chengwei003}@e.ntu.edu.sg
{xingxuan.li, l.bing}@alibaba-inc.com
srjoty@ntu.edu.sg

Abstract

As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks. For reproducing our results and extending the framework further, we make our codebase available at https://github.com/RuochenZhao/Verify-and-Edit

1 Introduction

Large Language Models (LLMs) have become the new norm in many downstream NLP tasks. In utilizing these LLMs, Chain-of-Thought (CoT) prompting (Wei et al., 2022) is found to improve performances for tasks that require complex reasoning, such as math word problems, commonsense reasoning, and symbolic manipulation. At the same time, it is able to generate interpretable reasoning chains. Recent work further explored how to use these reasoning chains to select better predictions. However, the primary focus of these methods has been to improve end-task performance by utilizing generated CoTs as-is. For example, Ye and Durrett (2022) train a calibrator that tunes prediction probabilities based on rationale scores; Wang et al. (2022) sample multiple reasoning paths to ﬁnd the most common (consistent) prediction. Only a few, such as Creswell et al. (2022) and Zhou et al. (2022), have explored ways to improve the quality of CoTs themselves.

In fact, improving the CoT quality could be beneﬁcial in enhancing both interpretability and end-task performance. Ye and Durrett (2022) point out that explanations judged as good by humans often indicate more accurate predictions. Intuitively, a better set of CoT prompts could provide better grounding and logically consistent thought processes, thus leading to more accurate predictions.

To improve generation quality, one important aspect is factual correctness, which is currently one of the most fatal drawbacks of LLMs (OpenAI-Blog, 2022; Zhao et al., 2023). In answering user queries, LLMs such as GPT-3 (Brown et al., 2020) tend to make up facts and details, which is now ﬂagged as a primary warning in their API usage. As a major use case of LLMs is the prospect of replacing traditional search engines and usage for more direct information access through question-answering, factuality concerns could largely undermine their validity and degrade users' level of trust (Marcus, 2022). Fixing this issue is challenging and the concerns still persist even after the models are instruction-tuned with human feedback (Ouyang et al., 2022). This is because the source of truth can be unavailable during the ﬁnetuning process (OpenAI-Blog, 2022).

Thus, it is of urgent concern to better control the generation and increase the factual correctness of predictions. As LLMs could fail to recall accurate details when functioning as a knowledge base (Ye and Durrett, 2022; Creswell et al., 2022), if possible, knowledge from external sources could be introduced as assistance. Assisted thought process is also common in human reasoning: when humans answer questions, they often search (or revisit) external knowledge sources for supporting facts in order to refresh their (internal) memory.

Inspired by this, in this work we propose a Verify-and-Edit (VE) framework to post-edit the reasoning chains for more factually aligned predictions. As shown in Fig. 1, we ﬁrst select uncertain instances to edit, which have a less-than-majority-agree consistency. These instances, as implied by Wang et al. (2022), often consist of plausible-sounding statements, such as the sentence "John Nyskohus played for the Norweigian football team Odd Greenland" in Fig. 1. When editing, we ﬁrst generate a question to verify this detail, such as "What team did John Nyskohus play for?" Then, to answer this query, we introduce external knowledge through open-domain retrieval systems. For example, the fact "John Nyskohus ... played for Adelaide City.." is retrieved in this instance. Then, the rationales are edited by providing the retrieved facts in the prompts as memory refreshments. Thus, the edited rationales could be updated corresponding to the retrieved facts (Fig. 1). Given the edited rationales, the new prediction is generated, which considers more factually aligned reasoning traces.

To our knowledge, our work is the ﬁrst to post-edit CoT-style reasoning chains to enhance prediction performance. We perform experiments on two open-domain Question Answering (QA) tasks that require reasoning: Adversarial HotpotQA (Yang et al., 2018) and 2WikiMultihop (Ho et al., 2020). We also test its performance on the Fact Veriﬁcation task using Fever (Thorne et al., 2018). We ﬁnd that the model is able to beneﬁt from more factual reasoning chains, thus generating more accurate predictions. For example, for open-domain QA, our model demonstrates 3.8x accuracy improvement compared to similar retrieval-augmented models on AdvHotpot. On 2WikiMultihop, Verify-and-Edit reaches 33.6% accuracy with open-domain search, while CoT Self-Consistency stands at 27.7%.

2 Related Work

Chain-of-Thought or CoT (Wei et al., 2022) is a prompting method for improving the reasoning abilities of LLMs, which enables LLMs to decompose complex problems into multiple intermediate steps. CoT provides interpretability and has been proven to be more capable of solving complex problems than standard prompting methods.

However, hallucination is a long-standing problem in NLP, especially for LLMs, which has drawn signiﬁcant attention from the research communities. The decoding process of LLMs is auto-regressive, which unavoidably makes it output nonfactual content without controlled generation (Ye and Durrett, 2022; Wiegreffe et al., 2022). As such, the lack of supporting facts during the generation process of CoT could largely undermine the validity of the ﬁnal answer (Golovneva et al., 2022). Ye and Durrett (2022) demonstrate that the accuracy of the ﬁnal answers largely correlates with the factuality and consistency of the reasoning explanations. The commonly proposed methods to improve the factuality of CoT reasoning process can be grouped into two categories: prompt engineering and result calibration.

Prompt engineering methods are usually applied to guide LLMs to generate better intermediate reasoning explanations. ReAct (Yao et al., 2022), which is the most comparable to our work, synergizes reasoning and acting in LLMs, where reasoning steps help the model induce and update actions, while action steps allow the model to consult additional information from Wikipedia for a factuality check. Compared to ReAct, we generate more natural and conversational CoTs for better interpretability and easier learning. As such, our framework requires a much shorter prompt to learn. Press et al. (2022) propose self-ask by instructing the LLM to explicitly ask itself (and then answer) follow-up questions before answering the initial question. One natural way of solving a complex problem is to decompose the problem into sub-problems and solve them sequentially. Zhou et al. (2022) adopt the idea and propose least-to-most prompting. However, both self-ask and least-to-most prompting still rely on repetitively retrieving internal knowledge learned by the LLM instead of connecting to external knowledge. Thus, their ability to improve factuality is limited.

Result calibration functions on the output of the LLMs. Ye and Durrett (2022) train a calibrator to calibrate the weights of the ﬁnal answers based on the factuality and consistency of the generated explanations, which efﬁciently improves the results. The decoding method in CoT is naive greedy, which simply outputs the next token with the highest probability. Wang et al. (2022) propose a self-consistency decoding method, which samples a diverse set of reasoning paths and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Selection-Inference (SI) (Creswell et al., 2022) framework is another state-of-the-art method that exploits LLMs as general processing modules. Out of all the methods, it is also the ﬁrst to systematically improve the factual correctness of CoTs in order to predict more accurately. It alternates between selection and inference to generate a series of interpretable, causal reasoning steps leading to the ﬁnal answer, which is proven to be efﬁcient. However, it is not designed for open-domain or commonsense question answering.

Moreover, another comparable line of work has been exploring retrieval-augmented language model pretraining (REALM) (Guu et al., 2020), which ﬁrst retrieves documents from an external knowledge source and then utilizes retrieved documents to process question-answering tasks. Lazaridou et al. (2022) propose to include Google search results of the question in the prompt to improve the factuality of the generated answer. However, such methods may fail in complex questions as it does not utilize the reasoning capability of LLMs. Thus, we consider retrieval-augmented reasoning paths as a natural way to increase factual alignment.

3 Verify-and-Edit Framework

Our goal is to make LLMs generate more factual reasoning chains with CoT prompting assisted with external knowledge, thereby also improving prediction accuracy of the ﬁnal answer. We hypothesize that this can enhance LLMs' capability to solve complex knowledge-intensive tasks that require multiple reasoning steps to arrive at an answer.

Generally, we hope to follow the human reasoning process: when a person answers a question, if he/she is unsure, he/she would search for a supporting fact and consider it before giving the ﬁnal answer. Thus, we could separate the Verify-and-Edit (VE) framework into 3 different stages: ﬁnding uncertain predictions, editing their rationales by searching for supporting facts, and using the edited rationales to generate ﬁnal answers (Fig. 1). In designing the stages, we hope to maximally preserve the LLMs' biggest advantage: their open-generation and reasoning ability. And we aim to design tasks and setups as natural and conversational as possible, thus making it easy to understand for humans and LLMs which are trained with natural texts.

3.1 Deciding when to edit

How can we identify when a model is unsure of its prediction? The self-consistency method (Wang et al., 2022) provides a solution. In sampling diverse reasoning paths and answers, self-consistency is found to be highly correlated with accuracy, suggesting that it could provide an uncertainty estimate and confer abilities for the model to "know when it doesn't know". Thus, we begin the VE framework by using the consistency method to sample n diverse reasoning paths for a prediction task. The highly consistent predictions are left as-is. When consistency is lower than dn=2e, i.e. the majority cannot agree on the same answer, we label it as "uncertain".

3.2 How to edit a speciﬁc rationale

The rationale, i.e. the thought process (CoT), could be viewed in two parts: facts and reasoning which combines facts to derive a new claim. Thus, we consider improving the CoT from both aspects.

Facts To make the thought process more factually correct, we search for supporting facts in external knowledge sources (e.g. Wikipedia, Google). First, to mimic a human's query when searching for validating facts, a natural question is generated to verify the rationale. For this, we use the in-context learning capability of the same LLM. The original question and the rationale are both provided in the prompt for verifying question generation to ensure that it asks for the most relevant information required to answer the original question, instead of other entities in the rationale. For example, if the rationale (wrong) is "the US president born on 4 August 1961 is John Kennedy." and the original question is "who is the spouse of the US president born on 4 August 1961", we expect the generated verifying question to be: "Who is the US president born on 4 August 1961?" instead of "When is John Kennedy's birthday?" By generating a relevant question instead of directly querying with the generated rationale, we eliminate potential noise brought by incorrect fact generation. In the example above, if one retrieves using the wrong claim "the US president born on 4 August 1961 is John Kennedy", the incorrect entity "John Kennedy" may obfusticate the search process.

In this paper, we use relevant contexts retrieved from 3 systems: (i) DrQA (Chen et al., 2017), an open-domain question-answering system; (ii) Wikipedia search of relevant pages; and (iii) Google search, which demonstrates possibilities of combining LLMs and search engines.

As the retrieved contexts from a retrieval system could be longer than desired, we use a pre-trained LM to rank and select the top-k sentences most similar to the verifying question query.

Reasoning While methods such as Selection-Inference (Creswell et al., 2022) directly use retrieved facts as rationales, they are usually too verbose, longer than desired, or contain irrelevant details. Ye and Durrett (2022) have made similar observations: directly using supporting sentences is usually too verbose and not sufﬁcient.

To obtain more relevant and logical rationales, we again utilize a natural and generative approach, as reasoning abilities are believed to be already built into LLMs (Wei et al., 2022). In particular, by feeding in prompts in the format of "question, rationale, answer", the LLM learns to reason for a few steps before answer generation. Upon investigating the original rationales, we observe that, even when they contain incorrect facts, the logical reasoning component seems to be generally intact. Thus, we use the verifying questions (as logic) and retrieved facts (as information) to generate informed answers. The informed answers are then composed into a new rationale, providing potentially a more factual CoT.

3.3 Answering again

Finally, with the post-edited CoT, new answers are generated by prompting the LLM. A pseudocode of the overall procedure is given in Alg. 1, and illustrated with an example in Fig. 1. We can see that, by allowing the LLM to incorporate external knowledge, our method could result in more factually-grounded rationales. When prompted into the LLM as a CoT, it could bring in the information necessary to make a new prediction, which was originally not remembered correctly by the model.

Compared to speciﬁcally designed prompts such as ReAct (Yao et al., 2022), the Verify-and-Edit framework is simple and arguably more natural. Its conversational nature could allow humans to better understand the model's thought processes and have the potential for users to naturally interfere and revise at any stage of inference. In the experiments presented next, we also observe that such a setup is effective in mitigating factuality concerns and boosting end-task performances.

4 Experiment Setup

4.1 Reasoning tasks

As the Verify-and-Edit framework offers more knowledge-grounded reasoning steps, it should beneﬁt tasks that fulﬁll the following two properties: (i) reliant on multi-hop reasoning to arrive at a later prediction, thus depending on rationale generation, and (ii) open-domain, thus needing to interact with an external knowledge source.

As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks. For reproducing our results and extending the framework further, we make our codebase available at https://github.com/RuochenZhao/Verify-and-Edit

1 Introduction

Large Language Models (LLMs) have become the new norm in many downstream NLP tasks. In utilizing these LLMs, Chain-of-Thought (CoT) prompting (Wei et al., 2022) is found to improve performances for tasks that require complex reasoning, such as math word problems, commonsense reasoning, and symbolic manipulation. At the same time, it is able to generate interpretable reasoning chains. Recent work further explored how to use these reasoning chains to select better predictions. However, the primary focus of these methods has been to improve end-task performance by utilizing generated CoTs as-is. For example, Ye and Durrett (2022) train a calibrator that tunes prediction probabilities based on rationale scores; Wang et al. (2022) sample multiple reasoning paths to find the most common (consistent) prediction. Only a few, such as Creswell et al. (2022) and Zhou et al. (2022), have explored ways to improve the quality of CoTs themselves.

In fact, improving the CoT quality could be beneficial in enhancing both interpretability and end-task performance. Ye and Durrett (2022) point out that explanations judged as good by humans often indicate more accurate predictions. Intuitively, a better set of CoT prompts could provide better grounding and logically consistent thought processes, thus leading to more accurate predictions.

To improve generation quality, one important aspect is factual correctness, which is currently one of the most fatal drawbacks of LLMs (OpenAI-Blog, 2022; Zhao et al., 2023). In answering user queries, LLMs such as GPT-3 (Brown et al., 2020) tend to make up facts and details, which is now flagged as a primary warning in their API usage. As a major use case of LLMs is the prospect of replacing traditional search engines and usage for more direct information access through question-answering, factuality concerns could largely undermine their validity and degrade users' level of trust (Marcus, 2022). Fixing this issue is challenging and the concerns still persist even after the models are instruction-tuned with human feedback (Ouyang et al., 2022). This is because the source of truth can be unavailable during the finetuning process (OpenAI-Blog, 2022).

Thus, it is of urgent concern to better control the generation and increase the factual correctness of predictions. As LLMs could fail to recall accurate details when functioning as a knowledge base (Ye and Durrett, 2022; Creswell et al., 2022), if possible, knowledge from external sources could be introduced as assistance. Assisted thought process is also common in human reasoning: when humans answer questions, they often search (or revisit) external knowledge sources for supporting facts in order to refresh their (internal) memory.

Inspired by this, in this work we propose a Verify-and-Edit (VE) framework to post-edit the reasoning chains for more factually aligned predictions. As shown in Fig. 1, we first select uncertain instances to edit, which have a less-than-majority-agree consistency. These instances, as implied by Wang et al. (2022), often consist of plausible-sounding statements, such as the sentence "John Nyskohus played for the Norweigian football team Odd Greenland" in Fig. 1. When editing, we first generate a question to verify this detail, such as "What team did John Nyskohus play for?" Then, to answer this query, we introduce external knowledge through open-domain retrieval systems. For example, the fact "John Nyskohus ... played for Adelaide City.." is retrieved in this instance. Then, the rationales are edited by providing the retrieved facts in the prompts as memory refreshments. Thus, the edited rationales could be updated corresponding to the retrieved facts (Fig. 1). Given the edited rationales, the new prediction is generated, which considers more factually aligned reasoning traces.

To our knowledge, our work is the first to post-edit CoT-style reasoning chains to enhance prediction performance. We perform experiments on two open-domain Question Answering (QA) tasks that require reasoning: Adversarial HotpotQA (Yang et al., 2018) and 2WikiMultihop (Ho et al., 2020). We also test its performance on the Fact Verification task using Fever (Thorne et al., 2018). We find that the model is able to benefit from more factual reasoning chains, thus generating more accurate predictions. For example, for open-domain QA, our model demonstrates 3.8x accuracy improvement compared to similar retrieval-augmented models on AdvHotpot. On 2WikiMultihop, Verify-and-Edit reaches 33.6% accuracy with open-domain search, while CoT Self-Consistency stands at 27.7%.
