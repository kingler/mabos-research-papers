PRODIGY: Enabling In-context Learning Over Graphs
Qian Huang1
qhwang@cs.stanford.eduHongyu Ren1
hyren@cs.stanford.edu
Peng Chen1
pengc@stanford.eduGregor Kržmanc2
gregor.krzmanc@ijs.siDaniel Zeng1
dzeng@cs.stanford.edu
Percy Liang1
pliang@cs.stanford.eduJure Leskovec1
jure@cs.stanford.edu
1Stanford University
2University of Ljubljana
Abstract
In-context learning is the ability of a pretrained model to adapt to novel and diverse
downstream tasks by conditioning on prompt examples, without optimizing any
parameters. While large language models have demonstrated this ability, how
in-context learning could be performed over graphs is unexplored. In this paper,
we develop Pretraining OverDiverse In-Context Graph S ystems (PRODIGY),
the ﬁrst pretraining framework that enables in-context learning over graphs. The
key idea of our framework is to formulate in-context learning over graphs with a
novel prompt graph representation, which connects prompt examples and queries.
We then propose a graph neural network architecture over the prompt graph and a
corresponding family of in-context pretraining objectives. With PRODIGY , the pre-
trained model can directly perform novel downstream classiﬁcation tasks on unseen
graphs via in-context learning. We provide empirical evidence of the effectiveness
of our framework by showcasing its strong in-context learning performance on tasks
involving citation networks and knowledge graphs. Our approach outperforms the
in-context learning accuracy of contrastive pretraining baselines with hard-coded
adaptation by 18% on average across all setups. Moreover, it also outperforms
standard ﬁnetuning with limited data by 33% on average with in-context learning.
1 Introduction
In-context learning is a novel and one of the most intriguing capabilities of language models [ 1]. It refers
to the capability of a pretrained model to perform novel and diverse tasks directly at the prediction time
when prompted with just a few examples, without the need to update the model weights. For example,
a person may describe the new task ( e.g., question answering, machine translation, or code generation)
using natural language and demonstrate it to the language model with several prompt examples. The
language model then directly without any model training or ﬁnetunning performs the task.
However, how to enable in-context learning for diverse graph machine learning tasks, such as
identifying misinformation spreader in social networks [ 14] and product suggestions across online
e-commerce websites [ 21], still remain unexplored and challenging. An in-context learner for
graphs should be able to solve novel tasks on novel graphs. For example, give music product
indicates equal contribution.
Preprint. Under review.arXiv:2305.12600v1  [cs.LG]  21 May 2023
TF??
input nodes (head)input nodes (tail)data nodeslabel nodesDataGraphsTaskGraphPrompt examples!Queries "PromptGraph!!!""#Input !(),Label "(),(),(),Prompt Examples !:(),(),??Queries #:Graph $:(A)(C)(B)Few-shot PromptingFigure 1: In-context few-shot prompting over graphs with prompt graph for edge classiﬁcation in
PRODIGY. (A) Given the source graph G, we provide prompt examples Sthat consist of the input
head/tail nodes and their labels, as well as the queries. (B) For each datapoint from both prompt
examples and the queries, we ﬁrst construct its data graph GDby retrieving context from the source
graphG. (C) Then we create a task graph to capture the connection between each datapoint and each
label, which includes a data node vxfor each datapoint and a label node vyfor each label inY. Each
pair of data and label nodes are connected with edge attributes corresponding to their binary labels.
recommendations on Spotify when being trained on Amazon purchasing graph. The ﬁrst challenge
here is how to formulate and represent node-, edge- and graph-level tasks over graphs with a uniﬁed
task representation that allows the model to solve diverse tasks without the need for retraining or
parameter tuning. In other words, the key challenge is: what is an analog of natural language prompting
for graph machine learning tasks? The second challenge is how to design model architecture and
pretraining objectives that enable models to achieve in-context learning capability across diverse tasks
and diverse graphs in the uniﬁed task representation. Existing graph pretraining methods [ 7,24,8,13]
only aim to learn a good graph encoder and require ﬁne-tuning to adapt to different tasks, while existing
meta-learning methods over graphs [ 19,9,3,17,25] only aim to generalize across different tasks
within the same graph. On the other hand, achieving in-context learning requires tackling the more
difﬁcult setting of generalizing across the graphs andtasks without ﬁnetuning.
Here we present a general approach for solving these two challenges for classiﬁcation tasks on graphs:
(1)prompt graph , an in-context graph task representation, and (2) Pretraining OverDiverse In-Context
Graph S ystems (PRODIGY), a framework for pretraining an in-context learner over prompt graphs.
We propose prompt graph (Figure 1) to provide uniﬁed way to represent diverse node-, edge- and graph-
level machine learning tasks. Prompt graph ﬁrst contextualizes the input nodes/edges on which we make
prediction (including both the prompt examples and the queries), then connects them with additional la-
bel nodes, such that the prompt examples are interconnected with queries. Such a uniﬁed representation
allows us to specify diverse graph machine learning tasks to the same model regardless of the graph size.
PRODIGY then designs both model architecture and pretraining objectives with the prompt graph
in-context task formulation, such that the model is pretrained to solve tasks across a wide range of
tasks and graphs, and can continue to do so out-of-the-box. We design a graph architecture that utilizes
graph neural networks to learn node/edge representations and an attention mechanism to communicate
over prompt graph. Furthermore, we propose a family of in-context pretraining objectives over prompt
graph. In particular, this includes a novel self-supervised pretraining task, neighbor matching , where
we classify which neighborhood a node or edge belongs to.
We use PRODIGY framework to pretrain on citation networks (MAG240M [ 5]) and knowledge graphs
(Wiki [ 22]). We then show that such model (without any retraining) provides strong performance on
in-context paper category classiﬁcation and knowledge graph completion tasks on novel graphs it
was never trained on (arXiv, ConceptNet, FB15K-237, NELL) [ 6,16,23]. Speciﬁcally, PRODIGY
improves upon contrastive pretraining baselines with hard-coded adaptation for in-context setup
by 18% on average across all datasets and numbers of labels to classify among. Moreover, it also
outperforms standard ﬁnetuning with limited data by 32.6% on average with in-context learning. It
even outperforms the state-of-the-art few-shot learning methods trained on the testing downstream
graph with pure in-context learning. Finally, we further demonstrate that our methods achieve
increasingly higher performance with more examples in the prompt even beyond what it was pretrained
with, which shows that the model really learns to learn from context.
2
2 In-context Learning over Graphs
In this work, we speciﬁcally focus on in-context learning for node and edge classiﬁcation tasks
on graphs with few-shot prompting, which are the forms of the most standard and important graph
machine learning tasks. In this section, we introduce the concrete classiﬁcation tasks over graphs and
few-shot prompting over them with our in-context task representation prompt graph.
2.1 Classiﬁcation Tasks over Graphs
We deﬁne a graph as G=(V;E;R), whereV,E,Rrepresent the set of nodes, edges and relations. An
edgee=(u;r;v )2Econsists of a subject u2V, a relationr2Rand an object v2V.
Given a set of classes Y, a standard classiﬁcation task is predicting the labeling y2Yof each input
x2X. A node-level classiﬁcation task is similar but each input is a single node in G,i.e.,X=V, with the
additional auxiliary information of the entire graph G. For example, over a citation network consisting
of authors and papers, a node-level classiﬁcation task could be predicting the primary institution of
each author. Similarly, an edge-level classiﬁcation task is predicting the best labeling of potential edges
formed by any pair of nodes, i.e.,X=VV . A common special case is that the classes are the same
as the relationsY=R, such as predicting the relation between entities over knowledge graphs. More
generally, the same deﬁnitions can be extended to subgraph and graph-level classiﬁcation tasks, where
the input data xmay consist of more nodes and edges, and essentially represents a subgraph of G.
Since we are interested in tasks of different types/levels, we design a uniﬁed formulation, where the
space of the inputXconsists of graphs, i.e.,xi2X;xi=(Vi;Ei;Ri). For node classiﬁcation task, Gi
only consists of the input node that we aim to make predictions on, i.e.,jVij=1andjEij=0; for edge
classiﬁcation task, it consists of (subject, object) pair, i.e.,jVij=2andjEij=0.
2.2 Few-shot Prompting
Here we deﬁne in-context learning setup for classiﬁcation tasks over graphs with few-shot prompting.
For ak-shot prompt with a downstream m-way classiﬁcation tasks with jYj=mclasses, we use a
small number of input-label pairs S=f(xi;yi)gmk
i=1asprompt examples of the task speciﬁcation, such
that there are kinput-label pairs with label yfor eachy2Y. We also give the model a set of queries
Q=fxign
i=1that we want to predict labels for.
We emphasize an important difference of classiﬁcation tasks on graphs from language and other
modalities. Namely, since all input datapoints are nodes/edges/subgraphs from the larger source
graphG, this graph contains critical information and provides contexts for the inputs, e.g., the local
neighborhood of the input node that we aim to predict. Hence, besides SandQ, we also need to
include the source graph Gin the prompt.
Given the above information as the prompt, the pretrained model should be able to directly output
the predicted labels for each datapoint in Qvia in-context learning. Thus, how to formulate the
information as a uniﬁed and efﬁcient form of input poses a unique challenge and affects the model
architecture. Below, we present our in-context task formulation prompt graph designed to do so.
2.3 Prompt Graph Representation
Inspired by [ 2], we propose prompt graph as a uniﬁed representation of a k-shot prompt over graphs for
anm-way classiﬁcation task (Figure 1). A prompt graph is composed of data graphs and a task graph :
Data graph. To construct a prompt graph, we ﬁrst perform contextualization of each datapoint
xi= (Vi;Ei;Ri)inSandQin the source graph Gto form data graphs. The goal is to gather more
information about the xifrom the source graph Gwithout having to represent the entire source graph
explicitly. There are many potential designs for contextualization, from explicitly retrieving subgraphs
to implicitly using embedding-based methods. Here we construct data graph GD
iby sampling k-hop
neighborhood ofViinG. In other words,GD
i= (VD
i;ED
i;RD
i)Lk
i=0Neighbor (Vi;G;i), where
ViVD
iV,EiED
iE,RiRD
iR, and Neighbor is a function that returns the exact i-hop
neighbors of each node in Vi. With this data graph GD
i, we call the node set that corresponds to the nodes
inVibefore contextualization input node set ,e.g., the target node to classify in node classiﬁcation
task and the pair of nodes in link prediction task.
3
Task graph. After contextualizing each datapoint to a data graph GD, we then construct task graph
GTto better capture the connection and relationship among the inputs and the labels. For each data
graphGD
ifrom the previous stage, we have a data nodevxithat represents each input; for each label,
we have a label nodevyi. So overall, a task graph contains mk+ndata nodes (mkprompt examples
andnqueries) and mlabel nodes, as shown in Figure 1.
Now we add edges between the data nodes and the label nodes: For the query set, since we do not
know the labels of each graph, we add single directional edges from all label nodes to each datapoint
in the query set, i.e., each query data node vxiwill be connected to all the label nodes as shown by
the yellow edges in Figure 1; For the prompt examples, we connect each data node to all the label
nodes, where the edge with the true labels is marked as Twhile the others are marked as F, as shown
by the green and red edges in Figure 1 respectively.
Together we propose the prompt graph that consists of both data graphs and a task graph. Prompt graph
effectively captures the relationship between input data xiand the label yithrough the context captured
in data graphGD
iand the data node vxiand the label node vyiin the task graphGT. It is also possible
to extend prompt graph to non-classiﬁcation tasks and free-form text prompting. For example, for
numerical regression (e.g. molecular energy prediction) and other free-form generation tasks (e.g.
text generation), one can extend our task graph to contain vector values on the edges to represent yi.
Then different label nodes would represent different prediction tasks. To support more general forms
of prompting, one can include additional task information and instructions in the feature of label nodes,
and additional description paired with each datapoint in the global feature in data graph.
3 Pretraining to Enable In-context Learning
So far given a few-shot prompt for a classiﬁcation task over graphs, we have deﬁned a prompt graph
representation for it that captures relationships between the prompt examples, queries, and labels. Now
we need to design a pretraining strategy that can pretrain a generalizable model capable of in-context
learning. We assume access to a pretraining graph Gpretrain that is independent of the source graph
Gfor the downstream task.
In this section, we introduce PRODIGY, a general pretraining framework over Gpretrain that is
designed speciﬁcally for enabling in-context learning over downstream classiﬁcation tasks without any
additional ﬁnetuning steps on arbitrary graphs. Our framework PRODIGY has two main components:
model architecture over prompt graph and in-context pretraining objectives.
3.1 Message Passing Architecture over prompt graph
Next we introduce our model architecture over the prompt graph consisting of two submodules:
Data graph Message Passing. First, we apply a message passing GNN module MDthat learns node
representation Efor nodes in eachGD.
E2RjVDjd=MD(GD) (1)
wheredis the embedding dimension. MDcan be implemented in multiple ways, such as using Graph
Convolutional Network (GCN) or Graph Attention Networks (GAT) [11, 18].
To read out a single embedding Gifor each data graph, we perform another aggregation step to pool
node embeddings. For node classiﬁcation tasks, we take the updated node representation of the single
input node that we aim to predict, i.e.:
Gi=EVi (2)
For link prediction tasks, we concatenate the pair of nodes, which we want to predict a link between, as
well as a max pooling over all node representations following [ 10] with an additional linear projection
layer at the end to convert the embedding size back to d.
Gi=WT(Ev12VijjEv22Vijjmax(Ei))+b; (3)
wherejjrepresents concatenation, W2R3ddis a learnable weight matrix and bis the learnable bias.
Task graph Message Passing. Note in the previous step there is no communication between different
datapoints inSandQ. Now we would like to communicate between them via message passing over
4
the task graphGT. We apply another GNN MTon the task graph to obtain updated representation of
data nodes and label nodes.
H=MT(GT) (4)
where H is the obtained embedding per node. The initial embedding of data node vxiisGiand the
embedding of label node vyican either be initialized with random Gaussian or additional information
available about the labels. Each edge also has two binary features eijthat indicate 1) whether the edge
comes from an example or a query, and 2) the edge type of TorF. ForMT, we use an attention-based
GNN, where each node performs attention to other nodes at each layer. See the architecture detail
in the appendix C.
The goal of this step is to learn a better representation of the label nodes using the support examples and
propagate label information back to the support and query graph representation for a more task-speciﬁc
graph representation.
Prediction Read Out. Finally, we readout the classiﬁcation logits Oiby taking cosine similarity
between each pair of query graph representation and label representation, as in contrastive learning:
Oi=[cosine_similarity (Hxi;Hy);8y2Y] (5)
Note that we could perform the two message passing steps for multiple rounds to have more communi-
cation between xiand learn a better representation. One key insight is that different in-context prompt
examples share information through the label nodes, which can be seen as an information bottleneck.
3.2 In-context Pretraining Objectives
In order to pretrain the model for solving the downstream graph tasks in-context, we propose a set
of in-context pretraining objectives. The goal is to pretrain the graph model using a large pretraining
graphGpretrain independent of the downstream task graph, such that the model can directly be applied
on downstream tasks with in-context learning.
Our main design principle is that we formulate each pretraining objective in an in-context learning
way. Most previous graph pretraining objectives only pretrain a shared graph encoder to perform
various tasks with task-speciﬁc heads, so they require ﬁnetuning for another task-speciﬁc head over
each downstream task. In contrast, we explicitly construct in-context pretraining tasks in prompt graph
form and pretrain the model to solve diverse tasks in-context with the same set of weights, such that
it can perform in-context learning directly over downstream tasks.
Below, we detail our proposed family of in-context pretraining objectives in terms of three components:
1) pretraining task generation, including few-shot prompt ( i.e. Figure 1(A)) and corresponding
labels, 2) converting generated few-shot prompt to prompt graph format ( i.e.Figure 1(B,C)) with
augmentation, and 3) pretraining loss over the generated prompt graph.
3.2.1 Pretraining Task Generation
We propose two methods to generate pretraining tasks from the pretraining graph Gpretrain in the form
of few-shot prompts: neighbor matching andmulti-task .
Neighbor Matching. Given the pretraining graph, we construct self-supervised in-context pretraining
tasks with the goal of classifying which local neighborhood a node belongs to, where each local
neighborhood is deﬁned by the example nodes belonging to that neighborhood. Intuitively, we sample
multiple subgraphs from the pretraining graph Gpretrain as the local neighborhoods, and we say a node
belongs to a local neighborhood if it is in the sampled subgraph.
Formally, we denote NMk;mas a sampler that generates m-way neighbor matching tasks, where each
includes ak-shot prompt (Gpretrain;SNM;QNM)(see subsection 2.2 and Figure 1(A)) and the labels of
the queries. For simplicity of the notation, we will include the labels in QNMas paired with the inputs:
(Gpretrain;SNM;QNM)NMk;m(Gpretrain ) (6)
To generate these, we ﬁrst sample mnodes from the pretraining graph Gpretrain , where each of the
sampled node corresponds to one class.
C=fcigm
i=1ciUniform (Vpretrain ) (7)
5
For each sampled node/class ci, we sample kdifferent nodes from its exact l-hop neighbors. These
knodes serve as examples of label ci. We also sample additional dn
menodes similarly for each label
cito form the query set. Formally,
Ni=Neighbor (ci;Gpretrain;l) (8)
Si=f(xj;yj=ci)gk
j=1xjUniform (Ni) (9)
Qi=f(xj;yj=ci)gdn
me
j=1xjUniform (Ni) (10)
In such a way, we constructed a neighbor matching pretraining task sample in the format of a few-shot
prompt (Gpretrain;SNM=SSi;QNM=SQi).
The neighbor matching task generation process outlined above is speciﬁcally applicable when the
downstream tasks are also node classiﬁcation. When the downstream task is link prediction, we may
adapt the above neighbor matching tasks to over edges correspondingly. Speciﬁcally, we can expand
each sampled input node xito an edge by randomly sampling an edge that contains xi. Then, instead
of classifying to which neighborhood a node in the query set belongs, now the neighbor matching
task is to classify to which neighborhood an edge in the query set belongs.
Multi-task. When the pretraining graphs have node or edge-level labeling f(xi) =yi2Yfor some
xi2V pretrain orEpretrain , we can further leverage this signal to perform supervised pretraining.
Similar to neighbor matching, the key is to construct such supervised pretraining tasks in the format
of few-shot prompts and corresponding labels.
(Gpretrain;SMT;QMT)MTk;m(Gpretrain;f) (11)
For node classiﬁcation tasks, we ﬁrst sample mlabels from the whole label set. Then, for each label, we
directly sample knodes as support examples and dn
menodes with labels in this set as query examples.
C=fcigm
i=1ciUniform (Y) (12)
Si=f(xj;yj=ci)gk
j=1xjUniform (fxijf(xi)=cig) (13)
Qi=f(xj;yj=ci)gdn
me
j=1xjUniform (fxijf(xi)=cig) (14)
We then construct a task with the few-shot prompt as (Gpretrain;SMT=SSi;QMT=SQi). For link
prediction, we directly use the edge type function as f, i.e.f((v1;v2))=r() (v1;r;v 2)2E. With
thisf, we may directly sample medge types and construct pretraining tasks in a similar way as above.
The beneﬁt of such a supervised pretraining objective is that it could directly resemble the format of
downstream tasks, compared with neighbor matching objective, which may only serve as a surrogate.
However, it requires extra labels if fis not part ofGpretrain , e.g. node classiﬁcation labels that may
not exist for some pretraining graphs.
3.2.2 Prompt graph generation with augmentation
After we obtained the few-shot prompts and labels for either of the two tasks (NeighborMatching
and multi-task), we need to construct the prompt graph for pretraining. In addition to the standard
construction process described in subsection 2.3, we add an additional augmentation step to augment
!!"!#"!$"!%"???Prompt Examples !:Queries #:?2 hops neighborsInput !Label "
Figure 2: Neighbor matching pretraining task generation. The key idea of neighbor matching is to
classify whether a node/edge is in the local neighborhood of a set of sampled nodes (as labels). Given
a set of sampled nodes and their two-hop neighbors as prompt examples, we aim to classify whether
a query node is also a two-hop neighbor of each sampled node.
6
the data graphs as inspired by Contrastive Learning. The key insight is to corrupt data graph such
that the pretrained model learns representation invariant to various corruptions.
Here we demonstrate how we adopt graph augmentation techniques during the construction of prompt
graph from a few-shot prompt generated from Gpretrain . We ﬁrst still sample the k-hop neighbor sub-
graph of each sample Giin the prompt examples and queries: GD
iLk
j=1Neighbor (Gi;Gpretrain;j).
Then we adopt the following two augmentation techniques to create augmented data graph Gaug
i,
including (1) node dropping, and (2) node feature masking [ 24]. For node dropping, we randomly
drop nodes from the k-hop neighbor subgraph and take the remaining graph as Gaug
i=DropNode (GD
i).
For node feature masking, we randomly mask the feature of a subset of nodes with value zero to create
Gaug
i=MaskNode (GD
i). With the augmented data graphs for each datapoint in the prompt examples
and the queries, we may accordingly construct the task graph GTby creating a data node vxifor each
augmented data graphs and the label node vyias introduced in subsection 2.3. Combining data graphs
with task graph, we obtain the prompt graph formulation with augmentation for the few-shot prompt.
3.2.3 Pretraining Loss
Finally, we pretrain the model with the cross-entropy objectives over generated prompt graphs:
(Gpretrain;SNM;QNM)NMk;m(Gpretrain ) (15)
(Gpretrain;SMT;QMT)MTk;m(Gpretrain;f) (16)
L=E
xi2Q NMCE(ONM;i;yNM;i)+E
xi2Q MTCE(OMT;i;yMT;i) (17)
whereONM;iis the logits produced by our model over input of Gaug
iandGTproduced fromQNM, as
described in subsection 3.1; yNM;iis the corresponding label of xiinQNM; Similar for MT terms.
4 Experiments
4.1 Experimental Setup
Datasets. For pretraining, we use two datasets: MAG240M [5], a large scale citation network with 122
million nodes and 1.3 billion edges; and Wiki , a knowledge graph (KG) constructed from Wikipedia
[22] with 4.8 million nodes and 5.9 million edges. After the model is pretrained we evaluate its
in-context learning capability on diverse classiﬁcation tasks over 4 graphs: arXiv [6],ConceptNet
[16],FB15K-237 [23],NELL [23]. We use subsets of knowledge graph datasets same as in [ 10,23].
For arXiv, the downstream task is an m-ways node classiﬁcation task that predicts the paper category.
For knowledge graph datasets ( ConceptNet ,FB15K-237 ,NELL ), the downstream task is an m-ways
relation type classiﬁcation task that predicts the relationship connecting the two input nodes.
Evaluation. We pretrain our model on MAG240M andWiki and then we evaluate the in-context
learning performance on differnt downstream datasets that belong to similar domain as the pretraining
dataset (e.g., pretraining on Wiki and evaluating on ConceptNet ,FB15K-237 , and NELL ). Each of
the downstream classiﬁcation datasets has its original train, validation, and test splits. To simulate
the situation where there are a limited amount of labeled data in the downstream task, we randomly
select 10 nodes (or edges) from the training split per way as the prompt examples with known labels.
Then, we construct a k-shot prompt for test nodes (or edges) from the test split by randomly selecting
kexamples per way from these available examples. This allows us to test the model’s ability to learn
in-context relationships and perform well on classiﬁcation tasks with truly limited known labels. By
default we use k=3shots in our experiments.
Methods and Baselines. We consider three versions of our proposed framework PRODIGY: 1)
PG-NM, which uses neighbor matching task for pretraining; 2) PG-MT, which employs multi-task
pretraining; and 3) full PRODIGY, which combines the previous two methods. To augment the data,
we use DropNode and MaskNode augmentations with a probability of 0.5 per node for each method.
We consider three baselines for comparison: 1) NoPretrain, which uses a randomly-initialized model
with the same architecture as our pretrained models; 2) Contrastive [ 24], which employs a standard
contrastive learning method with the same augmentation as above and uses a hard-coded nearest
neighbor algorithm to adapt to our in-context learning setting. Speciﬁcally, we classify the query by
7
Table 1: In-context learning accuracy (%) on arXiv paper category classiﬁcation on 500 sampled test
tasks with 3-shot prompts. PRODIGY was pretrained on MAG240M and is then applied in-context
to arXiv, which has completely different structure and a different set of paper categories. PG-NM
and PG-MT are ablations of PRODIGY.
Classes NoPretrain Contrastive PG-NM PG-MT PRODIGY Finetune
3 33.16 0.30 65.08 0.34 72.50 0.35 65.64 0.33 73.09 0.36 65.42 5.53
5 18.33 0.21 51.63 0.29 61.21 0.28 51.97 0.27 61.52 0.28 53.49 4.61
10 9.19 0.11 36.78 0.19 46.12 0.19 37.23 0.20 46.74 0.20 30.22 3.77
20 4.72 0.06 25.18 0.11 33.71 0.12 25.91 0.12 34.41 0.12 17.68 1.15
40 2.62 0.02 17.02 0.07 23.69 0.06 17.19 0.08 25.13 0.07 8.04 3.00
Table 2: In-context learning accuracy (%) on ConceptNet, FB15K-237 and NELL (from top to
bottom) on 500 sampled test tasks with 3-shot prompts. PRODIGY was pretrained on Wiki, which
has completely different node and relation types from graphs it is then applied on in-context.
Classes NoPretrain Contrastive PG-NM PG-MT PRODIGY Finetune
4 30.4 0.63 44.01 0.61 46.94 0.61 51.78 0.63 53.97 0.63 53.85 9.29
5 33.54 0.61 81.35 0.58 80.35 0.57 89.15 0.46 88.02 0.48 82.01 12.83
10 20.0 0.35 70.88 0.48 71.68 0.45 82.26 0.40 81.1 0.39 71.97 6.16
20 9.20.18 59.8 0.35 59.9 0.35 73.47 0.32 72.04 0.33 64.01 4.66
40 2.50.08 49.39 0.23 46.82 0.21 58.34 0.22 59.58 0.22 57.27 3.33
5 33.440.57 84.080.54 80.530.58 84.790.51 87.020.44 87.2212.75
10 18.820.31 76.540.45 72.770.48 78.50.44 81.060.41 71.905.90
20 7.420.16 66.560.35 62.820.36 69.820.34 72.660.32 66.198.46
40 3.040.07 57.440.24 49.590.22 53.550.23 60.020.22 55.064.19
comparing its pretrained embedding against the average embedding of the example inputs of each
class. 3) Finetune [ 7], which trains an additional linear classiﬁcation head on top of the graph encoder
pretrained with contrastive learning, following the standard practice.
4.2 In-Context Learning Results
We ﬁrst evaluate the in-context learning capability for node classiﬁcation and link prediction with
various numbers of ways (i.e. number of classes to classify among).
Strong in-context learning performance. The results demonstrate that our method PRODIGY
consistently outperforms all other baselines in this setting. It achieves the highest average accuracy
across all ways on arXiv , with an average improvement of 28.6% and up to 48% over the best baseline
of Contrastive. Over KGs, PRODIGY also outperforms contrastive learning on average by 12.2%.
PRODIGY also demonstrates similar-to-better performance compared to Finetune, which requires
additional training on downstream tasks. On arXiv , we see an average improvement of 77.7% over
all ways. This can be attributed to the diverse set of pretraining tasks incorporated in PRODIGY, which
allows the model to avoid overﬁtting on speciﬁc tasks and learn in-context.
Self-supervised pretraining PG-NM bridges different tasks. In particular, we highlight that the
pure self-supervised pretraining method PG-NM produces signiﬁcantly higher in-context learning
performance over arXiv than baselines, even though the model is pretrained on different tasks from the
downstream task. This advantage can be further leveraged by pretraining on even larger-scale unlabeled
datasets. On the other hand, PG-MT follows the supervised pretraining objective that directly resembles
the format of downstream tasks. On KGs, this allows PG-MT to adapt better to downstream task
even sometimes compared to the full PRODIGY ( marked by underlines), while PG-NM might have
overﬁtted to the incorrect strategy of only identifying co-occurring nodes. Yet, PG-MT performs worse
onarXiv potentially due to less diversity. The full PRODIGY, which ensembles the two, achieves
more diversity than either single task and therefore achieves the best performance over both worlds.
Outperforming meta-learning method trained on test graph. Finally, we compare PG-NM
in-context learning performance against state-of-the-art meta-learning method TENT [ 20] over the
downstream test graph arXiv . We evaluate the average 3-ways classiﬁcation tasks performance over
only test labels, since TENT trains on train labels from arXiv . PG-NM achieves 69:07% over the
65:13% of TENT, even though PG-NM has never been trained on any paper category classiﬁcation
8
0 4 8 12 16 20 24 28
shots0.360.400.440.480.520.560.600.64accuracy
ConceptNet - 4 way
Contrastive
PRODIGYFigure 3: In-context learning accuracy on Concept-
Net in a 4-ways setting wrt. the number of prompt
examples (shots).
2500 5000 7500 10000 12500 15000 17500 20000
training steps0.450.500.550.60accuracy
Contrastive
PRODIGYFigure 4: With more training steps and data on
arXiv (5-ways), PRODIGY keeps improving
while the baseline (Contrastive) saturates.
task during pretraining. This demonstrates the power of self-supervised pretraining over large amount
of data compared to supervised meta-learning over the limited labeled data (train labels in arXiv ).
4.3 Ablations
Aside from PG-NM and PG-MT, we also conduct ablation studies on various conﬁgurations of the self-
supervised objective PG-NM as described in 3.2. See the full results in Appendix E and Table 4. Overall,
the ablation results reveal that using all of the elements together results in the highest performance.
Speciﬁcally, attribute prediction (see appendix A) has the greatest impact on PG-NM’s performance,
as its removal results in an average 7% drop across all ways, shown in the ‘No-Attr’ column.
4.4 Evaluation using different numbers of in-context examples
We investigate our method’s ability to learn from the context by analyzing its performance as the number
of prompt examples changes. Figure 3 shows the result on ConceptNet . See full results on other
datasets in Appendix F. As the number of prompt examples increases, the margin of our proposed PG
models over the baseline increases. This supports the hypothesis that the PRODIGY models can more
effectively learn the unknown task by reasoning about the common characteristics of prompt examples.
4.5 Scaling with Data Size
Finally, we explore how the model scales with more pretraining data. The result on arXiv in a 5-ways set-
ting is illustrated in Figure 4. It shows that the Contrastive baseline saturates quickly and its performance
ﬂuctuates as trained over more pretraining data. Instead, PRODIGY consistently shows an improvement
in performance as more data is pretrained on, since the pretraining tasks are harder and more diverse.
5 Related Work
5.1 In-context Learning of Large Language Models
Pretrained large language models can make predictions for diverse downstream tasks directly by
prompting with a few examples of the task or more generally any textual instructions. This ability
is called in-context learning. Comparing to previous language encoder models like BERT [ 4], it drasti-
cally reduces the adaptation effort comparing to ﬁne-tuning, and has demonstrated strong performance
in a broad range of models and tasks. Our work extends this success similarly to graph data compared
to the current pretrained graph encoders, such that a single pretrained model can be adapted to different
classiﬁcation tasks over different graphs without additional ﬁne-tuning but only few-shot prompting.
5.2 Pretraining on Graphs
There are many existing works on pretraining over graphs[ 7,24,8,13]. However, they all follow the
general paradigm of learning a good graph encoder that can perform certain pretraining tasks, such
as masked feature prediction [ 7] and paired graph classiﬁcation [ 24]. To adapt to any downstream
tasks, it then requires ﬁnetuning a classiﬁcation head on top of the encoder with large amount of task
speciﬁc data for each downstream task. In contrast, we explore pretraining methods for inducing
general in-context learning ability, such that the pretrained model can be directly used for various
downstream tasks with no gradient updates.
9
5.3 Meta Learning on Graphs
Another closely related line of works is meta-learning methods over graphs that aim to address standard
few shot learning problems over graphs[ 19,9,3,17,25]. However, existing meta-learning methods
are only designed and tested for generalizing across different tasks on the same graph: the methods are
trained on a set of training tasks on a graph, then tested over a disjoint but similar set of test tasks over
the same graph. They are shown to exhibit optimal performance only when trained on similar curated
tasks [ 10]. Different from this, our work explicitly focuses on the in-context learning performance, i.e.
model performance on graphs and tasks completely different from the pretraining without additional
ﬁne-tuning.
6 Conclusion
We introduce PRODIGY, the ﬁrst framework that enables in-context learning on graphs. A model
that is pretrained using PRODIGY can seamlessly execute a new classiﬁcation task over new graphs
represented by prompt graph. It markedly surpasses the performance of other baseline models with
in-context learning, even those that employ ﬁnetuning, in both the node and edge classiﬁcation tasks.
References
[1]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems , 33:1877–1901, 2020.
[2]K. Cao, J. You, and J. Leskovec. Relational multi-task learning: Modeling relations between
data and tasks. In International Conference on Representation Learning (ICLR) , 2022.
[3]M. Chen, W. Zhang, W. Zhang, Q. Chen, and H. Chen. Meta relational learning for few-shot
link prediction in knowledge graphs. arXiv preprint arXiv:1909.01515 , 2019.
[4]J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. ArXiv , abs/1810.04805, 2019.
[5]W. Hu, M. Fey, H. Ren, M. Nakata, Y . Dong, and J. Leskovec. Ogb-lsc: A large-scale challenge
for machine learning on graphs. arXiv preprint arXiv:2103.09430 , 2021.
[6]W. Hu, M. Fey, M. Zitnik, Y . Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph
benchmark: Datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687 , 2020.
[7]W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V . Pande, and J. Leskovec. Strategies for
pre-training graph neural networks. arXiv preprint arXiv:1905.12265 , 2019.
[8]Z. Hu, Y . Dong, K. Wang, K.-W. Chang, and Y . Sun. Gpt-gnn: Generative pre-training of
graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining , pages 1857–1867, 2020.
[9] K. Huang and M. Zitnik. Graph meta learning via local subgraphs, 2020.
[10] Q. Huang, H. Ren, and J. Leskovec. Few-shot relational reasoning via connection subgraph
pretraining. Advances in neural information processing systems , abs/2210.06722, 2022.
[11] T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with graph convolutional networks,
2016.
[12] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V . Stoy-
anov. Roberta: A robustly optimized BERT pretraining approach. CoRR , abs/1907.11692, 2019.
[13] J. Qiu, Q. Chen, Y . Dong, J. Zhang, H. Yang, M. Ding, K. Wang, and J. Tang. Gcc: Graph con-
trastive coding for graph neural network pre-training. In Proceedings of the 26th ACM SIGKDD
international conference on knowledge discovery & data mining , pages 1150–1160, 2020.
[14] S. Sharma and R. Sharma. Identifying possible rumor spreaders on twitter: A weak supervised
learning approach. In 2021 International Joint Conference on Neural Networks (IJCNN) , pages
1–8, 2021.
10
[15] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y . Liu. MPNet: Masked and Permuted Pre-training for
Language Understanding. In Advances in Neural Information Processing Systems , volume 33,
pages 16857–16867. Curran Associates, Inc., 2020.
[16] R. Speer, J. Chin, and C. Havasi. Conceptnet 5.5: An open multilingual graph of general
knowledge. In Proceedings of the AAAI conference on artiﬁcial intelligence , volume 31, 2017.
[17] J. Sun, Y . Zhou, and C. Zong. One-shot relation learning for knowledge graphs via neighborhood
aggregation and paths encoding. Transactions on Asian and Low-Resource Language Information
Processing , 21(3):1–19, 2021.
[18] P. Veli ˇckovi ´c, G. Cucurull, A. Casanova, A. Romero, P. Liò, and Y . Bengio. Graph attention
networks, 2017.
[19] S. Wang, K. Ding, C. Zhang, C. Chen, and J. Li. Task-adaptive few-shot node classiﬁcation. Pro-
ceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , 2022.
[20] S. Wang, K. Ding, C. Zhang, C. Chen, and J. Li. Task-adaptive few-shot node classiﬁcation. Pro-
ceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining , 2022.
[21] S. Wu, W. Zhang, F. Sun, and B. Cui. Graph neural networks in recommender systems: A survey.
ACM Computing Surveys , 55:1 – 37, 2020.
[22] W. Xiong, M. Yu, S. Chang, X. Guo, and W. Y . Wang. One-shot relational learning for knowledge
graphs, 2018.
[23] W. Xiong, M. Yu, S. Chang, X. Guo, and W. Y . Wang. One-shot relational learning for knowledge
graphs. In EMNLP , 2018.
[24] Y . You, T. Chen, Y . Sui, T. Chen, Z. Wang, and Y . Shen. Graph contrastive learning with
augmentations. Advances in neural information processing systems , 33:5812–5823, 2020.
[25] C. Zhang, H. Yao, C. Huang, M. Jiang, Z. Li, and N. V . Chawla. Few-shot knowledge graph
completion. In Proceedings of the AAAI conference on artiﬁcial intelligence , volume 34, pages
3041–3048, 2020.
11
A Attribute Prediction Loss
For each augmented DataGraph Gaug, the certain node features Fvare masked during MaskNode
augmentation. Therefore, we can reconstruct them using the learned embedding Evwith a MLP and
train with MSE reconstruction Loss.
Lattr(Gaug)=1
jVDjX
vMSE(Fv;MLP(Ev))
B Dataset Statistics
Table 3: Dataset statistics
Dataset # Nodes # Edges # Classes
MAG240M 122M 1.3B 153
Wiki 4.8M 5.9M 639
arXiv 169K 1.2M 40
ConceptNet 791K 2.5M 14
FB15K-237 15K 268K 200
NELL 69K 181K 291
C Task Graph GNN Architecture
For the GNN over task graph MT, we use an attention-based GNN, where each node performs attention
to other nodes at each layer:
ij=MLP 
WT
qHl
ijjWT
kHl
jjjeij
(18)
ij=exp(ij)P
k2N(i)[figexp(ik)(19)
Hl+1
i=ReLU0
@BN0
@Hl
i+WT
oX
j2N(i)[figijWT
vHl
j1
A1
A (20)
D Hyperparameters
D.1 Model Architecture, MAG240M and arxiv
We initialize the node features in citation network datasets using a pretrained language model
(RoBERTa [ 12] base model trained on NLI and STSB).The architecture of our PromptGraph model
in all of our proposed methods for citation network datasets (full PRODIGY, PG-NM, and PG-MT)
and the baseline (NoPretrain), consists of two message passing layers, MD, over the DataGraph and
one message passing layer, MT, over the TaskGraph. These layers are deﬁned in Section 3.1.
For the Contrastive method, the architecture includes two message passing layers, Md, over the
DataGraph, and a contrastive learning component that is deﬁned in Section 4.1. The mode for Finetune
is the same as the Contrastive method, with the addition of a linear layer head over the output of the
twoMdlayers, also described in Section 4.1.
D.2 Model Architecture, knowledge graph datasets
We initialize node and edge features in knowledge graph datasets using a pretrained language model
(MPNet [ 15]). The architecture of our PromptGraph model in all of our proposed methods for
knowledge graph datasets (full PRODIGY, PG-NM, and PG-MT) and the baseline (NoPretrain),
consists of two message passing layers, MD, over the DataGraph, an aggregator as described by
12
Table 4: Ablation of PG-NM on arXiv.
Ways PG-NM 3!1 shot No Attr No Aug No Attr, Aug No Attr, Aug, MT
3 72.500.35 69.131.09 65.741.12 68.981.09 66.531.12 63.601.06
5 61.210.29 57.490.92 52.780.90 57.500.85 53.890.92 51.270.69
10 46.120.19 42.030.60 37.990.63 42.430.64 38.870.59 37.620.34
20 33.710.11 30.180.38 26.600.36 30.890.38 27.500.36 27.440.17
40 23.690.07 21.440.22 18.030.21 21.970.24 18.520.22 19.690.08
Equation 3, and two message passing layers, MT, over the TaskGraph, which only pass messages along
the positive and query edges. These layers are deﬁned in Section 3.1.
For the Contrastive method, the architecture includes two message passing layers, Md, over the
DataGraph, an aggregator as described by Equation 3 and a contrastive learning component that is
deﬁned in Section 4.1. The mode for Finetune is the same as the Contrastive method, with the addition
of a linear layer head over the output of the two Mdlayers, also described in Section 4.1.
D.3 Training, MAG240M
The following describes our pretraining setup:
The pretraining task we used consisted of 30 ways, 3 shots, and 4 queries per task. This speciﬁc
task conﬁguration was carefully selected to strike a balance between complexity and diversity in the
training data, without overwhelming the GPU memory.
We checkpoint the model every 500 steps.
Our pretraining setup included a model with an input dimension of 768 and an embedding dimension of
256, batch size of 1, and the AdamW optimizer with a learning rate of 110 3and weight decay of 1
10 3, a pretraining task with 30 ways, 3 shots, and 4 queries per task, and checkpointing every 500 steps.
This consistent conﬁguration was applied across all the methods for fair comparison. Our full PRODIGY
setup, on average, involves sampling 1 Neighbor Matching task per 1 multitask pretraining tasks.
For our evaluation process, we computed zero-shot transfer performance of the model on the test set,
using the pretraining checkpoint at the 10,000 step of pretraining. The evaluation was conducted on
500 test tasks, with batch size of 5, measured on the downstream task of graph classiﬁcation accuracy.
To maintain consistency, we kept the number of shots and queries constant at 3 for all evaluation tasks.
D.4 Training, Wiki
The following describes our pretraining setup:
Our pretraining setup included a model with an input dimension of 768 and an embedding dimension of
256, the AdamW optimizer with a learning rate of 110 3and weight decay of 110 3, a pretraining
task with 30 ways, 3 shots, and 4 queries per task, using a batch size of 10, and checkpointing
every 500 steps. This speciﬁc task conﬁguration was carefully selected to strike a balance between
complexity and diversity in the training data, without overwhelming the GPU memory. This consistent
conﬁguration was applied across all the methods for fair comparison. Our full PRODIGY setup
involves sampling one neighbor matching task per 50 multitask pretraining tasks.
For our evaluation process, we computed zero-shot transfer performance of the model on the test set,
using the pretraining checkpoint at the 8,000 step of pretraining. The evaluation was conducted on 500
test tasks, with batch size of 1, measured on the downstream task of graph classiﬁcation accuracy. To
maintain consistency, we kept the number of shots and queries constant at 3 for all evaluation tasks. We
sample 1-hop neighbourhoods for ConceptNet and FB15K-237 and 2-hop neigbourhoods for NELL
and Wiki.
E Ablation on Table 4 for the PG-NM setting
In Table 4, we ablate on various conﬁgurations of the self-supervised objective PG-NM. As described in
Section 3.2, PG-NM is composed of an attribute prediction loss, dropnode and zeronode augmentations,
13
with the default setting of sampling 3 shots neighbor matching tasks. Our best setting, referred to
as simply “PG-NM”, is also shown in Table 1 and comprises of attribute prediction, dropnode and
zeronode augmentations, with the default setting of 3 shots.
The ablation results reveal that using all of these elements together results in the highest performance.
Speciﬁcally, attribute prediction has the greatest impact on PG-NM’s performance, as its removal
results in an average 7% drop across all ways, as shown in the ‘No-Attr’ column.
Removing the dropnode and zeronode augmentations results in an average 3% drop across all ways, as
shown in the No Aug’ column. Removing both attribute prediction and augmentations results in perfor-
mance that is similar to just removing attribute prediction alone, which is also roughly a 7% drop across
all ways, as shown in the ‘No Attr, Aug’ column. Additionally, we found that decreasing the number
of shots to 1 from the default setting of 3 resulted in an average 3.5% drop across all ways, as shown.
F Evaluation using different numbers of shots
We show evaluation using different numbers of shots, as shown in Figures 3, 5, and 6, 7.
0 4 8 12 16 20 24 28
shots0.520.560.600.640.680.720.760.80accuracy
FB15K-237 - 20 way
Contrastive
PRODIGY
Figure 5: In-context learning accuracy on FB15K-237 in a 20-ways setting wrt. the number of prompt
examples (shots).
0 4 8 12 16 20 24 28
shots0.570.600.630.660.690.720.750.780.81accuracy
NELL - 20 way
Contrastive
PRODIGY
Figure 6: In-context learning accuracy on NELL in a 20-ways setting wrt. the number of prompt
examples (shots).
G Scaling with Data Size
We explore how the model scales with more pretraining tasks. Note that we use the number of train
steps as a proxy because the model sees more pretraining tasks as the training proceeds with almost
no redundancy (0.20% for 10k tasks). The result on arXiv in a 5-ways setting is illustrated in Figure 4.
It shows that the Contrastive baseline saturates quickly and its performance ﬂuctuates given more
14
0 5 10 15 20 25 30
shots0.450.500.550.600.650.70accuracy
arXiv - 5 way
Contrastive
PRODIGYFigure 7: In-context learning accuracy on arXiv in a 5-ways setting wrt. the number of prompt
examples (shots).
pretraining tasks. Instead, PRODIGY consistently shows an improvement in performance as more
data is pretrained on.
H Compute
We use one NVIDIA A100-SXM4-80GB GPU for all our experiments. One pretrain run of 10k steps
takes 3 to 4 hours.
I Broader Impacts
Our work aims to extend the success of in-context learning to graphs and start building toward graph
foundation models. This would allow cost-effective and accurate predictions, especially in domains
where labeled data is scarce and long tail such as network anomaly detection, rare disease diagno-
sis/treatment, supply chain disruption, and recommendations for new users. However, overreliance
on prior knowledge from pretraining could also lead to increased social bias and unfair beneﬁts to
the dominate groups. To mitigate this, pretraining data should be diverse and well-balanced, and the
pretrained models should be tested on downstream tasks over different groups and subdistributions.
15
