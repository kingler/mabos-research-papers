Hint of Thought prompting: an explainable and zero-shot approach to
reasoning tasks with LLMs
IokTong Lei
Department of Computer Science
Tsinghua University
China, Beijing
lixt22@mails.tsinghua.edu.cnZhidong Deng
Department of Computer Science
Tsinghua University
China, Beijing
michael@mail.tsinghua.edu.cn
Abstract
As a way of communicating with users and
any LLMs like GPT or PaLM2, prompting be-
comes an increasingly important research topic
for better utilization of LLMs. Although simple
prompting performs well on single-step ques-
tions, it cannot permanently activate the cor-
rect knowledge path for multi-step reasoning
tasks. The chain of thought (CoT), which often
contains zero-shot CoT and few-shot CoT, is a
recently developed prompting method that can
explain the reasoning process to the LLM and
outperforms simple prompting in three chal-
lenging reasoning tasks, including arithmetic,
symbolic, and commonsense reasoning. In
this paper, we propose a novel hint of thought
(HoT) prompting with explainability and zero-
shot generalization. First, it is decomposed
into the following three steps: explainable sub-
questions, logical reasoning, and answer extrac-
tion. Second, such three steps are sequentially
ordered in the format of step-by-step hints,
which can be easily adjusted and explained to
different tasks. Finally, experimental results
demonstrate that our HoT prompting has a sig-
nificant advantage on the zero-shot reasoning
task compared to existing zero-shot CoT. We
did zero-shot experiments on math tasks like
GSM8K, ADDSUB, AQUA, SV AMP and com-
monsense tasks such as StrategyQA. In particu-
lar, the accuracy of the proposed HoT prompt-
ing is improved with GSM8K from 40.50% to
67.80%, with AQUA from 31.9% to 46.4%,
with SV AMP from 63.7% to 76.9%, and with
ADDSUB from 74.7% to 87.34%, respectively,
which even defeats the competitive PoT ap-
proach on GSM8k, AQUA, and SV AMP.
1 Introduction
Many researchers indicate that scaling up the size
of generative language models and training datasets
plays a critical role in recent NLP research. There
are some powerful examples of large language mod-
els (LLMs), such as ChatGPT ( ?), PaLM (Chowd-
hery et al., 2022), and LAMDA (Thoppilan et al.,2022). Indicated by the robustness of GPT-4 (Ope-
nAI, 2023), many valuable applications can be de-
veloped, and (Bubeck et al., 2023) believes that
it has the potential to lead the world of artificial
intelligence (AI) to the world of artificial general
intelligence (AGI).
The success of LLMs is often related to zero-shot
or few-shot learning (in-context learning). This at-
tribute helps the model to understand and solve
different tasks by sampling a few examples (in-
context learning) or only providing the instructions
(zero-shot). The prompt engineering (Liu et al.,
2021), or prompting, is a method for the users to
interact with LLMs. To implement this method, re-
searchers either design prompts manually (Schick
and Schütze, 2021; Reynolds and McDonell, 2021)
or generate them automatically (Gao et al., 2021;
Shin et al., 2020) per task. Recently, prompt engi-
neering has become a hot topic in NLP.
Although there is a fantastic performance that
LLMs can solve single-step or intuitive tasks very
well with task-specific zero-shot prompting or in-
context learning prompting (Liu et al., 2021), mod-
els with scale up to 100B+ parameters are strug-
gling to solve multi-step reasoning tasks (Rae et al.,
2022). To address this drawback, researchers have
introduced a method called chain of thought (CoT)
prompting (Wei et al., 2022a), rather than standard
question and answer examples prompting, and it
feeds LLMs with a chain of reasoning examples
(example). In other words, CoT demonstrates a
reasoning path that composes an original complex
question into multiple more straightforward steps.
With CoT, the performance of language models on
a large scale has dramatically improved. For exam-
ple, a giant PaLM with 540B parameter (Chowdh-
ery et al., 2022) with CoT can increase the accuracy
of math calculations.
A few-shot prompting with CoT (Wei et al.,
2022a) or without CoT (Brown et al., 2020) can
finish many task-specific prompting tasks. Decom-arXiv:2305.11461v6  [cs.AI]  5 Jun 2024
posed Prompting (Khot et al., 2023) proposed a
model to decompose the task into sub-tasks and
solve them iterative; however, it requires few-shot
prompts and has to prompt a few times to get the
result. To do zero-shot reasoning, researchers pro-
posed zero-shot CoT, showing that LLMs are good
zero-shot reasoners by simply adding "let’s think
step by step" (Kojima et al., 2022). However, the
solving process is provided by the LLM without
any explanation between each step. Also, Program
of Thought (PoT) (Chen et al., 2023) performed
better using Python as an extended tool for zero-
shot math reasoning but mainly focused on math
reasoning tasks. Both works show the difficulty of
reasoning tasks with LLMs. To build an explain-
able, logical, and user-friendly end-to-end zero-
shot prompt method, we propose hint of thought
(HoT) from another point of view. To approach the
solution, we take the traditional zero-shot CoT as
an activation path that activates the correct reason-
ing path within LLMs. We propose a hints chain
that is explainable to humans and efficient for the
LLM to understand. It contains three main steps (1)
ask for five subquestions; (2) ask pseudocode as
an answer for subquestions; (3) get the answer
in a wanted way . The hints chain automatically
leads the LLM to do zero-shot reasoning by asking
small sub-questions and answering sub-questions
in pseudocode. The sub-questions can help users
understand the “mind map” of the LLM, and pseu-
docodes provide a transparent, logical reasoning
process. Moreover, answering in pseudocode can
avoid semantic ambiguity. We used GPT-3.5-turbo
(Brown et al., 2020) as our LLM; it is designed to
give a complete sentence as an answer (Ye et al.,
2023).
2 Background
2.1 Large language models and prompting
A language model (LM) is a model that is designed
for estimating the probability distribution of text.
In recent research, they found that scaling up the
model size can help improve the performance (from
a few million (Merity et al., 2016) to hundreds of
millions (Devlin et al., 2019) to hundreds of bil-
lions (Brown et al., 2020) parameters). And the
training data also becomes more extensive, e.g.,
webtest corpora (Gao et al., 2020). These improve
the abilities of pre-trained LLMs in many down-
stream NLP tasks. Unlike the classic paradigm of
"pre-train and fine-tune," an LLM that scales to100B+ parameters displays the ability to few-shot
learning (Brown et al., 2020). Using in-context
learning, we can use prompts to strongly lead the
generation to output a desired answer to a specific
task. It is called pre-train and prompts (Liu et al.,
2021).
2.2 Zero-shot CoT (Kojima et al., 2022)
Based on the drawback of few-shot CoT that costs
time and people to design the prompt, (Kojima
et al., 2022) proposed a zero-shot CoT prompt.
They added "Let’s think step by step" or a similar
text. This work showed that the potential of LLMs
is zero-shot. Compared with standard zero-shot
and zero-shot CoT, the latter significantly improved
with GSM8K from 12.5% to 40.5%.
2.3 Program of Thought (Chen et al., 2023)
To make more accurate calculations on math tasks.
PoT proposed the use of an extended tool to cal-
culate the answer. In their work, they ask the
LLM to propose and execute a Python code to
get the math answer. In their zero-shot exper-
iment, PoT performs better than zero-shot CoT
on GSM8K (57.0%), AQuA (43.9%), SV AMP
(70.8%), TabMWP (66.5%), MultiArith (92.2%),
and Avg (66.1%).
3 HoT
We propose an explainable, logical, and end-to-end
hint of thought prompting (HoT) method that uses
reasoning paths to represent knowledge to activate
more profound reasoning within an LLM. In our
work, an end-to-end hints chain (in the step-by-step
format) is used as the input of LLMs, and the LLMs
itself is required to generate explainable semantic-
level step-by-step reasoning steps without any extra
prompts. This employs a chain-like instruction to
activate more profound knowledge within the LLM.
Consequently, the input prompting to the LLM
is " [X]. Hints: hints _chain " where [X]is the
question, and hints _chain is our HoT prompting.
3.1 End-to-end hints chain
The hints chain is a chain of instructions that de-
livers the LLM with instructions step-by-step. The
purpose of the hints chain is to tell the LLM how to
generate a reasoning chain. There are three parts of
such hints chains: (1) Explainable sub-questions:
ask the LLM to partition the question into five step-
by-step sub-questions; (2) Logical reasoning: tell
the LLM to use the step-by-step sub-questions to
Figure 1: HoT examples on GSM8K
generate a pseudocode and calculate the answer;
(3) Answer extraction: ask the LLM to give a cus-
tomized format of the answer. The result produced
by the hints chain follows the order of these three
parts and provides a consequential reasoning chain
that supports its answer. It is an end-to-end way,
meaning we only need to prompt once for a single
question to get an answer, which can help the LLM
respond quickly.
3.2 Explainable question decomposition and
sub-question-wise problem solving
Although the prompt in LLM is also in the back box
representation, our HoT can produce an understand-
able and explainable consequent problem-solving
procedure. It indeed helps the users to verify the
answer more intuitively. Sub-questions not only
help the LLM to solve the whole problem but also
transparently explain the mind map of the LLM. In
our examples (Appendix), the reasoning process
and logical reasoning can be clearly seen.
3.3 Logical pseudo-code reasoning
An interpretable problem-solving chain can en-
hance LLM to solve reasoning problems more ac-
curately. However, it is likely to have semantic
ambiguity in semantic-level reasoning, although
LLM is designed to work with language. There-
fore, pseudocode, regarded as a more accurate log-
ical language in programming design, is further
exploited.
4 Experimental Results
All the experiments done with our HoT are based
on the GPT-3.5 family with GPT-3.5-turbo. Our
baselines are adopted from zero-shot CoT (Kojimaet al., 2022) with text-davinci-002 as well as PoT
(Chen et al., 2023) with code-davinci-002. Ad-
ditionally, text-davinci-002 and code-davinci-002
also belong to the GPT-3.5 family.
4.1 Tasks and datasets
We evaluate the HoT prompting on the five
datasets on the four main arithmetic reasoning
tasks: GSM8K, AQUA, SV AMP, and ADDSUB. In
addition, we complete experiments on the big com-
monsense reasoning benchmark StrategyQA. Note
that all the datasets we utilize in our experiments
are publicly released.
Dataset # of samples
GSM8K 1319
AQUA 254
SV AMP 1000
ADDSUB 395
StrategyQA 2290
Table 1: Number *# of samples in different datasets
4.2 Arithmetic tasks
For the arithmetic reasoning task, the following
four benchmarks are considered: i.e., 1) GSM8K
(Cobbe et al., 2021), 2) AQUA-RAT (Ling et al.,
2017), 3) SV AMP (Patel et al., 2021), and 4)
AddSub (Hosseini et al., 2014). The first three
datasets, especially GSM8K, are more recently pub-
lished benchmarks and have more challenges be-
cause they require multi-step reasoning to solve
problems. Also, the AQUA is a multiple-choice
question dataset. The baseline results are quoted
from zero-shot CoT (Kojima et al., 2022) and zero-
shot experiment done in PoT (Chen et al., 2023).
Method GSM8K AQUA
zero-shot CoT 40.50% 31.9%
PoT with zero-shot 57.0% 43.9
HoT (Ours) 67.80% 46.4%
Table 2: Results on GSM8K and AQUA
Method SV AMP ADDSUB
zero-shot CoT 63.7% 74.7%
PoT with zero-shot 70.8% -
HoT (Ours) 76.9% 87.34%
Table 3: Results on SV AMP and ADDSUB
We can observe that HoT has the best performance
among the zero-shot reasoning prompts. We design
the HoT prompts here, requiring a numeral answer
in the third part of the hints chain. From the results,
it is easy to see that HoT has a higher potential for
arithmetic reasoning than the previous zero-shot
approaches.
4.3 Commonsense tasks
We employ StrategyQA (Geva et al., 2021) for the
commonsense reasoning task, a large dataset that
requires the model to conduct implicit multi-hop
reasoning to answer questions. The baseline results
are from zero-shot CoT.(Kojima et al., 2022). The
reasoning accuracy with HoT is about 30% higher
than CoT. We make the HoT prompt here, which
requires a Yes/No in the third part of the hints chain.
It is readily observed from the results that HoT has
a higher potential for commonsense reasoning than
existing zero-shot approaches.
Method StrategyQA
zero-shot CoT 52.3%
HoT (Ours) 82.96%
Table 4: Results on StrategyQA
4.4 Answer verification
GPT-3.5-turbo always answers (output) in a com-
plete sentence, making it difficult to automatically
verify if the output is the same answer as that in
the metrics. To check the answer, we indepen-
dently prompt again to the LLM with its answer as
a record. In this verification prompt, we take the
model answer as user input and verify if it’s the
same as the answer given by the LLM. To verify
the reliability of our verification method, we pro-
duce a correct validation and a wrong validation by
Figure 2: Full pipeline of answer verification
manually setting up the positive and negative sam-
ples in GSM8k which is the largest dataset within
our experiment. We manually set a prompt that
always with the fake/correct answer: " Therefore,
the answer is fake/correct_answer " and then we
follow our verification process and gain a result.
In the correct validation test, we obtain 1293 true
positive samples out of 1319 samples. In the wrong
validation test, we obtain 1299 false negative sam-
ples out of 1319 samples. So, the correct validation
rate is 98.02%, and the wrong validation rate is
98.48%. This shows the reliability of our verifica-
tion method is strong and robust.
5 Error Analysis
We divide the reasoning errors into (1) reasoning
errors and (2) calculation errors. Our experimental
results illustrate that reasoning errors always occur
when the question (Error on GSM8K) has semantic
ambiguity or if it is too complex. It is hard to avoid
such errors because it is hard even for a human
being. Although the calculation error rarely occurs,
this can happen, and this can be potentially tackled
by adding an external calculator.
6 Ablation study
6.1 The number of sub-questions.
We conduct HoT experiments with three sub-
questions (HoT-3) and seven sub-questions (HoT-7)
on the SV AMP dataset, respectively, while the stan-
dard one is HoT-5 with five sub-questions. We
can see that more sub-questions may not provide a
more accurate answer. The LLMs start making sto-
ries when those steps are not needed. Also, more
sub-questions cost more for the LLMs to respond,
which takes more time and effort.
7 Related work
7.1 Complex reasoning with LLMs
Reasoning skills are essential for general intelli-
gence systems, and the ability to reason in LLMs
Method SV AMP
zero-shot CoT 52.3%
PoT with zero-shot 43.9.0%
HoT-3 46.1%
HoT-5 46.4%
HoT-7 44.4%
Table 5: Ablation study on SV AMP
gained significant attention from the research com-
munity. Several studies (Brown et al., 2020; Smith
et al., 2022) have shown that asking pre-trained
models to produce step-by-step reasoning or fine-
tuning (Cobbe et al., 2021) can increase their ability
on complex reasoning tasks. GPT-3 (Brown et al.,
2020) has illustrated its robust few-show reasoning
(Wei et al., 2022b; Wang, 2022; Chowdhery et al.,
2022) that a few examples in natural language are
given to the model to describe the task. The most
classic reasoning tasks are mathematical reasoning.
PoT (Chen et al., 2023) has shown great ability on
math reasoning tasks with LLMs with the help of
Python programs. They aim to generate an exec-
utive Python program by the LLM to solve math
problems. However, their work primarily focuses
on math reasoning tasks. A more general approach
would be CoT (Wei et al., 2022a), which works
well on mathematical, logical, common sense, and
symbolic reasoning tasks with few-shot prompts.
7.2 Zero-shot reasoning with LLMs
It was indicated that LLMs have excellent zero-
shot abilities in many system-1 tasks, including
reading comprehension, translation, and summa-
rization (Radford et al., 2019). This ability can also
be fine-tuned to get a better performance (Ouyang
et al., 2022). However, we focus on system-2 tasks
beyond system-1 tasks. The recent work, zero-shot
CoT, increases zero-shot performance. Also, PoT
in zero-shot format provides good results in math
reasoning tasks.
7.3 Discussion about Existing Work
Recently, there have been many approaches to en-
hance the reasoning ability of LLMs, including
CoT (Wei et al., 2022a), zero-shot CoT (Kojima
et al., 2022), Auto-CoT (Shin et al., 2020), PoT
(Chen et al., 2023), decomposed prompting (Khot
et al., 2023). They all aim to provide accurate rea-
soning results. On the other hand, they did not fo-
cus on answer explanation and the LLM efficiency.Therefore, HoT is proposed and gives another ap-
proach to prompt engineering.
8 Discussion
In our work, we approach the reasoning tasks from
other points of view. We care about accuracy and
want a more precise explanation of the reasoning
process. We have verified that HoT can work effi-
ciently on arithmetic and common sense reasoning
tasks and provide a clear description intuitively. Al-
though LLMs are black boxes, we tried to explain
them with a prompt approach.
9 Conclusion
This paper investigates how to generate a more
intuitively explainable end-to-end prompting on
reasoning tasks. Our experimental results on the
mathematical reasoning tasks significantly surpass
that of prior zero-shot methods with GPT-3.5 fam-
ily, CoT, and PoT. Also, on the common sense
benchmark StrategyQA, our HoT reaches a new
zero-shot GPT-3.5 result with 82.96%.
10 Limitation and Social Impact
Our work is based on GPT-3, a pre-trained lan-
guage model trained from various sources and
shown to capture and amplify biases found in the
training data. We use prompting to guarantee our
reasoning answers, which takes advantage of the
patterns learned by language models. However,
our zero-shot approach directly probes complex
reasoning inside pre-trained LLM, which can also
cause bias. Also, our ability to reason is based on
the power of the LLM. Therefore, the accuracy de-
pends on GPT-3, which may cause fluctuation in
different test environments.
References
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, et al. 2020. Language
models are few-shot learners.
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-
ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,
and Yi Zhang. 2023. Sparks of artificial general in-
telligence: Early experiments with gpt-4.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W. Cohen. 2023. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, et al. 2022. Palm: Scaling language
modeling with pathways.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The pile: An
800gb dataset of diverse text for language modeling.
Tianyu Gao, Adam Fisch, and Danqi Chen. 2021.
Making pre-trained language models better few-shot
learners. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 3816–3830, Online. Association for Computa-
tional Linguistics.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics , 9:346–
361.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren
Etzioni, and Nate Kushman. 2014. Learning to solve
arithmetic word problems with verb categorization.
InProceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 523–533, Doha, Qatar. Association for Com-
putational Linguistics.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,
Kyle Richardson, Peter Clark, and Ashish Sabharwal.
2023. Decomposed prompting: A modular approach
for solving complex tasks.
Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In Advances in
Neural Information Processing Systems , volume 35,
pages 22199–22213. Curran Associates, Inc.Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 158–167, Vancouver,
Canada. Association for Computational Linguistics.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.
OpenAI. 2023. Gpt-4 technical report.
Siqi Ouyang, Rong Ye, and Lei Li. 2022. On the impact
of noises in crowd-sourced data for speech transla-
tion. In Proceedings of the 19th International Confer-
ence on Spoken Language Translation (IWSLT 2022) ,
pages 92–97, Dublin, Ireland (in-person and online).
Association for Computational Linguistics.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 2080–2094, Online.
Association for Computational Linguistics.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, et al.
2022. Scaling language models: Methods, analy-
sis and insights from training gopher.
Laria Reynolds and Kyle McDonell. 2021. Prompt
programming for large language models: Beyond the
few-shot paradigm.
Timo Schick and Hinrich Schütze. 2021. It’s not just
size that matters: Small language models are also few-
shot learners. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 2339–2352, Online. Association
for Computational Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric
Wallace, and Sameer Singh. 2020. AutoPrompt: Elic-
iting Knowledge from Language Models with Auto-
matically Generated Prompts. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 4222–4235,
Online. Association for Computational Linguistics.
Eric Smith, Orion Hsu, Rebecca Qian, Stephen Roller,
Y-Lan Boureau, and Jason Weston. 2022. Human
evaluation of conversations is an open problem: com-
paring the sensitivity of various methods for eval-
uating dialogue agents. In Proceedings of the 4th
Workshop on NLP for Conversational AI , pages 77–
97, Dublin, Ireland. Association for Computational
Linguistics.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, et al. 2022. Lamda: Language
models for dialog applications.
Zhengxiang Wang. 2022. Linguistic knowledge in data
augmentation for natural language processing: An ex-
ample on Chinese question matching. In Proceedings
of the 5th International Conference on Natural Lan-
guage and Speech Processing (ICNLSP 2022) , pages
40–49, Trento, Italy. Association for Computational
Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2022a. Chain-of-thought prompting
elicits reasoning in large language models.
Kangda Wei, Sayan Ghosh, and Shashank Srivastava.
2022b. Compositional generalization for kinship
prediction through data augmentation. In Proceed-
ings of the 4th Workshop of Narrative Understanding
(WNU2022) , pages 13–19, Seattle, United States. As-
sociation for Computational Linguistics.
Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai
Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao
Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui,
Qi Zhang, and Xuanjing Huang. 2023. A comprehen-
sive capability analysis of gpt-3 and gpt-3.5 series
models.
A Appendix
A.1 Example on GSM8K
Question: Eliza’s rate per hour for the first 40
hours she works each week is $10. She also
receives an overtime pay of 1.2 times her regular
hourly rate. If Eliza worked for 45 hours this week,
how much are her earnings for this week?
Answer: 460
Output:
Sure! Let’s break down the question into step-
by-step sub-questions and provide answers in
pseudocode and numerals:
1. Calculate Eliza’s regular earnings for the first
40 hours at a rate of $10 per hour:
a. Pseudo-code: regular_earnings = rate_per_hour
* 40
b. Numerical calculation: regular_earnings =$10/hour * 40 hours = $400
2. Calculate Eliza’s overtime earnings for the
additional 5 hours at a rate of 1.2 times her regular
hourly rate:
a. Pseudo-code: overtime_earnings = over-
time_rate * overtime_hours
b. Numerical calculation: overtime_earnings =
$10/hour * 1.2 * 5 hours = $60
3. Calculate Eliza’s total earnings:
a. Pseudo-code: total_earnings = regular_earnings
+ overtime_earnings
b. Numerical calculation: total_earnings = $400 +
$60 = $460
Therefore, Eliza’s earnings for this week are
$460.
A.2 Example on AQUA
Question: Find out which of the following values
is the multiple of X, if it is divisible by 9 and 12?
[’A)36’, ’B)15’, ’C)17’, ’D)5’, ’E)7’]
Answer: A
Output:
Step 1: Check if the given values are divisible by
both 9 and 12.
- 36 is divisible by both 9 and 12.
- 15 is not divisible by 9.
- 17 is not divisible by 9 or 12.
- 5 is not divisible by 9.
- 7 is not divisible by 9 or 12.
Step 2: Identify which value is divisible by both
9 and 12.
- The multiple of both 9 and 12 is 36.
Therefore, the answer is A) 36.
A.3 Example on SV AMP
Question: Each pack of dvds costs 76 dollars. If
there is a discount of 25 dollars on each packHow
much do you have to pay to buy each pack?
Answer: 51.0
Output:
Sure! Let’s break down the question into five
step-by-step sub-questions:
1. What is the original cost of each pack of
DVDs?
Answer: $76
2. What is the discount on each pack?
Answer: $25
3. What is the new cost of each pack after the
discount?
Answer: Original cost - Discount = $76 - $25 = $51
4. How much do you have to pay to buy each
pack?
Answer: $51
Now, let me provide you with the answers in
pseudocode :
original_cost = 76
discount = 25
new_cost = original_cost - discount
And finally, in numerals, the answer is: $51.
A.4 Example on ADDSUB
Question: Joan found 70 seashells on the beach .
she gave Sam some of her seashells . She has 27
seashell . How many seashells did she give to Sam
?
Answer: 43
Output:
Sure! Let’s break down the question into step-by-
step sub-questions and provide the answers using
pseudocode first:
1. How many seashells did Joan originally
have?
Answer: Joan originally had 70 seashells.
2. How many seashells does Joan have after
giving some to Sam?
Answer: Joan now has 27 seashells.
3. How many seashells did Joan give to Sam?
Answer: To find the number of seashells Joan gave
to Sam, we can subtract the number of seashells
she has now (27) from the number she originally
had (70).
Pseudocode: Joan’s seashells given to Sam = 70 -
274. Convert the pseudocode into numerals:
Answer: Joan gave Sam 43 seashells.
So, Joan gave Sam 43 seashells.
A.5 Example on StrategyQA
Question: Would Firefighters be included in a
September 11th memorial?
Answer: Yes. September 11th is remembered as a
day of mourning for the lives lost during a terrorist
attack in NYC. Firefighters were among the first
responders to the crisis, and many died.
Output:
Sure! Let’s break down your question into five
step-by-step sub-questions and provide answers
using pseudocode :
1. Is September 11th a memorial day?
- Answer: Yes
2. Does the September 11th memorial commem-
orate the tragic events of the terrorist attacks on
September 11, 2001?
- Answer: Yes
3. Were firefighters involved in responding to
the terrorist attacks on September 11, 2001?
- Answer: Yes
4. Are individuals who were involved in re-
sponding to the terrorist attacks on September 11th,
including firefighters, honored and remembered in
the September 11th memorial?
- Answer: Yes
5. Are firefighters included as part of the
memorial tribute for their bravery and sacrifice
during the events of September 11th?
- Answer: Yes
So, in summary, the answer to your question is:
Yes, firefighters are included in the September 11th
memorial.
Please note that the pseudocode used here is
to illustrate the logical flow of the sub-questions
and their answers. The actual implementation may
vary based on the specific context or programming
language.
A.6 Error example on GSM8k
Question: An oil pipe in the sea broke . Before
engineers started to fix the pipe , 2475 gallons of
oil leaked into the water . A total of 6206 gallons
of oil leaked before the pipe was fixed . How many
gallons of oil leaked while the engineers were
fixing the pipe ?
Answer: 3731.0
Output:
Sure! Let’s break down the problem into step-by-
step sub-questions:
Step 1: Calculate the total amount of oil leaked
after the engineers fixed the pipe.
This can be found by subtracting the amount
leaked before the engineers started from the total
amount leaked.
Total leaked after fixing = Total leaked - Leaked
before fixing
Total leaked after fixing = 6206 gallons - 2475
gallons
Step 2: Calculate the amount of oil leaked while
the engineers were fixing the pipe.
This can be found by subtracting the total leaked
after fixing from the amount leaked before fixing.
Leaked while fixing = Leaked before fixing - Total
leaked after fixing
Step 3: Convert the answer from step 2 into
numerals.
Now let’s provide you with the pseudocode :
Step 1:
Define the variable "totalLeaked" and assign it the
value of 6206 gallons.
Define the variable "leakedBeforeFixing" and
assign it the value of 2475 gallons.
Define the variable "totalLeakedAfterFixing" and
assign it the result of subtracting leakedBeforeFix-
ing from totalLeaked.
Step 2:
Define the variable "leakedWhileFixing" and
assign it the result of subtracting totalLeakedAfter-
Fixing from leakedBeforeFixing.
Step 3:
Print the value of leakedWhileFixing.The answer in numerals is the value of leaked-
WhileFixing obtained from the pseudocode.
No, your answer is not correct. Let’s go through
the calculations again:
Step 1: Total leaked after fixing = 6206 gallons -
2475 gallons = 3731 gallons
Step 2: Leaked while fixing = 2475 gallons -
3731 gallons = -1256 gallons
It seems that we made a mistake in the calcula-
tion. The result is a negative value, which doesn’t
make sense in this context. Let’s double-check the
steps:
Step 1: Total leaked after fixing = 6206 gallons -
2475 gallons = 3731 gallons
Step 2: Leaked while fixing = 3731 gallons -
2475 gallons = 1256 gallons
So, the correct answer is 1256 gallons.
