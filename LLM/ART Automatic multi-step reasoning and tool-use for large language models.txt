ART: Automatic multi-step reasoning and tool-use for
large language models
Bhargavi Paranjape1Scott Lundberg2Sameer Singh3Hannaneh Hajishirzi1;4
Luke Zettlemoyer1;5Marco Tulio Ribeiro2
1University of Washington,2Microsoft Research,3University of California, Irvine,
4Allen Institute of Artiﬁcial Intelligence,5Meta AI
Abstract
Large language models (LLMs) can perform
complex reasoning in few- and zero-shot set-
tings by generating intermediate chain of
thought (CoT) reasoning steps. Further, each
reasoning step can rely on external tools to sup-
port computation beyond the core LLM capa-
bilities (e.g. search/running code). Prior work
on CoT prompting and tool use typically re-
quires hand-crafting task-speciﬁc demonstra-
tions and carefully scripted interleaving of
model generations with tool use. We in-
troduce Automatic Reasoning and Tool-use
(ART), a framework that uses frozen LLMs
toautomatically generate intermediate reason-
ing steps as a program. Given a new task to
solve, ART selects demonstrations of multi-
step reasoning and tool use from a task li-
brary. At test time, ART seamlessly pauses
generation whenever external tools are called,
and integrates their output before resuming
generation. ART achieves a substantial im-
provement over few-shot prompting and auto-
matic CoT on unseen tasks in the BigBench
and MMLU benchmarks, and matches perfor-
mance of hand-crafted CoT prompts on a ma-
jority of these tasks. ART is also extensible,
and makes it easy for humans to improve per-
formance by correcting errors in task-speciﬁc
programs or incorporating new tools, which
we demonstrate by drastically improving per-
formance on select tasks with minimal human
intervention.
1 Introduction
In-context learning allows large language models
(LLMs) to quickly adapt to new tasks simply by us-
ing natural language instructions and a few demon-
strations as a prompt to the LLM (Xie et al., 2021;
Brown et al., 2020; Chowdhery et al., 2022). While
this circumvents annotating large datasets or even
hosting the LLM itself (since many are available
through APIs), there are severe performance limita-
tions around multi-step reasoning (Liu et al., 2022),
Human Feedback Tool OutputTask: Translate into Pig Latin Input:  albert goes home
Frozen LLMTask: Anachronisms
Input: George HW ... Gulf
War
Q1: [search] When was
George H. W .
Bush, president?
#1: From 1989-1993 ...
Q2: [EOQ]
Ans: TrueTask: Arithmetic
Input: V iola bought 167 books...
Q1: [gen code]  Write a rithmetic as
python code
#1: viola = 167, nancy = 137
ans = viola - nancy
Q2: [exec code] Execute code
Q3: [EOQ]
Ans: 30
Q1: [search] How to write english as pig latin?
#1: Add "yay" if it starts with a vowel ...
Q2: [gen code] W rite code to translate "albert goes
driving" to pig latin.
#2:  for w in ["albert", "goes", "home"]: if w[0] in "aeiou":
print(w + "yay") ...
Q3: [exec] Execute snippet
#3: albertyay oesgay rivingday
Q4: [EOQ]
Ans: albertyay oesgay rivingday
Q1: [search] How to write english as pig latin?
#1: Add "yay" if it starts with a vowel ...
Q2: [gen code] W rite code to translate "albert ...
#2:  for w in ["albert", "goes", "home"]: if w[0] in "aeiou":
print(w + "yay") ... consonent_cluster = find_clstr(w)  
Q3: [exec code] Execute snippet
#3: albertyay oesgay ivingdray
Q4: [EOQ]
Ans: albertyay oesgay ivingdray
Task Library examplesA
B
CSelect ExamplesTask Library
LLM OutputRun Program
Fix Mistakes (optional)Figure 1: ART generates automatic multi-step decom-
positions for new tasks by selecting decompositions of
related tasks in the task libray (A) and selecting and
using tools in the tool library alongside LLM genera-
tion (B). Humans can optionally edit decompositions
(eg. correcting and editing code) to improve perfor-
mance (C).
math (Patel et al., 2021), having up-to-date informa-
tion (Komeili et al., 2022), and others. To address
these limitations, recent work proposes prompting
LLMs to mimic a chain of thought (CoT) for multi-
step reasoning (Wei et al.; Zhou et al., 2022; Wang
et al., 2022; Press et al., 2022; Khot et al., 2022;
Arora et al., 2022) or providing them with access
to tools (e.g. a calculator or QA model) to enable
more complex reasoning steps (Gao et al., 2022;arXiv:2303.09014v1  [cs.CL]  16 Mar 2023
Chen et al., 2022; Press et al., 2022; Wei et al.;
Schick et al., 2023). However, existing methods
for chained reasoning with tool use are difﬁcult to
extend to new tasks and tools, requiring ﬁne-tuning
or prompt-engineering tailored for a speciﬁc task
(Parisi et al., 2022) or tool (Schick et al., 2023).
In this paper, we present Automatic Reasoning
andTool use (ART), a framework that automati-
cally generates decompositions (multi-step reason-
ing) for instances of new tasks. The framework
also selects and uses the most appropriate avail-
able tools (like search engines, and code execution)
in individual steps. Given a new task, ART re-
trieves demonstrations of related tasks from a task
library to enable few-shot decomposition and tool
use. These demonstrations follow a ﬂexible but
structured query language (Beurer-Kellner et al.,
2022), such that it is easy to parse intermediate
steps, stop generation to call external tools, and
resume it after including the output of such tools
(Figure 1). ART provides the LLM with demonstra-
tions of how to decompose instances of several re-
lated tasks, and how to select and use any tool from
thetool library that is represented in these demon-
strations. This encourages the model to generalize
from demonstrations to decompose a new task and
use tools in appropriate places, zero-shot. It also
enables users to ﬁx any mistakes in the reasoning
chain or add new tools by simply updating the task
and tool libraries, providing new demonstrations
where necessary (e.g. for the task at hand).
We construct a task library for 15 diverse Big-
Bench (Srivastava et al., 2022) tasks, and eval-
uate ART on 19 unseen test tasks from Big-
Bench, 6 MMLU tasks, and various tasks used
by related work on tool use (SQUAD, TriviaQA,
SV AMP, MAWPS). ART consistently matches or
outperforms automatically generated CoT reason-
ing chains on 32 / 34 BigBench and all MMLU
tasks, by an average of over 22 percentage points.
Tool-use in particular improves performance on test
tasks by an average of over 12.3 percentage points,
as compared to when no tools are allowed (Table 3).
ART improves over direct few-shot prompting by
10.8% percentage points on average across unseen
BigBench and MMLU tasks. Improvements are
particularly notable on unseen tasks requiring arith-
metic and algorithmic reasoning, where ART im-
proves over direct few-shot prompting by 12.5%
and previous best-known results for GPT3 that use
supervision for decomposition and/or tool use by6.1% percentage points (Table 3).
Finally, ART enables human intervention and im-
provement of the reasoning process by simply up-
dating the task and tool libraries with new demon-
strations, making it very easy to improve perfor-
mance on any speciﬁc task with minor human feed-
back. On 12 test tasks, ART with additional hu-
man feedback surpasses the best-known results for
GPT3 by an average of over 20% points (Table 6).1
2 Related Work
Scaled ﬁnetuning for low-resource adaptation
Recent work has shown that ﬁnetuning LLMs on
a broad range of public NLP datasets (with pre-
ﬁxed instructions) is an effective technique for
cross-task generalization (Mishra et al., 2021; Sanh
et al., 2021; Khashabi et al., 2020; Wei et al.,
2021) in both the zero-shot and few-shot settings.
Ouyang et al. (2022) show that aligning language
models with user intent on a wide range of tasks
by ﬁne-tuning with human feedback for desired
model behavior (InstructGPT) further improves
in-context learning performance on complex NLP
tasks. Chung et al. (2022) show that ﬁnetuning
on an aggregated mixture of tasks (T0, CoT, dia-
log, and code datasets) together with scaling mod-
els to 540B parameters achieves state-of-the-art
in-context learning performance on several bench-
marks such as BigBench and MMLU. ART uses
API access to InstructGPT and Codex (LLM ﬁne-
tuned on code (Chen et al., 2021)) to leverage their
emergent in-context learning abilities. Future im-
provements in scaled ﬁnetuning in LLMs will likely
improve the performance on ART.
Prompting with intermediate reasoning steps
Chain-of-thought (CoT) prompting (Wei et al.,
2022; Suzgun et al., 2022) is a popular gradient-
free technique that encourages LLMs to gener-
ate intermediate reasoning steps prior to the ﬁnal
answer, with multiple task-speciﬁc variants (e.g.
Least-to-most prompting (Zhou et al., 2022), Self-
Ask (Press et al., 2022), Ask-me-anything (Arora
et al., 2022), Successive prompting (Dua et al.,
2022), decomposed prompting (Khot et al., 2022)).
While such prompts were initially hand-crafted, re-
cent work (Kojima et al., 2022) showed that LLMs
can generate CoT-style multi-step reasoning in a
zero-shot manner, when prompted with the preﬁx
“Let’s think step-by-step". Zhang et al. (2022) use
1Code is available at https://github.com/
bhargaviparanjape/language-programmes/
Table 1: Comparing ART with related approaches for
multi-step reasoning and tool-use
Feature CoT Auto Tool- ART
CoT former
Multi-step reasoning X X X
Limited supervision X X X
Tool use X X
Extendable libraries X
Cross-task transfer X X X
Human feedback X X
LLMs to automatically generate such CoT-style
prompts—AutoCoT—which are competitive with
hand-crafted prompts in their performance on arith-
metic and commonsense reasoning tasks. We com-
pare ART, CoT and AutoCoT in Table 1. ART
builds on this line of work, introducing a common
language that enables cross-task demonstrations
and ﬂexible and extensible tool use, improving ac-
curacy of intermediate reasoning steps.
Tool Use There is growing interest in overcom-
ing LLM limitations with external tools such as
search engines, web browsers, calculators, trans-
lation systems, and python interpreters (Komeili
et al., 2022; Thoppilan et al., 2022; Lazaridou et al.,
2022; Shuster et al., 2022; Nakano et al., 2021;
Thoppilan et al., 2022; Cobbe et al., 2021; Thop-
pilan et al., 2022; Gao et al., 2022; Chen et al.,
2022). Most of these approaches either require
large amounts of human supervision (Thoppilan
et al., 2022; Komeili et al., 2022) or carefully con-
structed prompts tailored to speciﬁc tasks and par-
ticular tools. An alternative line of recent work
uses self-supervision to teach LLMs to use search,
translation, and a calculator (Schick et al., 2023)—
Toolformer. In contrast, since ART does not require
any additional training or tool-speciﬁc prompts, it
allows users ﬂexibility both in terms of replacing
the underlying LLM (e.g. when a new version
of GPT-3 is released), and in replacing or adding
new tools (either general-purpose tools or tools that
are important for a speciﬁc task of interest). We
compare ART and Toolformer in Table 1. In Sec-
tion 3.4, we show how human-in-the-loop feedback
— analyzing and debugging LLM generations and
extending tool-use — can provide a large boost in
the performance of ART while also extending it
with new tools. This built-in feedback loop and
adaptive capability of ART extends the capabilities
of LLMs that are ﬁnetuning to follow instructions
and use tools.3 ART
With ART, a frozen LLM decomposes instances
of a new task into multiple steps (using external
tools whenever appropriate), despite not having ex-
plicit supervision for decomposition or tool use.
In this section, we present an overview of ART,
followed by more thorough descriptions of each
individual component. We use the Physics Ques-
tion Answering (PQA) task as a running example,
which consists of high-school physics problems.
3.1 Overview
In Figure 2, ART is presented with a new task
description and input instance. We also assume
access to a few input-output pairs (not shown), with
no decomposition or tool use supervision.
Prompt building. ART retrieves similar tasks
from a task library (Figure 2(A); Section 3.2), and
adds instances of those tasks as demonstrations in
the prompt.
A demonstration in the task library is written in a
speciﬁc format, deﬁned by a custom parsing expres-
sion grammar (PeG) (Section 3.2). The grammar
is deﬁned such that each task instance is decom-
posed into a sequence of sub-steps. Some of these
sub-steps contain symbols corresponding to tools
in atool library (Section 3.3). We refer to these
decompositions as programs, since the sequential
reasoning steps and symbolic calls to tools are sim-
ilar to a conventional program with function calls.
The resultant prompt consists of programs from
related tasks and teaches the LLM how to effec-
tively decompose instances of a new task—related
sub-steps and tools in these programs can be used
by the LLM for cross-task generalization.
In Figure 2(A), the demonstrations include calls
to both search and code tools.
Generation. At generation time (Figure 2(B)),
the LLM writes its own program. ART parses the
program as it is generated, and pauses generation
whenever a tool call is encountered in the generated
text, resuming generation after the tool is called and
its output is integrated back into the program. As
illustrated in the ﬁgure, a search engine is used
to ﬁnd the appropriate physics formula, and then
the LLM uses code generation and execution to
substitute the given values and compute the answer.
Human feedback (optional). Humans can add
new decomposition demonstrations to the task li-
brary, or add/edit tools in the tool library in order
Input: ...Q1: [search] ...#1: ...
Q2: [generate code] Use the formula Fx = Ftens*cosine(θ) to solve: Hank ...
#2: ... Fx = T*math.cos(radians)
Q3: [code execute] Execute the python code and get the value of "Fx"
#3: 58.9789 
Q4: [EOQ] Ans: 58.9789New T ask (Physics QA) Answer this high-school physics question
Input:  Hector yanks on the chain with a 72.0 N force at an angle of 35.0° above the horizontal. Determine the horizontal components of the tension force.
Does the sentence contain an anachrornism? Yes/No.
Input:  President George H. W . Bush called his generals at the outset of the 
Gulf W ar.
Q1: [search] When was President George H. W . Bush, president?
#1: George H. W . Bush's tenure started on January 20, 1989, 
and ended on January 20, 1993.
Q2: [search] When was the Gulf W ar fought? #2: The Gulf W ar was a 1990–1991
Q3: [subquestion] Could these entities have co-existed? #3: Yes. Their time
periods intersect.
Q4: [generate output] Is this an anachronism? #4: No Q5: [EOQ] Ans:  NoCode
operationsCoT-style
reasoningSearch ArithmeticString
operations
TASK LIBRAR YTOOL  LIBRAR Y
Input: ... Q1: [search] ...
#1: ... can be calculated as Ftens*cosine( θ)where θ is ...
Q2: [generate code] Use the formula Fx = Ftens*cosine(θ) to solve: Hank ...
#2:T = 72.0, theta = 35.0
radians= math.pi*theta/180
Fx = T*math.cos(radians)
Input:  Hector yanks on the chain with a 72.0 N force at an angle of 35.0° above the horizontal. 
Determine the horizontal components of the tension force.
Q1: [search] What is the formula for the horizontal component of the tension force?
#1: The formula for the horizontal component of the tension force is Tcosθ. The horizontal 
component (Fx) can be calculated as Ftens*cosine( θ) where θ is the angle which the force make
s with the horizontal in radians.  
LLM
AB
Solve these arithmetic problems using python code
Input:  Viola had 167 breads. Nancy took 137from him. How
many does V iola have now?
Q1: [generate code] W rite down arithmetic as python code
#1: viola_bought = 167, nancy_took = 137
ans = viola_bought - nancy_took
Q2: [code execute] Execute snippet #2: 30 Q3: [EOQ] Ans:  NoFigure 2: A run-through of ART on a new task, Physics QA. (A) Programs of related tasks like anachronisms and
Math QA provide few-shot supervision to the LLM — related sub-steps and tools in these programs can be used
by the LLM for cross-task generalization (shown in purple). (B) Tool use: Search is used to ﬁnd the appropriate
physics formula, and code generation and execution are used to substitute given values and compute the answer
(shown in orange).
to improve performance on a particular task of in-
terest, or in general. In Figure 3(C) a user corrects
a speciﬁc program by including a step that adds
the unit of measurement, and adds this (modiﬁed)
program to the task library. While most of our ex-
periments do not use such feedback, we show that
it is very effective at drastically improving perfor-
mance when task generalization does not happen
automatically. Further, it gives users ﬂexibility to
add custom tools without retraining of the LLM.
3.2 Task Library
We construct a library of programs for a small
seed set of tasks from Big-Bench (Srivastava et al.,
2022), a collaborative benchmark that measures
the capabilities and limitations of language mod-
els. Big-Bench tasks span categories of traditional
NLP, mathematics, commonsense reasoning, and
question-answering.
Constructing the task library. We identify ﬁve
skills that are useful across more than half of the
tasks in BigBench that encompass text classiﬁca-
tion or generation of short answers in English (see
A.1). We group tasks in the benchmark by these
skills into the following clusters:
• Arithmetic: arithmetic and algebra problems.
•Code: Generating and executing python code.
•Search and question decomposition: Single or
multi-step questions that require search•Free-form reasoning: Explaining step-by-step
reasoning in natural language
•String Operations: Reformatting/editing
strings, checking string entailment, etc.
We then select 2-4 tasks from each cluster and write
programs (decompositions) for a few instances of
each task, including calls to external tools and real
outputs of those tools. Examples of programs in
each cluster are in Appendix A.1. These programs
follow a speciﬁc grammar, as outlined below.
Program grammar The program format must
be ﬂexible in terms of task inputs , steps, and tool
calls, such that a wide variety of NLP tasks can
be covered. To do so, we deﬁne a query language
(Beurer-Kellner et al., 2022) that extends the de-
composed prompting format of Khot et al. (2022),
since it can represent decomposed reasoning steps
sequentially and incorporates function calls to ex-
ternal tools (like other LLMs). Each program con-
sists of a series of nodes — a task input node, sev-
eral sub-step nodes, and an answer node. The input
node contains the task name, a simple instruction
describing the task, and the input for an instance
of the task: “Answer this high-school Physics ques-
tion.
Input: Hector yanks...”. The input node is fol-
lowed by a sequence of sub-task nodes, represented
as a (query, answer) pair “ Qi::::;#i::::”. The
sub-task query Qihas a sub-task name and sub-
Q1: [string split] What are the letters in "nwist"
#1: %s
Q2: [string permutation] What are the possible permutations of 'nwisr'?
#2: ['nwist', 'nwits', 'nwsit', 'nwsti', 'nwtis', 'nwtsi', 'niwst', 'niwts', 'niswt',...
Q3: [lookup] which word in the list is a common English word ?
#3: twins
Q4: [EOQ]
Ans: twinsQ1: [search]...What is the formula for the horizontal component of 
the tension force?
#1: ... calculated as Ftens*cosine( θ)where θ is ...
Q2: [generate code] Use formula Fx = Ftens*cosine(θ) to solve: Hanks...
#2: Fx = T*math.cos(radians) ... print(Fx)
Q3: [code execute] Execute snippet get the value of "Fx"
#3: 58.9789
Q4: [arithmetic] Round the answer to the nearest integer
#4: 59
Q5: [add unit] Add the appropriate unit of measurement to the answer .
#5: 59 N
Q4: [EOQ]
Ans: 59 Ndef lookup(word_list ):
  import  enchant
  d = enchant.Dict("en_US")
  valid_list = []  
  for word in word_list :
    if d.check(word):
      valid_list .append(word)
(a) Correcting generated programs
by adding additional reasoning steps(b) Adding additional tool use examples and
new tool definitionsTOOL  LIBRAR Y
TASK LIBRAR YHuman feedback CFigure 3: Human feedback to ART shown for (a) PQA where reasoning steps are added to the program and; (b)
Word unscrambling where tool library is augmented with a new lookup tool.
task input (“Q1: [search] What is the formula...”),
while the sub-task answer #iis simply the output
of the sub-task (“#1: The horizontal component
(Fx) can be calculated...”). The program ends with
a dummy sub-task (“Q3: [EOQ]”), followed by a
ﬁnal answer node (“Ans: 59N”). All examples in
Figures 1 and 2 follow this format.
Task Retrieval Given a new task, ART retrieves
Ntasks from the task library to construct a dy-
namic multi-task prompt. We explore two strate-
gies to retrieve similar tasks, depending on what
data is available. If a small number of labeled
examples for the new task is available ( 50), we it-
erate over all ﬁve task clusters and select a few task
programs from each cluster to compose the prompt.
Ultimately, the task cluster with the highest perfor-
mance on the held-out set of examples is chosen
when predicting on all unlabeled examples from
the task. While this strategy requires a held-out set
of input-output pairs, no additional supervision is
needed to generate a decomposed program.
In the second strategy, we craft a few-shot
prompt (Appendix A.2) with task pairs, where each
task includes a name, instructions, and a few input-
output examples. For each pair, we provide a la-
bel of “Similar” or “Not similar”, and reasoning
(e.g. “These are related because they require solv-
ing arithmetic word problems”). At run time, we
pair the test task with every task in the task library,
and choose the highest-ranked ones based on the
log probability ratio between “Similar” and “Not
similar”. We explore both strategies in Section A.2.3.3 Tool Library
Whenever a sub-task query name matches a tool
name in the task library (e.g. “ Qi: [search]”), gen-
eration is stopped and resumed after the tool is
called and its output is incorporated into the par-
tially completed program. We seed the tool library
with the following tools (all of which have demon-
strations in the task library). In particular, we de-
scribe the symbols used to represent these tools and
their inputs. We also specify how the tool output is
incorporated back into the program. Tool-speciﬁc
implementation details and other tools added to
ART during feedback (3.4) are in Appendix A.3.
Search We use SerpAPI2, which provides an API
for Google search. The input to search is the se-
quence generated by the LLM after “ Qi: [search]”.
We extract answer box snippets when they are avail-
able or combine the top-2 search result snippets to-
gether. For PQA in Figure 2(B), the search query is
the original input followed by “What is the formula
for the horizontal component of tension force?”,
and the output is ““... horizontal component (Fx)
can be calculated as Ftens*cosine( ) ...””.
Code Generation We use the Codex (Chen
et al., 2021) model for code generation. Input
to code generation is the sequence generated by
the LM after the sub-task query symbol “ Qi:
[generate python code ]”. This argument is an
instruction for code generation and is prompted
to Codex as a multi-line comment in Python. For
example, in Figure 2, Codex is prompted the in-
struction ““Use the formula Fx = Ftens * cosine( )
to solve...”” as a comment and generates “T = 72.0,
2https://serpapi.com
theta = 35.0, ..., Fx = T*math.cos(radians)”, which
is appended to the incomplete program.
Code Execution We run Python code in a virtual
Python environment with arithmetic, symbolic, and
scientiﬁc computing packages pre-installed. The
argument to code execute is the previous sub-task’s
answer sequence “ #(i 1) ::::”, i.e. the python
code snippet to be executed. For i= 1, the task
input is used as the argument since it potentially
contains the code snippet to be executed. In Fig-
ure 2, the code snippet generated in the previous
step is executed and the value of variable “Fx” is
added to the incomplete program.
3.4 Human feedback
ART is speciﬁcally designed to be amenable to hu-
man feedback since it does not require additional
ﬁnetuning. Consequently, users can incorporate
feedback immediately into ART, by editing the
task library and/or the tool library. Since ART
generates multi-step reasoning programs that are
interpretable, we explore feedback in the form of
debugging, i.e. users editexisting programs rather
than creating programs from scratch. These edits
can be in the form of correcting sub-step outputs,
adding/removing sub-steps (with appropriate in-
puts and answers), adding calls to new tools, etc.
For example, in Figure 3(a) the user edits a pro-
gram by adding two sub-steps, in order to round
the answer to the nearest integer and include the ap-
propriate unit of measurement to the answer. This
feedback demonstrates appropriate decompositions
for the task, as these operations are still performed
by the LLM (the tool library does not have “[arith-
metic]” or “[add unit]” APIs). In contrast, in Figure
3(b) the user demonstrates the use of a dictionary
“[lookup]” andimplements it as a tool in the tool
library. While most of our experiments do not rely
on such feedback (and thus measure “zero shot”
task transfer with no supervision for reasoning/tool-
use), we show that simple operations like these can
drastically improve performance on target tasks.
4 Experimental Setup
Evaluation Datasets In addition to 15tasks in
the task library (Section 3.2), we evaluate ART on
19additional test tasks from BigBench which also
belong to the ﬁve task clusters identiﬁed in Sec-
tion 3.2. To check for cross-benchmark generaliza-
tion, we further evaluate ART on a random subset
of tasks from the MMLU benchmark (Hendryckset al., 2020). Finally, we also evaluate on a sub-
set of tasks used to evaluate Toolformer (Schick
et al., 2023), in order to compare ART to a model
ﬁne-tuned for tool use.
Details We use InstructGPT (text-davinci-002)
as the frozen LLM, and Codex as the code gen-
eration tool, with temperature set to 0:3. We set
the number of seed tasks in the prompt to N= 3
and use 2demonstration programs from each task.
We measure the preferred scoring metric for each
task as in Srivastava et al. (2022), and report per-
formance averaged over 5 runs.
Baselines ART proposes an automatic frame-
work to generate multi-step reasoning decompo-
sitions and use relevant available external tools
within those decompositions. We compare with the
following baselines:
•Few-shot/Direct : Prompting LLMs with
input-output pairs (but no intermediate rea-
soning). We use 3 examples for BigBench
and 5 examples for MMLU, as done in prior
work (Suzgun et al., 2022). We evaluate this
baseline for both, GPT-3 and Codex, and re-
port the higher of the two.
•Auto-CoT : A baseline that automatically gen-
erates multi-step reasoning in natural lan-
guage. A random subset of 5 examples is
ﬁrst used to elicit CoT-style reasoning ( Input
+ Let’s think step-by-step. ). These examples
and their generated output form the prompt for
other unseen examples of the task. This base-
line is free-form and does not include tools,
and thus allows us to verify the effectiveness
of our query language and task library. We
evaluate this baseline for GPT-3.
•ART-tool : ART with tool-use turned off, i.e.
the LLM generates the output of every sub-
step, to verify the gains from tool use.
•GPT-3 Best : Best published GPT-3/Codex
(175B) result with multi-step decomposition
and/or tool use. These often include additional
human supervision to decompose reasoning
steps, and external tools to boost performance
(with carefully constructed prompts).
Additional details about baselines and GPT-3 best
models are in Appendix A.4.
5 Results
We evaluate ART (without human feedback) on
tasks in the task library (5.1), and on a variety
Task Name (Cluster) Few Shot AutoCot ART ART GPT-3
w/o Tool Use Best
Anachronisms (Search) 71.3551.48 70.87 75.66 -
Musique (Search) 2.03512.88 10.04 19.19 15.23
Hindu Knowledge (Search) 85.02573.03 83.42 87.98 -
Known Unknown (Search) 68.90556.09 80.43 80.43 -
with ART (Search) +9.0 +17.44 +4.6 +4.0
Elementary Math QA (Arithmetic) 56.40774.52 58.04 68.04 -
Aqua-rat (Arithmetic) 20.54734.41 36.29 54.20 54.14
GSM8K (Arithmetic) 7.79721.99 53.4 71.00 71.64
Navigate (Arithmetic) 60.7761.7 72.4 72.4 85.901
with ART (Arithmetic) +30.0 +18.25 +11.4 -4.7
K’th letter concatenation (String) 3.250.64 8.19 40.00 98.02
Language games (String) 35.14518.58 11.19 23.08 -
Date Understanding (String) 37.53538.90 52.05 - 70.411
Auto Debugging (Code) 62.94538.24 55.29 62.94 -
Code Description (Code) 97.99788.67 84.67 88.00 -
Formal Fallacies (CoT) 44.84556.4 64.76 - 58.41
Hyperbation (CoT) 62.72555.4 80.80 - 72.41
with ART (Misc) +9.6 +16.4 +13.7 -15.4
with ART (Overall) +14.90 +17.17 +7.91 -9.0
Table 2: ART performance on tasks in the task library. (1Human-crafted CoT (Wei et al., 2022; Suzgun et al., 2022),
2Decomposed Prompting (Khot et al., 2022),3Self-Ask (Press et al., 2022),4PoT (Chen et al., 2022),5InstructGPT
(Ouyang et al., 2022),7Code-davinci-002 (Chen et al., 2021)). (-) For tasks using CoT reasoning, no tool use is
used.
of test tasks from BigBench, MMLU, and QA
benchmarks (5.2). Then, we show that ART can
be further improved with more compute (self-
consistency) and with human feedback (5.3).
5.1 Results on the task library
For tasks in the task library, demonstrations in
the prompt include two instances of the task it-
self, along with other instances from tasks in the
same cluster. We present results in Table 2, where
tasks are organized by skill cluster. Even with de-
composition demonstrations for only two instances,
ART drastically improves performance over few-
shot learning (+14.9 % points on average), in line
with prior work on CoT. It does not do as well on
language games, code description, and auto debug-
ging — tasks that use code generation and/or code
editing models. We observe that code generation
errors often lead to cascading errors in reasoning.
Similarly, ART outperforms AutoCoT on most
tasks even without any tool use (by 8% points on
average). We hypothesize that the program format
(and PeG grammar) is better at eliciting multi-step
reasoning from models than free-form CoT due to
the added structure to the reasoning. When tool
use is turned on, ART outperforms AutoCoT onall tasks (+17.7 % points) minus one. Tools are
called in 95% of test instances, and signiﬁcantly
improve performance (+7.91 % points). Gains from
tool use are particularly signiﬁcant for arithmetic
tasks that beneﬁt from representing the arithmetic
problem as code that executes complex arithmetic
accurately (+21.85 on average). This has also been
noted in prior work (Chen et al., 2022; Gao et al.,
2022).
Compared to the best published GPT-3 results,
ART is stronger or comparable in 5/8 tasks. For
the others, further investigation indicates that the
demonstrations provided by Khot et al. (2022) and
Suzgun et al. (2022) are just more effective than the
two programs we author for these tasks (we explore
further human feedback for these in Appendix A.5).
In sum, ART is stronger than few-shot learning and
AutoCoT on the library tasks (where we provide
2labeled decompositions), and comparable to the
best published GPT-3 results.
5.2 Test tasks (cross-task transfer)
We measure cross-task generalization on test tasks
where ART does not use explicit supervision for
decomposition and tool use. ART retrieves demon-
strations from the task library according to the ﬁrst
strategy in Section 3.2, which uses a small amount
Task Name (Cluster) Few Shot AutoCot ART ART GPT-3
w/o Tool Use Best
Test Tasks
Sentence Ambiguity (Search) 70.67551.47 71.00 73.33 -
Strategy QA (Search) 55.49527.22 59.37 66.44 -
Physics (Search) 70.09561.83 59.13 67.55 -
with ART (Search) +3.7 +22.27 + 5.9
Physics Questions (Arithmetic) 7.0255.56 6.30 20.37 -
Operators (Arithmetic) 71.23775.52 71.80 92.00 -
Unit interpretation (Arithmetic) 58.2741.20 51.4 53.99 -
Repeat copy logic (Arithmetic) 50.01715.63 31.25 44.38 -
Object Counting (Arithmetic) 39.2726.80 42.2 87.00 81.201
Penguins in a table (Arithmetic) 58.23740.40 68.86 77.85 72.341
Reasoning about objects (Arithmetic) 71.00733.33 45.35 64.34 52.691
Tracking shufﬂed objects (Arithmetic) 22.39719.44 18.14 37.67 36.321
with ART (Arithmetic) +19.0 +36.7 + 23.1 +6.1
Word Unscramble (String) 40.72732.44 23.03 42.7 -
Simple Text Editing (Code) 35.31530.21 20.74 27.65 -
CS Algorithms (Code) 73.4870.0 41.59 88.11 -
Sports Understanding (CoT) 69.74551.47 92.89 - 86.591
Snarks (CoT) 54.58557.24 57.13 - 65.21
Disambiguation QA (Free-form) 55.03548.45 55.89 - 60.621
Temporal sequences (CoT) 55.80719.70 49.5 - 81.81
Ruin names (CoT) 71.01555.28 60.22 - -
with ART (Misc) 2.4 22.5 24.37 -9.4
with ART (Overall) +6.9 +24.6 +16.7 -1.7
MMLU
College Computer Science (Search) 41.00 43.99 63.40 67.80 63.66
Astronomy (Search) 62.10 41.48 76.71 79.1 62.56
Business Ethics (Search) 61.60 48.8 77.17 81.16 72.76
Virology (Search) 50.03 49.52 71.60 71.49 50.726
Geography (Search) 77.67 57.07 70.30 71.71 81.86
Mathematics (Arithmetic) 36.67 33.77 39.50 45.66 34.56
with ART (MMLU) +14.6 +23.7 +3.0 +8.5
Table 3: ART performance on BigBench tasks and MMLU tasks. (1Human-crafted CoT (Wei et al., 2022; Suzgun
et al., 2022),5InstructGPT (Ouyang et al., 2022),6Scaled instruction ﬁnetuning (Chung et al., 2022),7Code-
davinci-002 (Chen et al., 2021)).
SQuAD T-REx SV AMP MA WPS NQ TriviaQA
GPT3 (175B) 29.90 39.8 10.0 19.8 22.6 65.9
Toolformer 33.8 53.5 29.4 44.0 17.7 48.8
ART 39.34(+5.5) 50.4(-3.1) 76.2(+46.8) 71.00(+27.0) 33.8(+16.1) 66.13(+17.33)
Table 4: Comparing ART results on GPT3 (175B) model and (Schick et al., 2023), which is a smaller GPT-J model
ﬁnetuned for tool-use. Results are reported from their paper (their code and models are not publicly available).
of labeled input-output pairs to pick a task clus-
ter and sample demonstration programs from that
cluster.3
BigBench test tasks Even though there is no de-
composition or tool use supervision, the results
in Table 3 are similar to those for tasks in the
3We compare both strategies in Appendix A.2task library. ART outperforms few-shot learning
(6.9 % points). In particular, ART has signiﬁcant
improvements on arithmetic tasks (+19.0) and is
comparable to the few-shot performance on search
tasks. Non-grammatical choices in ruin names and
choices not in the input in temporal sequences are
often incorrect, which the few-shot baseline may
potentially learn to ignore, while ART attempts to
Simple Text CS Strategy QA Physics Unit Reasoning about
Editing Algorithms Questions Interpretation colored objects
ART 27.65 88.11 66.44 20.37 53.99 64.34
+ Self Consistency 30.67(+3.0) 90.99(+2.9) 70.76(+4.3) 24.07(+3.7) 57.20(+3.2) 69.11(+4.8)
Table 5: Improving ART via self-consistency (Wang et al., 2022). Ensembling model generations over 15 runs
further boosts performance.
Task CoT ART GPT-3 Human
+Human + Human Best Feedback
CS Algorithms 0.0 23.0 88.11 92.73 73.48 C: longest common subsequence code
Reasong about objs. 33.33 67.75 64.34 98.90 71.00 C: Deﬁne object, color, count data structure
Repeat Copy Logic* 15.63 45.22 44.38 80.31 50.01 C: string edit operation
Sentence Ambiguity 51.47 72.33 73.33 83.67 70.67 C: Constrain queries to extract relevant info.
Simple Text editing* 30.21 35.31 27.65 36.11 35.31 C: string edit operation
Strategy QA* 27.22 29.19 66.44 69.15 55.49 C: Constrain queries to extract relevant info.
Physics* 61.83 68.21 67.55 72.55 70.09 A: [search] Formula that connects mass, ...
Temporal Sequences 19.70 30.22 49.5 88.00 81.8 A: [subquestion] Is X free Yam to Zam?
Track Shufﬂed objs. 19.44 36.48 37.67 99.86 36.32 C: Deﬁne object pair data struct, swap logic
Unit Interpretation* 41.2 41.2 53.99 95.0 58.2 A: [add unit] Add the right unit to the answer
Word Unscrambling* 32.44 33.40 42.70 62.11 40.72 T: lookup permutations in dictionary
Average 30.2 43.8 56.0 79.85 58.5
Table 6: Improving ART and free-form CoT via self-consistency and human-in-the-loop feedback. (*) indicates
that human-in-the-loop improvement was done over automatically generated CoT reasoning for these tasks. Feed-
back for ART includes correcting sub-steps in programs (“C:”), adding additional sub-steps(“A:”), and deﬁning
new tools(“T:”). Note that only ﬁve examples were edited for each task.
explicitly reason about them. As with library tasks,
we observe that string manipulation tasks like sim-
ple text editing, word unscrambling, and repeat
copy logic suffer from code generation errors.
As observed in the case of library tasks, ART
is better than AutoCoT on almost all tasks (24.6
% points). Tools are once again called very fre-
quently (89% of instances), and are responsible for
a signiﬁcant fraction of the gains over baselines.
When compared to the best published GPT-3
results, ART performs favorably on average, es-
pecially on arithmetic tasks (+6.1 % points). As
before, it does worse in tasks where good human
demonstrations of how to decompose the task it-
self(provided by Suzgun et al. (2022)) have a big
impact. We re-evaluate ART with more human
feedback on these tasks in 5.3, but even without
that we conclude that ART is competitive on Big-
Bench even when we do not have supervision for
decompositions for the task at hand (i.e. there is
cross-task generalization).
Other benchmarks To make sure ART does not
overﬁt to BigBench-style tasks, we evaluate per-
formance on additional benchmarks. We report
performance on randomly selected tasks from the
MMLU benchmark (Hendrycks et al., 2020) inTable 3, where ART is more effective than all base-
lines on 5/6 tasks (+8.5 points better than few-shot
baseline on average), despite having no supervision
for demonstrations or tool use. MMLU requires
extensive world knowledge, and thus most of these
tasks beneﬁt the most from the search tool.
In Table 4, we compare ART to a random subset
of tasks used to evaluate Toolformer (Schick et al.,
2023), a model ﬁnetuned to use a variety of tools.
The comparison is not exact since Toolformer uses
a smaller GPT-J model, but it is informative that
ART outperforms Toolformer by a large margin on
5/6 of these tasks. To make sure these gains are not
simply a result of model scale, we also use vanilla
GPT-3 as a baseline, which yields much worse re-
sults than ART on all tasks. Besides improved
performance, we note again that ART does not re-
quire additional ﬁne-tuning when new tools or new
base LLMs are introduced, and also is amenable
to further improvement at the cost of compute or
human feedback.
5.3 Improving ART
Self-consistency Previous work has noted bene-
ﬁts in generating multiple LLM outputs and tak-
ing the most frequent answer (a process known
as self-consistency), particularly for settings with
multi-step reasoning (Khot et al., 2022; Wang et al.,
2022). In Table 5, we present self-consistency re-
sults (generating 15outputs) for ART on a subset
of tasks and see that it consistently improves per-
formance, at the cost of extra computation.
Human feedback We also pilot the use of task-
speciﬁc feedback in Table 6, by having one of
the authors edit5random instances of model-
generated programs that resulted in errors for each
task. When editing, we correct errors in sub-steps
(denoted as “C:”), adds missing substeps (“A:”), or
deﬁnes a new tool and demonstrates its use (“T:”).
For example, this involved introducing an “add
unit” sub-step for the PQA task, and implementing
a dictionary lookup function as a tool for the “Word
Unscrambling” task (both illustrated by Figure 3).
We also compare human feedback applied to
CoT-style reasoning. Suzgun et al. (2022) already
provide reference CoT-style reasoning for some
tasks. For datasets where human-authored CoT
reasoning is unavailable, we correct the output of
the automatic CoT baseline, as indicated in Table 6.
The same author edits5random instances of Auto-
CoT decompositions that lead to errors on the same
tasks, correcting errors in sub-steps or adding new
sub-steps. As a reference, the edits included 35%
of tokens in the baseline, and 15.7% of tokens in
the ART programs. This included correcting sub-
step arguments and outputs in 72% of the chosen
tasks and adding additional sub-steps in 44% of
the tasks. New tool deﬁnitions were added for two
tasks — dictionary lookup for word unscrambling
and a Prolog engine for formal fallacies.
In both cases, editing programs and adding them
as demonstrations leads to signiﬁcant gains in per-
formance on the task at hand. However, the gain is
much more dramatic in ART, leading it to consis-
tently outperform the best published GPT-3 base-
line for the task at hand. Further, these corrected
programs and tools can be added to the task and
tool libraries, and our prior results in Table 3 sug-
gest that they potentially help improve ART on
other tasks as well. This pilot indicates that be-
sides being competitive on cross-task generaliza-
tion, ART is very amenable to task-speciﬁc im-
provement with minimal human intervention. We
report similar results in the task library in A.5.
6 Conclusion
We introduce ART, a gradient-free approach for
automatic multi-step reasoning generation andautomatic tool-use for a large black-box lan-
guage model. Our main contributions include a
lightweight grammar to represent multi-step reason-
ing as a program (with tool calls and arguments),
an extensible library of seed tasks for which pro-
grams are authored, and a tool library that con-
sists of useful external utilities like search, code
generation, and execution. The interpretable rea-
soning framework also allows humans to improve
task decomposition and tool use to boost perfor-
mance. ART achieves a substantial improvement
over few-shot prompting and automatic generation
of CoT reasoning on unseen tasks in the BigBench
and MMLU benchmarks, and substantially exceeds
performance on hand-crafted CoT prompts when
human feedback is incorporated. ART also beneﬁts
from approaches such as self-consistency, or from
new and more powerful LLMs trained for tool use.
References
Simran Arora, Avanika Narayan, Mayee F Chen, Lau-
rel J Orr, Neel Guha, Kush Bhatia, Ines Chami, Fred-
eric Sala, and Christopher Ré. 2022. Ask me any-
thing: A simple strategy for prompting language
models. arXiv preprint arXiv:2210.02441 .
Luca Beurer-Kellner, Marc Fischer, and Martin Vechev.
2022. Prompting is programming: A query lan-
guage for large language models. arXiv preprint
arXiv:2212.06094 .
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021. Evaluating large lan-
guage models trained on code. arXiv preprint
arXiv:2107.03374 .
Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reason-
ing for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588 .
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022. Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311 .
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-ﬁnetuned language mod-
els.arXiv preprint arXiv:2210.11416 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training veriﬁers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Dheeru Dua, Shivanshu Gupta, Sameer Singh, and
Matt Gardner. 2022. Successive prompting for
decomposing complex questions. arXiv preprint
arXiv:2212.04092 .
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language
models. arXiv preprint arXiv:2211.10435 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2020. Measuring massive multitask language
understanding. arXiv preprint arXiv:2009.03300 .
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. Uniﬁedqa: Crossing format
boundaries with a single qa system. arXiv preprint
arXiv:2005.00700 .
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao
Fu, Kyle Richardson, Peter Clark, and Ashish Sab-
harwal. 2022. Decomposed prompting: A modular
approach for solving complex tasks. arXiv preprint
arXiv:2210.02406 .
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large
language models are zero-shot reasoners. arXiv
preprint arXiv:2205.11916 .
Mojtaba Komeili, Kurt Shuster, and Jason Weston.
2022. Internet-augmented dialogue generation. In
Proceedings of the 60th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers) , pages 8460–8478, Dublin, Ireland.
Association for Computational Linguistics.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech
Stokowiec, and Nikolai Grigorev. 2022. Internet-
augmented language models through few-shot
prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115 .
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay
Mohta, Tenghao Huang, Mohit Bansal, and Colin
Raffel. 2022. Few-shot parameter-efﬁcient ﬁne-
tuning is better and cheaper than in-context learning.
arXiv preprint arXiv:2205.05638 .
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
arXiv preprint arXiv:2104.08773 .Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
et al. 2021. Webgpt: Browser-assisted question-
answering with human feedback. arXiv preprint
arXiv:2112.09332 .
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow in-
structions with human feedback. arXiv preprint
arXiv:2203.02155 .
Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:
Tool augmented language models. arXiv preprint
arXiv:2205.12255 .
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 2080–2094, Online.
Association for Computational Linguistics.
Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A Smith, and Mike Lewis. 2022. Measuring
and narrowing the compositionality gap in language
models. arXiv preprint arXiv:2210.03350 .
Victor Sanh, Albert Webson, Colin Raffel, Stephen H
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun
Raja, et al. 2021. Multitask prompted training en-
ables zero-shot task generalization. arXiv preprint
arXiv:2110.08207 .
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì,
Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, and Thomas Scialom. 2023. Tool-
former: Language models can teach themselves to
use tools. arXiv preprint arXiv:2302.04761 .
Kurt Shuster, Mojtaba Komeili, Leonard Adolphs,
Stephen Roller, Arthur Szlam, and Jason We-
ston. 2022. Language models that seek for
knowledge: Modular search & generation for di-
alogue and prompt completion. arXiv preprint
arXiv:2203.13224 .
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615 .
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi,
Denny Zhou, et al. 2022. Challenging big-bench
tasks and whether chain-of-thought can solve them.
arXiv preprint arXiv:2210.09261 .
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
et al. 2022. Lamda: Language models for dialog
applications. arXiv preprint arXiv:2201.08239 .
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022. Self-consistency
improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171 .
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903 .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou,
et al. Chain-of-thought prompting elicits reasoning
in large language models. In Advances in Neural
Information Processing Systems .
Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2021. An explanation of in-context
learning as implicit bayesian inference. arXiv
preprint arXiv:2111.02080 .
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2022. Automatic chain of thought prompt-
ing in large language models. arXiv preprint
arXiv:2210.03493 .
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
Least-to-most prompting enables complex reason-
ing in large language models. arXiv preprint
arXiv:2205.10625 .
A Appendix
A.1 Task Library
Library Design We analyzed input-output in-
stances of all 200 tasks in BigBench, ﬁltered out
text classiﬁcation and short answer generation tasks
in English, and created a list of reasoning skills that
were relevant to solving each task. We do not focus
on long text understanding, long text generation,
and multi-lingual tasks in this work. We ﬁnd that
most of these tasks rely on a few common skills
mentioned below:
Visual Reasoning, Temporal Reasoning, Proposi-
tional logic, Natural Logic, Machine Translation,
Web Search, Knowledge Base or Database lookup,
Recursive sub-question decomposition, Long textunderstanding, Database Operations, Algebra and
Arithmetic, Code Generation and Editing, Text Tag-
ging/Annotation(linguistic markers), Specialized
Search(eg. looking up linguistic knowledge, sci-
entiﬁc knowledge etc), String editing, Recursive
operations over multiple choices, Topic classiﬁca-
tion, Evidence extraction, conditional Text Genera-
tion/Editing, and Sentence similarity.
In this work, we choose to focus on the ﬁve most
used skills that cover a signiﬁcant proportion of
BigBench tasks for classiﬁcation (over 50 of the
91 tasks that remained after ﬁltrating out long-text
understanding, generation, and multi-lingual tasks).
We randomly select 2-4 tasks from each of these 5
task clusters and author decomposed programs with
appropriate tool use for these tasks. This results in
a total of 15 tasks that compose the task library .
•Arithmetic: Elementary MathQA, Grade
school math (GSM8K), arithmetic Questions
about ratios (Aqua-Rat), Navigate
• Code: Auto Debugging, Code Description
•Search and question decomposition:
Anachronims, Multi-step question answering
(Musique), Hindu Knowledge, Known
Unknown
•Free-form reasoning: Formal fallacies, Hyper-
bation
•String Operations: Kth letter concatenation,
Language games, Date understanding
Cluster Programs The programs written for
tasks in each task cluster are shown in Table 7
for tasks involving string editing and manipulation,
in Table 8 for arithmetic and algebra tasks, in Ta-
ble 10 for code generation, editing and debugging
tasks, in Table 9 for tasks beneﬁt from search of
world knowledge, and in Table 11 for tasks that
beneﬁt from eliciting chain-of-thought reasoning
following the prompt “Let’s think step-by-step”.
Program Format We deﬁne a parsing expres-
sion grammar (PEG) (shown in Figure 4) that de-
scribes the language used to write multi-step rea-
soning programs. This grammar is designed to
parse full programs of the form “Input: ... Q1: ...
#1:...Qn: [EOQ] Ans: ”. We use the python li-
brary parsimoneous4to construct the grammar and
parse programs generated by LLMs.
4https://pypi.org/project/parsimonious/
String Operations
In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the
task. You can use string operations like splitting, reformatting, editing or merging. You can also use other operations like
arithmetic and logic.
Description: (Date Understanding) Find the required date in MM/DD/YYYY using information about related events and
dates in the input. Clue: First ﬁnd what day is today.
Input: The deadline is Jun 1, 2021, which is 2 days away from now. What is the date 24 hours later in MM/DD/YYYY?
Q1: [string reformat] Jun 1, 2021 in MM/DD/YYYY
#1: 06/01/2021
Q2: [arithmetic] 06/01/2021 is 2 days away from now. What date is today?
#2: Today is 04/01/2021
Q3: [arithmetic] What date is 24 hours later than today?
#3: 05/01/2021
Q4: [EOQ]
Ans: 05/31/2021
—-
Description: (Language games) Translate English into Pig Latin.
Input: (English) Sami made his way across the bar and hugged Layla.
Q1: [string split] What are the words in "Sami made his way across the bar and hugged Layla."?
#1: ["Sami", "made", "his", "way", "across", "the", "bar", "and", "hugged", "Layla", "."]
Q2: [string edit] Transfer the initial consonant of each word to the end of the word and adding "ay" after it.
#2: ["Amisay", "ademay", "ishay", "ayway", "acrossyay", "ethay", "arbay", "andyay", "uggedhay", "Aylalay", "."]
Q3: [string merge] Concatenate #2 into a full sentence.
#3: Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay.
Q4: [EOQ]
Ans: Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay.
—-
Description: (Kth letter concatenation) Take the letters at position 3 of the words in a list of words and concatenate them
using a space.
Input: Take the letters at position 3 of the words in "Savita Saeed Ramos Sato Yadav" and concatenate them using a space.
Q1: [string split] What are the words in "Savita Saeed Ramos Sato Yadav"?
#1: ["Savita", "Saeed", "Ramos", "Sato", "Yadav"]
Q2: [string index] What is the third letter of words in the list in #1?
#2: ["v", "e", "m", "t", "d"]
Q3: [string merge] Concatenate #2 with spaces
#3: "v e m t d"
Q4: [EOQ]
Ans: v e m t d
—-
Descripton: %s
Input: %s
Q1:
Table 7: Programs in the task library for tasks requiring string manipulation.
.
Arithmetic
In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task.
You can generate python code to solve arithmetic and algebra equations in using functions from sympy.
from sympy import Symbol
from sympy import simplify
import math
from sympy import solve_it
# solve_it(equations, variable): solving the equations and return the variable value.
Description: (Aqua-rat) Solve the following arithmetic problems on ratios and fractions, writing out intermediate arithmetic
calculations as python code. Store your result as a variable named ’ans’.
Input: In a ﬂight of 600 km, an aircraft was slowed down due to bad weather. Its average speed for the trip was reduced by
200 km/hr and the time of ﬂight increased by 30 minutes. The duration of the ﬂight is: A)1 hour B)2 hours C)3 hours D)4
hours E)5 hours
Q1: [generate python code] write python code to solve the problem, using math and sympy.
#1:
duration = Symbol(’duration’, positive=True)
delay = 30 / 60
total_disntace = 600
original_speed = total_disntace / duration
reduced_speed = total_disntace / (duration + delay)
solution = solve_it(original_speed - reduced_speed - 200, duration)
ans = solution[duration]
print(ans)
Q2: [code execute] Execute the python code in #1 and get the value of "ans"
#2:
1.0
Q3: [compare] Which of the options among A)1 hour B)2 hours C)3 hours D)4 hours E)5 hours is most similar to the answer?
#3: A
Q4: [EOQ]
Ans: A
—-
Description: (Elementary Math) Solve the following middle-school arithmetic problems, writing out intermediate arithmetic
calculations as python code. Store your result as a variable named ’ans’.
Input: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes mufﬁns for her friends every
day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she
make every day at the farmers’ market?
Q1: [generate python code] write down the arithmetic or algebra equations as python code, storing the answer as ’ans’
#1:
total_eggs = 16
eaten_eggs = 3
baked_eggs = 4
sold_eggs = total_eggs - eaten_eggs - baked_eggs
dollars_per_egg = 2
ans = sold_eggs * dollars_per_egg
print(ans)
Q2: [code execute] Execute the python code in #1 and get the value of "ans"
#2: 18
Q3: [EOQ]
Ans:18
—-
Description: (Grage school Math) Solve the following middle-school arithmetic problems, writing out intermediate arithmetic
calculations as python code. Store your result as a variable named ’ans’.
Input: Joseph and Getty went to buy ice creams, they together bought 36 ice creams. On the way back, Joseph ate 12 of the
ice creasm, and he has 2 ice creams left now.
Q1: [generate python code] write down the arithmetic or algebra equations as python code, storing the answer as ’ans’
#1:
num_ice_creams_bought_by_joseph = 2 + 12
total_ice_creams = 36
ans = total_ice_creams - num_ice_creams_bought_by_joseph
print(ans)
Q2: [code execute] Execute the python code in #1 and get the value of "ans"
#2: 22
Q3: [EOQ]
Ans: 22
—-
Descripton: %s
Input: %s
Q1:
Table 8: Programs in the task library for tasks requiring arithmetic operations.
.
Search
In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task.
You can use search functions like Google search in one or more of your substeps, if there in insufﬁcient information. Other
functions like arithmetic and logical operations can also be used.
Description: (Knwon or Unknwon) Choose the option that best answers the question. If the question does not have a known
answer, choose "Unknown".
Input: How many hairs were on Neil Armstrong’s head when he landed on the moon?
choice: Unknown
choice: Five million
Q1: [search] How many hairs were on Neil Armstrong’s head when he landed on the moon?
#1:
Apollo 11 (July 16–24, 1969) was the American spaceﬂight that ﬁrst landed humans on the Moon. Commander Neil
Armstrong and lunar module pilot Buzz Aldrin.
Neil Alden Armstrong (August 5, 1930 – August 25, 2012) was an American astronaut and aeronautical engineer who became
the ﬁrst person to walk on the Moon.
Q2: [subquestion] Does the information help answer the question? There could be no deﬁnitive answer because the question is
too speciﬁc, about personal details not in public record, because the answer is not yet known, or the question is opinion-based.
#2: No. The question is too speciﬁc
Q3: [compare] What is the ﬁnal answer?
#3: Unknown
Q4: [EOQ]
Ans: Unknown
—-
Description: (Anachronisms) An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper
time. Does the sentence contain an anachrornism? Answer Yes/No.
Input: President George H. W. Bush called his generals to the Oval Ofﬁce at the outset of the Gulf War.
Q1: [tag] What are the entities in this sentence?
#1:
President George H. W. Bush
Gulf War
Q2: [search] When was President George H. W. Bush president?
#2: George H. W. Bush’s tenure as the 41st president of the United States began with his inauguration on January 20, 1989,
and ended on January 20, 1993.
Q3: [search] When was the Gulf War fought?
#3: The Gulf War[b] was a 1990–1991 armed campaign waged by a 35-country military coalition in response to the Iraqi
invasion of Kuwait.
#4: [subquestion] Could these entities have co-existed based on thier time periods alone?
Yes. Their time periods intersect.
Q5: [generate output] Is this an anachronism?
#5: No
Q6: [EOQ]
Ans: No
—-
Description: (Hindu Knowledge) Answer questions about Hindu mythology by choosing the option that best answers the
question.
Input: In the Mahabharata, Karna is cursed to forget the incantations needed to use which weapon?
choice: Anjalikastra
choice: Narayanastra
choice: Agneyastra
choice: Brahmastra
Q1: [search] In the Mahabharata, Karna is cursed to forget the incantations needed to use which weapon?
#1: As a result, he cursed Karna, saying that HIS MARTIAL SKILLS, including the use of BRAHMASTRA, would abandon
him when he needed them most. Indra, the King of Gods, stung Karna in the form of a bee to get him cursed by Parshuram.
Karna walked through the woods in despair, feeling dejected by the curse. A skilled & devoted warrior...
Q2: [compare] Which option is the answer in #3 most similar to?
#2: Brahmastra
Q3: [EOQ]
Ans: Brahmastra
—-
Descripton: %s
Input: %s
Q1:
Table 9: Programs in the task library for tasks requiring search for world knowledge.
.
Code Generation and Editing
In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task.
You can use a python code generation and execution function in one or more of your substeps, if required. Other functions
like arithmetic and logical operations can also be used.
Description: (Auto Debugging) Debug the following code snippets by ﬁnding the answer or the error message.
Input:
“‘
if x < 5:
pass
“‘
What error does this program surface?
Q1: [execute] Execute the following python code snippet.
if x < 5:
pass
#1:
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
NameError: name ’x’ is not deﬁned
Q2: [generate answer] What is the ﬁnal error message?
#2: NameError: name ’x’ is not deﬁned
Q3: [EOQ]
Ans: NameError: name ’x’ is not deﬁned
—-
Description: (Code description) Given a python code snippet, choose the option that is the best description of the code
snippet.
Input:
Python code:
try:
n = int(input())
m = int(input())
integer_sum = int(n) + int(m)
print(integer_sum)
except:
print(’error’)
choice: prints number between 5 and 6
choice: try input and except error
choice: inputs the string ’try’
choice: prints sum of two input numbers only if they are integers otherwise raises error
Q1: [code generate] prints number between 5 and 6
#1:
import random
print(random.uniform(5,6))
Q2: [code generate] try input and except error
#2:
try:
ﬁle = open(ﬁle_name, "r")
data = ﬁle.read()
ﬁle.close()
except:
print(’error’)
Q3: [code generate] inputs the string ’try’
#3: print(’try’)
Q4: [code generate] prints sum of two input numbers only if they are integers otherwise raises error
#4:
#!/usr/bin/python
a=raw_input("enter ﬁrst number: ")
b=raw_input("enter second number: ")
try:
sum=int(a)+int(b)
print "sum is: ",sum
except:
print "enter integer values only"
Q5: [compare] Which of the generated code snippets are most like the original one?
#5: prints sum of two input numbers only if they are integers otherwise raises error
Q6: [EOQ]
Ans: prints sum of two input numbers only if they are integers otherwise raises error
—-
Descripton: %s
Input: %s
Q1:
Table 10: Programs in the task library for tasks requiring code generation or editing.
.
CoT Reasoning
In these examples, you are given a task description and an input. Break the input down into subtasks in order to solve the task.
Thinking though the problem explicitly can be one of the substeps you use.
Description: (Sports Understanding) Determine whether an artiﬁcially constructed sentence relating to sports is plausible.
The ﬁnal answer should be "yes" or "no".
Input: Is the following sentence plausible? "Santi Cazorla scored a touchdown."
Q1: [think step-by-step]
#1: Let’s think step-by-step. Santi Cazorla is a soccer player. Touchdown is part of American football and rugby. So the
answer is no.
Q2: [EOQ]
Ans: no
—-
Description: (Hyperbation) Identify correct adjective ordering from the two choices. This involves selecting what would be
considered the more inexplicably "intuitive" sentence by a native English speaker.
Input: Which sentence has the correct adjective order:
Options:
(A) repulsive small Brazilian exercise ship
(B) Brazilian repulsive exercise small ship
Q1: [think step-by-step]
#1: Let’s think step-by-step. When there is more than one adjective before a noun, the adjectives need to respect the following
order before a noun: "[1. opinion] [2. size] [3. age] [4. shape] [5. color] [6. origin] [7. material] [8. purpose] noun".
Option (A): "repulsive small Brazilian exercise ship". (1) "repulsive" falls into the opinion category. (2) "small" falls into the
size category. (3) "Brazilian" falls into the origin category. (4) "exercise" falls into the purpose category. Option (A) has the
following adjective order: [1. opinion] [2. size] [6. origin] [8. purpose] (or, in numeric terms, 1 2 6 8). Because 1 < 2 < 6 < 8
is correct, (A) has the correct ordering.
Option (B): "Brazilian repulsive exercise small ship". Option (B) has the following adjective order: [6. origin] [1. opinion] [8.
purpose] [2. size] (or, in numeric terms, 6 1 8 2). Because 6 < 1 < 8 < 2 is not correct, (B) does not have the correct ordering.
So the answer is (A).
Q2: [EOQ]
Ans: (A)
—-
Description: (Formal Fallacies) Distinguish deductively valid syllogistic arguments from formal fallacies, paying speciﬁc
attention to negations.
Input: "It is not always easy to see who is related to whom – and in which ways. The following argument pertains to this
question: To begin with, Lesley is a close friend of Fernando. Moreover, being a close friend of Fernando or a schoolmate of
Lowell is sufﬁcient for being a great-grandfather of Leroy. It follows that Lesley is a great-grandfather of Leroy."
Is the argument, given the explicitly stated premises, deductively valid or invalid?
Options:
- valid
- invalid
Q1: [think step-by-step]
#1:
Let’s think step-by-step.
(1) Lesley is a close friend of Fernando: Lesley = friend(Fernando).
(2) Being a close friend of Fernando or a schoolmate of Lowell is sufﬁcient for being a great-grandfather of Leroy: If X =
friend(Fernando) OR SCHOOLMATE(Lowell), then X = great-grandfather(Leroy).
Hypothesis: Does it follow that Lesley is a great-grandfather of Leroy: Lesley = great-grandfather(Leroy)?
Let’s see whether the Hypothesis can be deduced from the arguments (1) and (2) by logical reasoning?
By (1), we have Lesley = friend(Fernando). By (2), we have if Lesley = friend(Fernando), then Lesley = great-
grandfather(Leroy).
So, it is true that Lesley is a great-grandfather of Leroy. So the answer is valid.
Q2: [EOQ]
Ans: valid
—-
Description: (Reasoning about colored objects) Given a collection of colored objects in the text input, answer the question at
the end of the input.
Input: On the nightstand, there is a red pencil, a purple mug, a burgundy keychain, a fuchsia teddy bear, a black plate, and a
blue stress ball. What color is the stress ball?
Q1: [think step-by-step]
#1: Let’s think step-by-step. According to this question, the color of the stress ball is blue. So the answer is blue.
Q2: [EOQ]
Ans: blue
—-
Descripton: %s
Input: %s
Q1:"""
Table 11: Programs in the task library for tasks requiring free-form chain-of-thought style reasoning about logic
and lingusitics.
.
A.2 Task Selection
When provided new task description and input in-
stance, ART retrieves N tasks from the task library
to constructs a dynamic multi-task prompt. We
explore two strategies for task selection.
Task-Cluster based 50 examples used for tun-
ing except in cases with fewer than 100 examples,
where we reduce this number to 10.
We iterate over all ﬁve task clusters in the library,
prompting the LLM with demonstration programs
from just one cluster at a time. For example, we
only use programs from arithmetic tasks as demon-
strations in the prompt in one such iteration. The
task cluster with the highest performance on the
held-out set of examples ( 50) is chosen. This strat-
egy requires as many API calls as there are task
clusters, and a held-out set of input-output pairs for
the new task. Note that no additional supervision is
needed for the new task to generate a decomposed
program.
LLM-Similarity based The LLM is prompted
with pairs of tasks. Some pairs contain two tasks
from the same cluster and are labeled "Similar"
while some pairs don’t and are labeled "Not simi-
lar". Additionally, we also provide reasoning for
the decision — “Elementary math QA and GSM8K
are related tasks because they both require solving
arithmetic word problems" . A task in this prompt
is represented by its name, an instruction, and a few
input-output pairs. We use the prompt in Table 13
to prompt LLMs.
The LLM is prompted for a decision for every
library task paired with the new task. We choose
the top-N tasks ranked by the ratio of log proba-
bilities of "Similar" to "Not similar". This strategy
requires fewer held-out examples but is prone to
high variance in performance based on the tasks
chosen in every experimental run. For PQA, the
most similar tasks chosen based on the LLM-based
similarity are anachronisms and GSM8K.
In Table 12, we examine the effect of chang-
ing the task selection strategy in ART. Instead of
choosing the task cluster with the highest held-out
performance over 50 examples, we use the LLM-
based similarity score to choose task programs for
the prompt. This strategy is worse on average com-
pared to tuning performance on a held-out set and
has high variance over several runs where differ-
ent tasks are chosen by the LLM. Selecting similar
tasks that share sub-tasks and tools (without any su-pervision) is still a challenging task for LLMs, and
will explore this direction further in future work.
A.3 Tool Use
Code Generation We use the Codex (Chen
et al., 2021) model for code generation. Argument
for code generation is the previous sub-task’s
answer sequence ““ #i 1 ::::"” and the
sequence generated by the LM after the sub-task
query symbol ““ Qi: [generate python code ]"”.
Wheni= 1, the instance input is used as the ﬁrst
argument. We include the previous answer/input
since it often contains information relevant to
generating accurate code, like the arithmetic word
problem for which code needs to be generated
(see Table 8 for examples). Both arguments are
provided to Codex as a multi-line python comment,
while maintaining their original formatting. To
keep the answer variable consistent, we also
append an additional instruction: Store the ﬁnal
answer in variable ’ans’ and print it. For example:
Janet’s ducks lay 16 eggs per day. She
eats three for breakfast every morning
and bakes muffins for her friends every
day with four. She sells the remainder
at the farmers market daily for \$2 per
fresh duck egg. How much in dollars does
she make every day at the farmers
market?
is used to prompt Codex as follows:
"""
Janet’s ducks lay 16 eggs per day. She
eats three for breakfast every morning
and bakes muffins for her friends every
day with four. She sells the remainder
at the farmers market daily for \$2 per
fresh duck egg. How much in dollars does
she make every day at the farmers
market?
Write down the arithmetic or algebra
equations as python code, storing the
answer as ’ans’ and print it.
"""
Codex generation temperature is set to 0.3 and the
maximum length to 500 tokens, with “print(ans)”
used as the stopping criterion.
Code Editing We use the Codex (Chen et al.,
2021) model for code generation and code edit-
ing. Arguments for both include the previous
Simple Text CS Strategy QA Physics Unit Reasoning about
Editing Algorithms Questions Interpretation colored objects
Best task cluster 27.65 88.11 66.44 20.37 53.99 64.34
LLM-based task sim. 38.30 83.71 60.39 14.06 43.56 62.00
Table 12: Comparing ART results on GPT3 (175B) model with two similar task selection strategies. LLM-based
similarity is worse on average compared to just choosing the best task cluster.
Prompt to LLM for selecting similar tasks
Give two tasks with their descriptions and examples of inputs and outputs for the tasks, determine if they are similar. Two
tasks are similar if require common subtasks like string operations, web search, translation, arithmetic, code execution, etc.
—-
Task1: [Date understanding] Find the required date in MM/DD/YYYY using information about related events and dates in the
input. Input: The deadline is Jun 1, 2021, which is 2 days away from now. What is the date 24 hours later in MM/DD/YYYY?
The ﬁnal answer is 05/01/2021.
Task2: [Language Games] Translate English into Pig Latin. Input: English sentence is "Sami made his way across the bar
and hugged Layla". The ﬁnal answer is "Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay."
Are these similar? Yes. They both require answering in a spciﬁc string format.
—-
Task1: [K’th letter concatenation] Take the letters at position 3 of the words in a list of words and concatenate them using a
space. Input: What are the words in "Savita Saeed Ramos Sato Yadav"? The ﬁnal answer is "v e m t d".
Task2: [Language Games] Translate English into Pig Latin. Input: English sentence is "Sami made his way across the bar
and hugged Layla". The ﬁnal answer is "Amisay ademay ishay ayway acrossyay ethay arbay andyay uggedhay Aylalay."
Are these similar? Yes. They both require accessing and manipulating characters in strings.
—-
Task1: [K’th letter concatenation] Take the letters at position 3 of the words in a list of words and concatenate them using a
space. Input: What are the words in "Savita Saeed Ramos Sato Yadav"? The ﬁnal answer is "v e m t d".
Task2: [Known Unknown] Choose the option that best answers the question. If the question does not have a known answer,
choose "Unknown". Input: How many hairs were on Neil Armstrong’s head when he landed on the moon? The ﬁnal answer
is "Unknown".
Are these similar? No. Task 1 requires manipulating strings and Task 2 requires answering a question by possibly looking up
information on the web.
—-
Task1: [Anachronisms] An anachronism is a mistake in chronology, or a person, thing, or event that is out of its proper time.
Does the sentence contain an anachrornism? Input: Kurt Cobain starred in the 1980 television show "Twin Peaks". The ﬁnal
answer is "Yes".
Task2: [Known Unknown] Choose the option that best answers the question. If the question does not have a known answer,
choose "Unknown". Input: Where was Mark Twain born? The ﬁnal answer is Florida, Missouri.
Are these similar? Yes. They both require searching information about entities mentioned in the text, like Kurt Cobain or
Mark Twain.
—-
Task1: [Hindu Knowledge] Answer questions about Hindu mythology by choosing the option that best answers the question.
Input: In the Mahabharata, Karna is cursed to forget the incantations needed to use which weapon? Choices: Anjalikastra,
Narayanastra, Agneyastra, Brahmastra. The ﬁnal answer is Brahmastra.
Task2: [Code Debugging] Debug the following code snippets by ﬁnding the answer or the error message. Input:
if x < 5:
pass
The ﬁnal answer is
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
NameError: name ’x’ is not deﬁned
Are these similar? No. Task 1 is about asnswering a question and requires searching information about entities mentioned in
the text. Task 2 is a question about debugging code and may require a Python interpreter.
Task 1: %s
Task 2: %s
Are these similar?
Table 13: Programs in the task library.
.
grammar = parsimonious .grammar.Grammar(
r"""
program = program_start *node*partial_command *final_answer
program_start = input_start~r" ( |\n)"text~r" \n"
input_start = ~r"Input:"
text = ~r" (?<=Input:( |\n))(.|\n|\t)*?(?=\nQ[0-9]+:)"
node = command_node~r" \n"output_node~r" \n"
command_node = command_start~r" ( |\n)"command_instruction
output_node = begin_answer~r" ( |\n)"output
command_instruction = ~r" (?<=\]( |\n))(.|\n|\t)*?(?=\n\#[0-9]+)"
command_start = ~r"Q [0-9]+: \[[A-Za-z_ ]+\]"
begin_answer = ~r" \#[0-9]+:"
output = ~r" (?<=\#[0-9]+:( |\n))(.|\n|\t)*?(?=\nQ[0-9]+:)"
partial_command = command_start~r" \n"
final_answer = ~r"Ans: ( |\n)(.|\n)*$"
""")Figure 4: PeG Grammar used to parse ART programs
sub-task’s answer sequence “ #i 1 ::::” (or
the input if i= 1 ), and the sequence gener-
ated by the LM after the sub-task query symbol
“Qi: [generate python code ]”. The ﬁrst argu-
ment is the code snippet that needs to be edited
and the second argument is a multi-line comment
in Python used as the instruction for editing/gen-
eration. To ensure that subsequent code execution
results in the generation of an answer string inde-
pendent of variable name, the edit instruction is to
print the required variable. For example, for the
auto debugging task in the task library, the follow-
ing program snippet:
Input:
‘‘‘
x = set([1, 1, 2, 3])
‘‘‘
What is the value of x after this
program executes?
Q1: [code edit] Edit the code to print
the value of x
is used to prompt Codex in edit mode as follows.
For code input:
x = set([1, 1, 2, 3])
For edit instruction:
Edit the code to print the value of x
Code Execution We run python code in a virtual
python environment with arithmetic, symbolic, and
scientiﬁc computing packages pre-installed. The
arguments to code execute include the previous sub-
task’s answer sequence “ #i 1 ::::", which is the
python code snippet that requires executing. If i=
1, the input contains the code. The other argument
is the sequence generated by the LM after the sub-
task query symbol “ Qi: [executecode ]" (which isfrom sympy.solvers import solve
from sympy import Symbol, Eq, simplify
import math
import numpy as np
import cvxpy as cp
import statistics
def solve_it(equation, variable):
solution=solve(equation, variable, dict=True)
if not solution:
if isinstance(variable, list):
solution=v: None for v in variable
else:
solution=variable: None
return solution
else:
solution = solution[0]
return solution
Table 14: Code preﬁx appended before a code snippet
prior to execution.
simply to the code snippet as a comment). Again, to
encourage executable code with consistent variable
usage, we also append the sequence "Store your
ﬁnal answer is variable ’ans’" to the comment. The
results of the execution call are used to replace the
answer sequence generated by the language model.
Finally, we prepend a code snippet consisting of
useful module and function imports so that function
calls external modules like numpy and scipy are
executed successfully. This code preﬁx is shown in
Table 14. We use the exec native python function to
execute the code snippet and access the ’ans’ local
variable if it exists.
Knowledge Base lookup This tool is added in
the Word Unscrambling task. This function call is
used to look up data by keys in a relational knowl-
edge base. For example, we use dictionary lookup
for the Word Unscrambling task. The input to this
function is again the previous sub-task’s answer se-
quence (if it exists, or the original input is used) and
the sequence generated by the LM after the func-
tion name symbol. The ﬁrst argument is parsed as
a python code snippet and interpreted as a list of
lookup keys. The second argument is parsed as a
code generation prompt which is consequently exe-
cuted. For example, the ﬁrst argument of l = [’yob’,
’boy’, ’oyb’] and the second argument Check which
of these list of words is a word in English. Store
the ﬁnal answer is ’ans’ and print it. results in the
following code snippet and ﬁnal answer ’boy’ :
def lookup(word_list):
import enchant
d = enchant.Dict("en_US")
valid_list = []
for word in word_list:
if d.check(word):
valid_list.append(word)
return valid_list
While this is a restricted deﬁnition for a general
knowledge base lookup or query, we explore how
human-in-the-loop feedback can be used to create
custom lookup tools.
Prolog Engine This tool is added in the formal
fallacies task. This task consits of ﬁrst-order logic
statements stated in natural language, as follows:
To begin with, Bonnie is a schoolmate of
Miranda. Moreover, whoever is a
workmate of Aubrey is not a schoolmate
of Miranda. All this entails that Bonnie
is not a workmate of Aubrey.
Is the argument, given the explicitly
stated premises, deductively valid or
invalid?
This can be written in Prolog5as:
workmate(X, aubrey) :- \+ schoolmate(X,
miranda).
schoolmate(bonnie, miranda).
?- workmate(bonnie, aubrey).
Humans provide feedback by authoring such pro-
log statements for a few instances with a new tool
symbol “[translate to prolog]”. They then author a
new tool that calls a python prolog parsing engine
to execute the prolog code and determine the bi-
nary value of the ﬁnal expression. This is integrated
back into the program.
5https://en.wikipedia.org/wiki/PrologA.4 Baselines
Few-shot baseline This is the direct prompting
baseline where the prompt consists of input-output
pairs only and no additional intermediate reason-
ing steps. Following prior work that reports re-
sults with direct prompting (Suzgun et al., 2022;
Wei et al., 2022), we use 3 randomly chosen
input-output instances. We run direct prompting
for both, InstructGPT (text-davinci-002) (Ouyang
et al., 2022) and Codex (code-davinci-002) (Chen
et al., 2021) and report the higher performance.
This follows from (Chung et al., 2022), where they
ﬁnd that Codex models are better at analytical tasks
than text models, even with direct prompting.
Auto CoT A baseline that generates automatic
CoT-style multi-step reasoning in a free-form nat-
ural language (as done in AutoCoT (Zhang et al.,
2022)). A randomly selected subset of examples in
the dataset is used to prompt the LLM to elicit CoT-
style reasoning ( Input + Let’s think step-by-step. ).
Since CoT-style generation is free-form and pars-
ing potential tool use symbols is harder, we don’t
use tools for this baseline. This baseline speciﬁ-
cally measures the effectiveness of a custom query
language (and PeG grammar) we use to write pro-
grams and parse tool calls; While (Zhang et al.,
2022) cluster training examples to provide diverse
demonstrations to the LLM, we choose a random
selection of 5 examples. A careful selection of
demonstration examples may also be used for ART,
and we leave an exploration of this choice to future
work. We parse the generated CoT-style reason-
ing to extract the answer string and add the phrase
“The ﬁnal answer i” along with the answer string to
the end of the reasoning. This pattern is used for
evaluation.
Best GPT-3 Approaches We brieﬂy describe the
GPT-3 best results reported in Tables 2 and Ta-
bles 3, which correspond to the best GPT-3 results
reported in approaches that use multi-step reason-
ing (like CoT) and tool use, with human supervi-
sion for both.
•(Suzgun et al., 2022): Human-authored CoT
reasoning for several tasks in BigBench.
A closer inspection of their hand-crafted
prompts revealed that they cast BigBench
tasks to multiple-choice tasks (selecting be-
tween options A,B,C,...), which differs from
the more challenging format proposed origi-
nally and used in this work. Hence, we modify
Task CoT ART Human feedback
+Human + Human
Kth letter concat* 0.64 59.40 40.0 100.0 Code C: k’th letter extraction and merge for a list of words
Language Games* 18.58 26.08 23.08 35.38 Code C: Eng->Pig Latin and vice-versa
Anachronisms* 51.48 49.82 75.66 82.91 C: search query constrained to extract time-periods
Auto Debugging* 38.24 61.18 62.94 67.05 Code C: Code edit ﬁxed to print variable asked in input
A: “[generate answer] What is the ﬁnal error message?”
Navigate 61.7 85.9 72.4 80.89 Code C: correct forward, backward, right, left distances
Date Understanding 38.9 70.4 52.05 65.45 A: First ﬁnd what date is today
Formal Fallacies 56.4 56.4 64.76 74.39 T: Translate to Prolog and add prolog engine
Table 15: Improving ART and free-form CoT via self-consistency and human-in-the-loop feedback on library tasks.
(*) indicates that human-in-the-loop improvement was done over automatically generated CoT reasoning for these
tasks.
their prompt to generate answers in the output
space, which is a fair comparison with ART.
Hence, the results reported in Table 2 and Ta-
ble 3 may differ from the results reported in
(Suzgun et al., 2022)
•(Khot et al., 2022): Decomposed prompting
for list reveral and kth letter comaparison.
•(Press et al., 2022): Decomposed multi-step
QA for musique.
•(Chen et al., 2022): Program-of-though:
Promtps are designed to convert Arithmetic
problems into code and execute them.
• (Ouyang et al., 2022): InstructGPT
•(Chung et al., 2022): PaLM language model.
Note that we only consider the highest re-
ported text-davinci-002 or code-davinci-002
results from this paper to compare with ART.
• (Chen et al., 2021): Codex
A.5 Human Feedback
We present feedback provided by humans for ﬁve
BigBench tasks.
•Table 16 for reasoning about colored objects,
where code is edited introduces a consistent
data structure to represent the input.
•Table 17 for physics, where substeps is intro-
duced that prompt for given physical quanti-
ties, required quantity and a physics formula
that connects them together.
•Table 18 for temporal sequences, where sub-
steps are introduced that go over all options
one by one to determine whether the subject
in the input is busy with an activity at those
times.
•Table 19 for tracking shufﬂed objects, where
code is edited introduces a consistent data
structure to represent the input and swap logic
is corrected.•Table 20 for unscrambling english words,
where code is edited in step 2 to get correct
permutations and a new tool is deﬁned in step
3 to lookup the english dictionary and return
permutations that are actual words.
In Table 15, we present human feedback results for
training tasks in the task library.
Reasoning about colored objects
Before
Input: On the nightstand, there are two silver textbooks and three silver cat toys. If I remove all the cat toys from the
nightstand, how many fuchsia things remain on it?
Q1: [generate python code] write down the arithmetic or algebra equations as python code, storing the answer as ’ans’
1:
num_of_silver_textbooks = 2
num_of_silver_cat_toys = 3
num_of_fuchsia_things = num_of_silver_textbooks
num_of_things_on_nightstand = num_of_silver_textbooks + num_of_silver_cat_toys
ans = num_of_things_on_nightstand - num_of_silver_cat_toys
print(ans)
Q2: [code execute] Execute the python code and get the value of "ans"
2: 2
Q3: [EOQ]
Ans: 2
After
Input: On the nightstand, there are two silver textbooks and three silver cat toys. If I remove all the cat toys from the
nightstand, how many fuchsia things remain on it?
Q1: [generate python code] Write python code to ﬁnd if I remove all the cat toys from the nightstand, how many fuchsia
things remain on it
#1:
color_object_count = [("silver", "textbooks", 2), ("silver", "cat toys", 3)]
removed_objects = [("silver", "cat toys", 3)]
def remove_objects(color_object_count, removed_objects):
color_object_count_updated = []
for color, object, count in color_object_count:
for color2, object2, count2 in removed_objects:
if color == color2 and object == object2:
count -= count2
color_object_count_updated.append([color, object, count])
return color_object_count_updated
color_object_count = remove_objects(color_object_count, removed_objects)
ans = 0
for color, object, count in color_object_count:
if color == "fuchsia":
ans = count
print(ans)
Q2: [code execute] Execute the python snippet.
#2: 0
Q3: [EOQ]
Ans: 0
Table 16: Human feedback: Reasoning about colored objects
Physics
Before
Input: Identify the physics formula that would be most useful for ﬁnding the answer to each of the following word problems.A
particle of mass 6.8 * 10 ^ -5 g and charge 58 mC moves in a region of space where the electric ﬁeld is uniform and is 7.2
N/C in the x direction and zero in the y and z direction. If the initial velocity of the particle is given by vy = 1.7 * 10 ^ 6 m/s,
vx = vz = 0, what is the speed of the particle at 0.7 s?
choice: F = m * a
choice: v = dx / dt
choice: E = q / ( * A * cos( ))
choice: a = dv / dt
Q1: [subquestion] Can this question be answered step-by-step?
#1: Yes.
Q2: [search] A particle of mass 6.8 * 10 ^ -5 g and charge 58 mC moves in a region of space where the electric ﬁeld is
uniform and is 7.2 N/C in the x direction and zero in the y and z direction. If the initial velocity of the particle is given by vy
= 1.7 * 10 ^ 6 m/s, vx = vz = 0, what is the speed of the particle at 0.7 s?
#2: v = dx / dt A particle of mass 6.6 ×10-5 g and charge 22 mC moves in a region of space where the electric ﬁeld is uniform
and is 5.6 N/C in the x direction and zero ...
Q3: [compare] Which option is the answer in #2 most similar to?
#3: v = dx / dt
Q4: [EOQ]
Ans: v = dx / dt
After
Input: Identify the physics formula that would be most useful for ﬁnding the answer to each of the following word problems.A
particle of mass 6.8 * 10 ^ -5 g and charge 58 mC moves in a region of space where the electric ﬁeld is uniform and is 7.2
N/C in the x direction and zero in the y and z direction. If the initial velocity of the particle is given by vy = 1.7 * 10 ^ 6 m/s,
vx = vz = 0, what is the speed of the particle at 0.7 s?
choice: F = m * a
choice: v = dx / dt
choice: E = q / ( * A * cos( ))
choice: a = dv / dt
Q1: [subquestion] What quantity is being asked for in "A particle of mass 6.8 * 10 ^ -5 g and charge 58 mC moves in a region
of space where the electric ﬁeld is uniform and is 7.2 N/C in the x direction and zero in the y and z direction. If the initial
velocity of the particle is given by vy = 1.7 * 10 ^ 6 m/s, vx = vz = 0, what is the speed of the particle at 0.7 s"
#1: speed of the particle
Q2: [subquestion] What other information is given in "A particle of mass 6.8 * 10 ^ -5 g and charge 58 mC moves in a region
of space where the electric ﬁeld is uniform and is 7.2 N/C in the x direction and zero in the y and z direction. If the initial
velocity of the particle is given by vy = 1.7 * 10 ^ 6 m/s, vx = vz = 0, what is the speed of the particle at 0.7 s"
#2: mass of particle (6.8 * 10 ^ -5 g), charge(58 mC), electric ﬁeld (7.2 N/C), initial velocity (vy = 1.7 * 10 ^ 6 m/s, vx = vz =
0)
Q3: [search] Given mass of the particle, charge and initial velocity of particle, and electric ﬁeld, what can we compute for the
particle? Give the physics formula.
#3:
We can compute acceleration of the particle, as follows:
a = (q/m) * E
This formula is derived from the Lorentz force law, which states that the force (F) on a particle of charge q in an electric ﬁeld
E is given by the equation:
F = ma = qE
Q4: [subquestion] What is the physics formula in the choices that is closest to F = ma = qE, a = (q/m) * E
#4: F = m * a
Q5: [EOQ]
Ans: F = m * a
Table 17: Human feedback: Physics
Temporal sequences
Before
Input: Today, Tiffany went to the swimming pool. Between what times could they have gone?
We know that:
Tiffany woke up at 6am.
Jessica saw Tiffany waiting at the airport from 6am to 10am.
William saw Tiffany attending class at the school from 12pm to 5pm.
David saw Tiffany walking towards the Statue of Liberty from 5pm to 6pm.
James saw Tiffany reading at the library from 6pm to 8pm.
The swimming pool was closed after 8pm.
Between what times could Tiffany have gone to the swimming pool?
choice: 12pm to 5pm
choice: 6pm to 8pm
choice: 10am to 12pm
choice: 6am to 10am
Q1: [think step-by-step]
#1: Let’s think step-by-step. Tiffany went to the swimming pool today. We need to ﬁnd out when she could have gone.
- Tiffany woke up at 6am.
- Jessica saw Tiffany waiting at the airport from 6am to 10am.
- William saw Tiffany attending class at the school from 12pm to 5pm.
- David saw Tiffany walking towards the Statue of Liberty from 5pm to 6pm.
- James saw Tiffany reading at the library from 6pm to 8pm.
- The swimming pool was closed after 8pm.
The swimming pool was closed after 8pm, so Tiffany couldn’t have gone at 6pm to 8pm. She was reading at the library from
6pm to 8pm, so she couldn’t have gone at 10am to 12pm. She was waiting at the airport from 6am to 10am, so she couldn’t
have gone at 6am to 10am. That leaves 12pm to 5pm. So the answer is 12pm to 5pm.
Q2: [EOQ]
Ans: 12pm to 5pm
After
Q1: [subquestion] Is Tiffany free 12pm to 5pm?
#1: No, she is attending classes at the school.
Q2: [subquestion] Is Tiffany free 6pm to 8pm?
#2: No, she is reading at the library.
Q3: [subquestion] Is Tiffany free 10am to 12pm?
#3: Yes, there is no other activity in her scehdule.
Q4: [subquestion] Is Tiffany free 6am to 10am?
#4: No, she is waiting at the airport.
Q5: [subquestion] Is the swimming pool open from 10am to 12pm?
#5: Yes, the swimming pool is open from 10am to 8pm.
Q6: [get answer] In conclusion, when should she go to the swimming pool?
#6: Tiffany should go to the swimming pool from 10am to 12pm.
Q7: [EOQ]
Ans: 10am to 12pm
Table 18: Human feedback: Temporal sequences
Tracking shufﬂed objects
Before
Input: Alice, Bob, Claire, Dave, and Eve are playing a game. At the start of the game, they are each holding a ball: Alice has
a pink ball, Bob has a white ball, Claire has a red ball, Dave has a purple ball, and Eve has a yellow ball.
As the game progresses, pairs of players trade balls. First, Alice and Dave swap balls. Then, Claire and Eve swap balls. Then,
Alice and Bob swap balls. Then, Dave and Claire swap balls. Finally, Alice and Claire swap balls. At the end of the game,
Claire has the
Q1: [generate python code] write down the arithmetic or algebra equations as python code, storing the answer as ’ans’
#1:
Alice = "pink"
Bob = "white"
Claire = "red"
Dave = "purple"
Eve = "yellow"
Alice_and_Dave = [Alice, Dave]
Claire_and_Eve = [Claire, Eve]
Alice_and_Bob = [Alice, Bob]
Dave_and_Claire = [Dave, Claire]
Alice_and_Claire = [Alice, Claire]
ans = Claire
print(ans)
Q2: [code execute] Execute the python code and get the value of "ans"
#2: red
Q3: [EOQ]
Ans: red
After
Q1: [generate python code] write down the arithmetic or algebra equations as python code, storing the answer as ’ans’
#1:
players = [’Alice’, ’Bob’, ’Claire’, ’Dave’, ’Eve’]
balls = [’pink’, ’white’, ’red’, ’purple’, ’yellow’]
swap_list = [[0, 3], [2, 4], [0, 1], [3, 2], [0, 2]]
for swap in swap_list:
balls[swap[0]], balls[swap[1]] = balls[swap[1]], balls[swap[0]]
ans = [players[i] + ’ has a ’ + balls[i] for i in range(5)]
print(ans)
Q2: [code execute] Execute the python code and get the value of "ans"
#2: [’Alice has a pink’, ’Bob has a purple’, ’Claire has a white’, ’Dave has a yellow’, ’Eve has a red’]
Q3: [subquestion] At the end, what color does Claire have?
#3: white
Q3: [EOQ]
Ans: white
Table 19: Human feedback: Tracking shufﬂed objects
Tracking shufﬂed objects
Before
Input: The word illoctnecos is a scrambled version of the English word
Q1: [string split] What are the letters in "illoctnecos"?
#1: ["i", "l", "l", "o", "c", "t", "n", "e", "c", "o", "s"]
Q2: [string permutation] What are the possible permutations of the letters in #1?
#2: ["i", "l", "l", "o", "c", "t", "n", "e", "c", "o", "s"], ["i", "l", "l", "o", "c", "t", "n", "e", "o", "c", "s"], ["i", "l", "l", "o", "c", "t",
"e", "n", "c", "o", "s"], ["i", "l", "l", "o", "c", "t", "e", "n", "o", "c", "s"], ["i", "l", "l", "o", "c", "n", "t", "e", "c", "o", "s"], ...
Q3: [string check] Among the permutations in #2, which one is an English word?
#3: ["i", "l", "l", "o", "c", "t", "n", "e", "c", "o", "s"]
Q4: [EOQ]
Ans: ["i", "l", "l", "o", "c", "t", "n", "e", "c", "o", "s"]
Word Unscrambling: After
Q1: [string split] What are the letters in "illoctnecos"?
#1: ["i", "l", "l", "o", "c", "t", "n", "e", "c", "o", "s"]
Q2: [string permutation] What are the possible permutations of the letters in #1?
#2: ["illoctnecos", "illoctneocs", "illoctenocs", "illoctencos", "illotnencos", ...]
Q3: [word lookup] Among the permutations in #2, which one is an English word?
#3: collections
Q4: [EOQ]
Ans: collections
Table 20: Human feedback: Word Unscrambling
