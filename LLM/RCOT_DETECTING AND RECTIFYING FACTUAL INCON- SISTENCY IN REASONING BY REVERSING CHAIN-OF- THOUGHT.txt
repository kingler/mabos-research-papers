Preprint
RCOT: D ETECTING AND RECTIFYING FACTUAL INCON -
SISTENCY IN REASONING BY REVERSING CHAIN -OF-
THOUGHT
Tianci Xue1∗, Ziqi Wang2, Zhenhailong Wang2, Chi Han2, Pengfei Yu2, Heng Ji2
1Department of Software, Nanjing University
2Department of Computer Science, University of Illinois Urbana-Champaign
xuetianci@smail.nju.edu.cn
{ziqiw9, wangz3, chihan3, pengfei4, hengji}@illinois.edu
ABSTRACT
Large language Models (LLMs) have achieved promising performance on arith-
metic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompt-
ing. However, LLMs face challenges in maintaining factual consistency during
reasoning, exhibiting tendencies to condition overlooking, question misinterpre-
tation, and condition hallucination over given problems. Existing methods use
coarse-grained feedback (e.g., whether the answer is correct) to improve factual
consistency. In this work, we propose RCOT(Reversing Chain- of-Thought), a
novel method to improve LLMs’ reasoning abilities by automatically detecting
and rectifying factual inconsistency in LLMs’ generated solutions. To detect fac-
tual inconsistency, RCOTfirst asks LLMs to reconstruct the problem based on
generated solutions. Then fine-grained comparisons between the original problem
and the reconstructed problem expose the factual inconsistency in the original
solutions. To rectify the solution, RCoT formulates detected factual inconsistency
into fine-grained feedback to guide LLMs in revising solutions. Experimental
results demonstrate improvements of RCoT over standard CoT, Self-Consistency
and Self-Refine across seven arithmetic datasets. Moreover, we find that manually
written fine-grained feedback can dramatically improve LLMs’ reasoning abilities
(e.g., ChatGPT reaches 94.6%accuracy on GSM8K), encouraging the community
to further explore the fine-grained feedback generation methods.
1 I NTRODUCTION
Large language models (LLMs) (Brown et al., 2020a; Zhang et al., 2022a; Narang & Chowdhery,
2022; Touvron et al., 2023) have showcased strong reasoning capabilities using chain-of-thought
(CoT) (Wei et al., 2023; Chowdhery et al., 2022; Fung et al., 2022), where LLMs are prompted to
generate intermediate steps before the final answer. Despite the impressive performance of CoT
prompting across various reasoning tasks (Dua et al., 2019; Miao et al., 2020; Cobbe et al., 2021b;
Yu et al., 2020; Bhagavatula et al., 2019; Talmor et al., 2019), LLMs still struggle to maintain factual
consistency in reasoning. Specifically, each reasoning problem usually consists of several conditions
and a question, and LLMs exhibit tendencies to hallucinate, overlook conditions and misinterpret
questions (Golovneva et al., 2022).
While previous research has proposed various methods to enhance Chain-of-Thought perfor-
mance (Zhang et al., 2022b; Fu et al., 2022; Diao et al., 2023; Shum et al., 2023; Zhou et al.,
2023; Wang et al., 2023; Gao et al., 2023; Chen et al., 2022; Weng et al., 2023; Paul et al., 2023;
Shinn et al., 2023), there remains a noticeable absence of explicit studies addressing the issue of
factual inconsistency. The most relevant work is probably Self-Verification (Weng et al., 2023), which
verifies answers by swapping conditions and answers. However, it can only tell whether answers are
correct and fail to give fine-grained feedback on factual inconsistency to guide LLMs in revising
solutions. Figure 1 shows an instance of factual inconsistent solutions generated by ChatGPT, where
∗Work done during the internship at UIUC.
1arXiv:2305.11499v2  [cs.CL]  2 Oct 2023
Preprint
the critical condition of "2 days away" is mistakenly overlooked. Despite the incorrect solution,
ChatGPT could be rectified if we explicitly point out that it overlooked the "tomorrow, 10/16/1924."
condition. Motivated by this observation, we introduce Reverse Chain-of-Thought (RCoT), a novel
method for detecting and rectifying condition hallucination (Figure 3), overlooking (Figure 1)and
question misinterpretation (Figure 2) in CoT. More examples could be found in Appendix B.1. The
schematics of our proposed method are illustrated in Figure 4. To detect factual inconsistency,
RCoT begins with reconstructing new problems from the original solutions. Next, RCoT detects
inconsistencies by conducting a fine-grained comparison between the reconstructed problems and
the original problems. To rectify inconsistencies, the detected inconsistencies will be formulated as
fine-grained feedback to guide the LLMs in revising their solutions. Fine-grained feedback can not
only be used to revise solutions but can also offer more interpretability of reasoning errors, allowing
humans to inspect solutions easier.
Problem: Today’s meeting is rescheduled to 11 am tomorrow, 10/16/1924. What is the date one year
ago from today?
ChatGPT Chain-of-Thought:
The current date is 10/16/1924. To find the date one year ago from today, you would subtract one year
from the current year, which would be 1923. The correct answer is 10/16/1923. ✗
Figure 1: A example of factual inconsistency (condition overlooking).
Problem: The cave is 1218 feet deep and they are already at 849 feet. If they are travelling at
speed of 17 How much farther until they reach the end of the cave?
ChatGPT Chain-of-Thought:
First, we need to find out how much deeper the cave is from their current position ...... So they still need
to descend 369 feet to reach the end of the cave. Next, we need to figure out how long it will take them
to descend that distance at a speed of 17 feet per unit of time ...... Therefore, it will take them 22 units
of time to reach the end of the cave. ✗
Figure 2: An example showing that LLMs may misinterpret questions. Green: The correct solution of
the problem. Red: The superfluous answer given by LLMs. Misinterpreting: LLMs have correctly
answered the question in the problem, but it continues answering "how long", resulting in a wrong
answer. This is a typical misinterpretation of the problem.
Problem: A wooden bridge can carry no more than 5000 pounds. A delivery truck filled with
identical boxes, each weighing 15 pounds, will pass over the bridge. The combined weight of
the driver and the empty truck is 3755 pounds. What is the maximum number of boxes which
can be loaded onto the truck while not exceeding the bridge’s weight limit?
ChatGPT Chain-of-Thought:
First, we need to subtract the weight of the driver and the empty truck from the maximum weight the
bridge can carry ...... Let’s assume each box weighs 16 pounds (15 pounds for the contents and 1 pound
for the box). ...... This means we can load a maximum of 77 boxes onto the truck without exceeding the
bridge’s weight limit. ✗
Figure 3: An example showing that LLMs hallucinate conditions. Red: Hallucinated conditions.
Hallucinating: The problem mentions that each box weighs 15 pounds. However, LLMs assume
each box weighs 16 pounds, which contradicts real conditions.
We evaluate RCOTon seven arithmetic reasoning datasets, including GSM8k (Cobbe et al., 2021a),
AQuA (Ling et al., 2017), SV AMP (Patel et al., 2021), AddSub (Hosseini et al., 2014), ASDiv
(Miao et al., 2021), Date (Srivastava et al., 2022) and SingelEq (Koncel-Kedziorski et al., 2016).
Experimental results demonstrate the effectiveness of RCOT, outperforming competitive baselines in
both zero-shot and few-shot settings. In-depth analysis and human evaluation suggest that fine-grained
feedback on factual inconsistency is crucial for LLMs to revise solutions for arithmetic problems.
For example, ChatGPT could achieve 94.6% accuracy on GSM8k with manually written fine-grained
2
Preprint
feedback. Moreover, we conduct comprehensive ablation studies examining the impact of individual
modules. Our findings encourage the community to further explore detecting and rectifying factual
inconsistency to enhance LLMs’ reasoning ability.
Our contributions are summarized as follows:
•We propose a novel prompting method, Reversing Chain-of-Thought ( RCOT) to effectively
detect and rectify the factual inconsistency of LLMs in arithmetic reasoning, focusing on
overlooked, hallucinated conditions and misinterpreted questions. RCoT demonstrates
improvement over competitive baseline models across seven arithmetic reasoning tasks.
•Prompting with fine-grained feedback on factual inconsistency shows encouraging results on
improving LLMs’ reasoning abilities. Though automatically generated feedback by RCoT
shows consistent improvement compared with standard CoT, we find that human-written
ground-truth feedback can further improve the LLMs’ reasoning ability (e.g., ChatGPT
reaches 94.6% accuracy on GSM8k). The gap between RCoT’s feedback and human-
written feedback encourages the community to further explore the automatic generation of
fine-grained feedback.
•RCoT offers more interpretability to the reasoning errors with fine-grained feedback on
factual inconsistency, allowing humans to inspect solutions easier.
2 R ELATED WORK
Language Model for Reasoning Reasoning ability is a critical skill to solve complex problems,
such as arithmetic reasoning (Koncel-Kedziorski et al., 2016; Roy & Roth, 2016; Miao et al., 2020;
Cobbe et al., 2021b; Dua et al., 2019), logical reasoning (Yu et al., 2020), commonsense reasoning
(Bhagavatula et al., 2019; Talmor et al., 2019; Zellers et al., 2018; Ye & Durrett, 2022) , and tabular
reasoning (Zhu et al., 2021). Recently, Large Language Models (e.g., GPT3 (Brown et al., 2020b),
ChatGPT, PaLM (Narang & Chowdhery, 2022) and LLaMA (Touvron et al., 2023)) have demonstrated
promising reasoning capability with Chain-of-Thought methods. However, large language models
exhibit tendencies to generate intermediate steps that are factually inconsistent, rendering them
incapable of solving complex problems requiring multi-step reasoning. In this work, we focus on
the detection and rectification of factually inconsistent errors in the intermediate reasoning steps,
including question misinterpretation, condition hallucination and condition overlooking.
Prompt Engineering Some prompting methods can elicit useful knowledge in large language
models to better solve complex tasks, two representative examples of which are In-context Learning
(Brown et al., 2020b) and Chain-of-Thought (Wei et al., 2023). In-Context Learning encourages the
language models to learn from a few input-output examples as prompts (Liu et al., 2022; Rubin et al.,
2022; Min et al., 2022). Chain-of-Thought prompting improves reasoning performance by prompting
LLMs to think of intermediate steps. Inspired by the promising performance of CoT, many methods
have explored how to further improve standard CoT. Least-to-most (Zhou et al., 2023) prompting
proposes to decompose a complex problem into a series of subproblems and solve them sequentially.
Self-Consistency prompting (Wang et al., 2023) improves performance through majority voting on
multiple solutions. Similarly, Complex CoT (Fu et al., 2022) emphasizes the importance of prompt
complexity and selects the most complex examples as prompts. Auto-CoT (Zhang et al., 2022b) is
proposed to reduce the workload of manual labeling. Active Prompting (Diao et al., 2023) selects
the most uncertain questions as demonstration examples to further improve performance. However,
these methods fail to address the factual inconsistency problem. Probably the most relevant work are
Self-Verification (Wang et al., 2023), REFINER (Paul et al., 2023), and Reflexion (Shinn et al., 2023).
These approaches focus on correcting LLMs outputs. However, Self-Verification can only generate
binary feedback and fail to get fine-grained feedback, REFINER needs externally trained models, and
Reflexion requires environmental feedback, which cannot be easily obtained in arithmetic reasoning.
Compared to these methods, RCoT entirely relies on the LLM itself to generate fine-grained feedback
on factual consistency.
Reverse Engineering. RCOTis inspired by the concept of Reverse Engineering, which has various
applications in machine learning research. (Fredrikson et al., 2014) proposes a reverse method
for linear models to evaluate models’ privacy safety. (Fredrikson et al., 2015) introduces a model
3
Preprint
Few-shot CoT
Zero -shot CoT
Q: Mary is an avid gardener. Yesterday, she 
received 18 new potted plants from her favorite 
plant nursery. She already has 2 potted plants on 
each of the 40 window ledges of her large … 
How many potted plants will Mary remain with?
A: Let’s think step by step.OR
Large Language Model
ReconstructionLarge Language Model

Original Solution
A: First,we need to find out how many potted 
plants Mary currently has.She has 2 potted plants 
on each of the 40 window ledges, soshe has a 
total of 2 x 40 = 80 potted plants. Next,we need to 
subtract the number of potted plants she will 
give … will remain with 80 -40 = 40 potted 
plants after giving away 1 potted plant from each 
of the 40 window ledges.Instruction: Give the concrete prompt (problem)…Decomposition
Original Problem:
Mary is an avid gardener. 
Yesterday, she received 18 
new potted plants from her 
favorite plant nursery.
potted plants on each of the 
40 window ledges …Reconstructed Problem:
Mary has 40 window ledges 
in her apartment, and she 
has 2 potted plants on each 
ledge. {Overlooked 
Condition} She has decided 
to give away 1 potted plant 
from ... Instruction: Please list the conditions of the …Output
Large Language Model
Original Conditions: Reconstructed Conditions:
Condition 1
Condition 2
she received 18 
new potted 
plants from her 
favorite plant 
nursery .…
Condition nCondition 1
Condition 2…
Condition mComparisonRevision
Question QuestionInstruction: Here are the mistakes and reasons in 
your answer to the problem…
Overlooked conditions prompt
You have ignored some real conditions:
1:she received 18 new potted plants from her 
favorite plant nursery.
Here are detailed reasons:
1: There is no information about whether or not 
Mary received any new potted plants from a 
nursery. Therefore, the real condition of Mary 
receiving 18 new potted plants cannot be logically 
derived or deduced from your thinking.
Hallucinated conditions prompt
Misinterpreted question  prompt
Large Language Model
A:Thank you for pointing out my mistakes. 
You are correct that I overlooked the fact that 
Mary received 18 new potted plants from the 
nursery.To correct my answer: First,we need to 
add the 18 new potted plants to the 80 potted 
plants Mary already has, giving us a total of 
98 potted  …so she will give away a total of 
40 potted plants.Therefore , Mary will remain 
with 98 -40 = 58 potted plants after giving 
away 1 potted plant from each of the 40 
window ledges.Revised Solution
Q: Will buys 15 oranges. When he gets home …
A: 1. Will buys 15 oranges…… The answer is 3
A: First, we need to convert…… The answer is 72
Q: Ashley has an internet connection speed …
Output
√Instruction: Can Condition2 be deduced 
from the reconstructed condition list?Condition 3NO
3: Mary received 
18 new potted 
plants from her 
favorite plant 
nursery.Original Condition:
1: Mary has 40 window ledges 
in her apartment.
2: She has 2 potted plants on 
each ledge.
3: She has decided to give away 
1 potted plant from each ledge.Reconstructed  Condition 
List:
Can this candidate 
condition be deduced 
from the condition list?NO
1: Mary received 18 new potted 
plants from her favorite plant nursery.
2: She already has 2 potted plants on 
each of the 40 window ledges of her 
large country home.
3: She will give 1 potted plant from 
each ledge to friends and family 
tomorrow.Original Condition List:
Can this candidate 
condition be deduced 
from the condition list?
Reconstructed ConditionYES
3: She has decided 
to give away 1 
potted plant from 
each ledge.
Figure 4: The framework of RCoT. (1) Reconstruction: Ask LLMs to reconstruct the problem
according to the original solution with instruction and demonstration examples. (2) Decomposition:
Decomposing the original problem and reconstructed problem into fine-grained condition lists.
(3)Comparison: Compare both lists of sub-conditions and questions to verify whether there are
hallucinations, overlookings and misinterpretations.(4) Revision: Gathering all factual inconsistencies
into fine-grained feedback to instruct LLMs to revise solutions.
inversion method for shallow neural networks, which can reconstruct the face information. (Geva
et al., 2022) unveils the internal prediction construction process of Transformer-based language
models by reverse engineering the operations of the feed-forward network (FFN) layers. Inverting
model hyperparameters is another application of reverse engineering techniques. (Bhagavatula et al.,
2019) reverses network parameters by repeatedly requesting the predicted label from the target model.
(Tramèr et al., 2016) develops an avatar method to estimate training data and model architectures,
while (Oh et al., 2019) trains a set of white-box models to estimate model hyperparameters. (Hua
et al., 2018) estimates both the structure and the weights of a CNN model on a hardware accelerator
from information leaks of memory access patterns. Different from their goal of opening up the
black-box of deep learning models, our work focuses on automatically detecting and rectifying
factual inconsistencies that appeared in the solutions generated by LLMs.
3 R EVERSING CHAIN -OF-THOUGHT (RC OT)
We propose RCoT for detecting and rectifying factual inconsistency (i.e., condition hallucinations,
overlookings, and question misinterpretation) in CoT to enhance LLMs’ reasoning ability. Specifically,
given a complex reasoning problem Qand original solution cgenerated by the LLM, we first ask
LLMs to detect factual inconsistency: (i) Problem Reconstruction: Reconstruct the problem
ˆQbased on the generated solution c. (ii) Fine-grained Comparison: Conduct a fine-grained
4
Preprint
comparison between the original problem Qand the reconstructed problem ˆQto detect condition
hallucinations, overlookings, and question misinterpretation. Then we rectify LLMs using detected
factual inconsistency: (iii) Fine-grained Feedback and Revision: The fine-grained comparison
reveals the factual inconsistency in original solutions. The detected factual inconsistencies are
formulated into fine-grained feedback to guide LLMs in revising their solution accordingly. The
overall schematic illustrations of our proposed approach are illustrated in Figure 4, and an example
of RCoT is shown in Appendix B.3.
3.1 P ROBLEM RECONSTRUCTION
Intuitively, if the generated step-by-step solution of an arithmetic problem is logically and factually
correct and complete, it is more likely for a human to infer what is the original problem. Similarly,
we ask the LLM to reconstruct the problem to get ˆQbased on its own solution c, in order to verify
whether it truly understands the problem. We manually write instructions and in-context examples as
the reconstruction prompt. We find that the factual inconsistencies such as condition hallucinations
(e.g., the LLM uses conditions that are not mentioned in the problem Q),condition overlookings (e.g.,
the LLM overlooks some important conditions in the problem Q), and question misinterpretations
(e.g., the LLM misunderstand the question of Q) can be effectively exposed by comparing the
reconstructed problem ˆQwith the original problem Q(§ 3.2), as shown in Figures 11, 8, and 17 in
Appendix B.1, respectively. The prompt template can be found in Figure 23.
3.2 F INE-GRAINED COMPARISON
To detect condition hallucinations and overlookings, as well as question misinterpretations in the
solution cfrom the reconstructed problem ˆQ, a naive approach is to ask the LLM to directly compare
QwithˆQ. However, such comparisons usually fail to produce high-quality detection results (Figure 5),
which is unsurprising because QandˆQcontain rich information, and the coarse-grained comparison
will inevitably ignore some vital information, causing a sub-optimal result. Therefore, we use fine-
grained step-by-step comparisons to improve the detection quality. All prompt templates are shown
in Figure 23. The process is as follows:
Problem Decomposition .QandˆQare unstructured texts, which are hard to be compared in an
organized manner. To overcome this issue, we ask the LLM to decompose the problem into a list
of conditions LQ= [L1
Q,···, Lm
Q], LˆQ= [L1
ˆQ,···, Ln
ˆQ]. The structured condition list will then be
used in fine-grained comparison.
Condition Comparison To find the differences between QandˆQ, we first check whether their
condition lists LQandLˆQare the same. Specifically, the LLM is required to answer whether each
Li
Qcan be inferred from LˆQ. IfLi
Qcannot be inferred from LˆQ, then Li
Qis either (1) overlooked in
the solution or (2) hallucinated by the LLM as a different condition. Similarly, we ask the LLM to
tell whether Lj
ˆQcan be inferred from LQfor every j. IfLj
ˆQcannot be inferred from LQ, then Lj
ˆQis
hallucinated. Apparently, we need to conduct comparisons for nmtimes in total.
Question Comparison The LLM sometimes will also misinterpret the question (Figure 2). Therefore,
we also ask LLM to compare the questions being asked in QandˆQ. If LLMs find the two questions
are different, then LLMs misinterpret the question in their solutions. This comparison only needs to
be done once since there is one question per problem in most cases.
After these comparisons, we detect hallucinated conditions, overlooked conditions, and misinterpreted
of questions. We then use them to formulate our fine-grained feedback to guide the LLM in revising
its solution.
3.3 F INE-GRAINED FEEDBACK AND REVISION
We assume the original solution is correct if we do not detect any factual inconsistency through
fine-grained comparison. On the contrary, we formulate fine-grained feedback to guide the LLM in
revising its solution if any factual inconsistency is detected. Specifically, the fine-grained feedback
will first state that the solution is incorrect, then list the detected factual inconsistency, and finally ask
5
Preprint
the LLM to revise its solution. Figure 23 shows the template we use to formulate the feedback. We
take the answer of the revised solution as the final output for evaluation.
4 E XPERIMENT
Our extensive experiments aim to show that (1) RCoT benefits arithmetic reasoning by automatically
detecting and rectifying condition hallucination and overlooking, and question misinterpretation;
(2) fine-grained feedback of factual consistency is critical for LLMs to self-revise the solution. (3)
fine-grained comparison is essential for constructing high-quality fine-grained feedback.
4.1 E XPERIMENT SETTING
We used closed-source ChatGPT and open-source LLaMA-13B-Chat (Touvron et al., 2023) as the
backbone LLMs for solution generation and set the temperature to 0 to improve reproducibility. We
evaluate RCoT on seven arithmetic datasets with different difficulties, including GSM8k (Cobbe
et al., 2021a), AQuA (Ling et al., 2017), SV AMP (Patel et al., 2021), AddSub (Hosseini et al., 2014),
ASDiv (Miao et al., 2021), Date (Srivastava et al., 2022) and SingelEq (Koncel-Kedziorski et al.,
2016). Due to the high time cost of API calls, we do not use the whole test set but randomly sample
test sub-sets. To reduce the randomness caused by test set sampling and make our results more
convincing, we sample three test sub-sets that each contains 256 inputs. We report the average
accuracy with deviation on the three test sub-sets. For the dataset that has less than 256 test inputs,
we still evaluate three times since ChatGPT’s outputs may change and report the average accuracy
with deviation. A detailed description of each dataset is shown in Appendix B.5.
We consider both zero-shot and few-shot settings. For the zero-shot setting, we add the prompt "Let’s
think step by step" to encourage LLMs to think intermediate steps but without any demonstration
examples (Kojima et al., 2023). For the few-shot setting, we use four-shot CoT prompts that consist
of problems, solutions, and final answers.
We compare our method with five baselines: (1) Chain-of-thought (CoT) (Wei et al., 2023) (2)
Active-Prompting (Diao et al., 2023), a method that selects the most uncertainty problems as
demonstration examples. (3) Double-Check asks LLMs to check their answers but does not point
out whether the answer is correct. In our experiment, we use the prompt "You should double-check
your answers". (4) Self-Consistency (SC) (Wang et al., 2023) through majority voting on multiple
solutions to improve the performance. (5) Self-Refine (Madaan et al., 2023) uses iterative feedback
and refinement to revise the answer. We use Tiktoken from Openai to calculate the cost of average
tokens.1
4.2 RC OTBENEFITS ARITHMETIC REASONING
Table 1 shows the results of RCoT on seven arithmetic datasets. Our method consistently outperforms
the standard CoT and the double-check methods in the zero-shot setting. Moreover, LLMs benefit
more from our method on more challenging tasks that require complex reasoning. For example, the
AQuA dataset contains diverse problems, and the Date dataset requires multi-hop reasoning and
common sense date knowledge. Both ChatGPT and LLaMA achieve lower accuracy scores on AQuA
and Date (51.3% and 66.7% for ChatGPT and 27.2% and 52.4% for LLaMA) among all seven tasks.
Meanwhile, we observe that our method helps LLMs improve by apparent margins on AQuA and
Date (4.1%, 5.0% and 4.7%, 2.9% for ChatGPT and LLaMA), the highest gains in all seven tasks.
Our method also remains effective for easier tasks. For example, RCoT enhances the performance of
the SV AMP dataset, which contains problems that usually only need one-step calculation, by 2.8%
and 2.5%. Moreover, we also observe greater improvements from our method on ChatGPT than
LLaMA, potentially due to the stronger abilities of ChatGPT to detect and correct errors.
We can observe similar results in the few-shot setting to the zero-shot setting. Although selecting the
most uncertain problems for LLMs as demonstration examples is helpful for reasoning (Diao et al.,
2023), RCoT still improves the accuracy. It is worth noting that the performance of Double-Check
method in the few-shot CoT setting decreases immensely. On the AQuA and GSM8K datasets, its
1https://github.com/openai/tiktoken
6
Preprint
Table 1: Average accuracy and standard deviation on seven arithmetic reasoning datasets. Bold
denotes the best result. Green: The performance improvement compared with Standard CoT and
Active-Prompting in Zero-shot and Few-shot settings, respectively. * denotes the LLM that uses
Manual-CoT. -denotes that Active-Prompting (Diao et al., 2023) does not support the dataset in their
source codes.
Model MethodArithmetic
GSM8K AQuA AddSub Date SingleEq ASDiv SV AMP
UL2-20B∗Standard 4.4 23.6 18.2 14.4 20.2 16.9 12.5
LaMDA-137B∗Standard 14.3 20.6 51.9 26.8 58.7 46.6 37.5
Text-davinci-002∗Standard 46.9 24.8 81.3 52.1 86.6 71.3 68.9
Zero-shot CoT
ChatGPTStandard 79.0±0.95 51.3±0.6 85.2±1.2 66.7±1.4 90.3±0.6 84.3±0.4 76.7±4.1
+Double-Check 79.3±2.1 42.7±0.6 85.6±1.2 60.5±6.5 88.8±0.8 82.8±1.4 77.6±2.0
+RCoT 82.0±0.3 55.5±0.8 87.1±1.1 71.7±1.3 91.4±0.8 86.0±0.3 79.6±4.1
(+3.1±0.6) +(4 .1±0.2) +(1 .8±0.1) +(5 .0±0.4) +(1 .1±0.2) +(1 .7±0.3) +(2 .8±0.2)
LLaMA-13B-ChatStandard 36.9±0.8 27.2±0.0 66.7±0.5 52.4±1.5 62.6±2.6 52.2±3.7 38.6±1.1
+Double-Check 35.6±1.1 24.8±0.0 62.0±0.7 27.0±0.9 62.1±3.2 53.2±3.6 41.1±0.2
+RCoT 39.8±0.8 31.9±0.0 67.4±0.5 55.3±2.0 63.5±2.1 53.0±3.7 41.1±0.8
(+2.9±0.4) +(4 .7±0.0) +(0 .7±0.3) +(2 .9±1.0) +(0 .9±0.5) +(0 .8±0.0) +(2 .5±0.4)
Few-shot CoT
ChatGPTActive-Prompting 81.8±0.6 53.3±0.6 87.2±1.2 - 91.7±0.4 87.9±0.8 82.5±0.6
+Double-Check 77.8±0.7 26.3±0.5 86.0±1.6 - 91.5±0.2 85.7±2.4 82.2±0.8
+RCoT 84.6±0.6 57.1±0.3 88.2±1.5 - 93.0±0.8 89.3±0.5 84.9±1.3
(+2.7±0.1) +(3 .7±0.9) +(1 .0±0.4) - +(1.2±0.4) +(1 .4±0.5) +(2 .3±1.0)
LLaMA-13B-ChatActive-Prompting 37.9±0.6 29.1±0.0 68.4±0.7 - 67.9±2.2 53.3±0.6 49.4±0.4
+Double-Check 36.2±0.1 23.2±0.0 61.9±2.1 - 64.9±1.3 50.3±3.5 47.4±0.8
+RCoT 40.1±0.4 30.7±0.0 68.8±0.9 - 68.1±2.3 53.6±0.4 51.2±0.3
(+2.1±0.3) +(1 .6±0.0) +(0 .4±0.3) - +(0.2±0.1) +(0 .3±0.2) +(1 .8±0.2)
performance drops by 27.0% and 4.0%, suggesting that few-shot examples may increase the risk
of revising correct solutions to the incorrect ones. LLaMA exhibits a lower degree of susceptibility
compared to ChatGPT.
We also compared RCoT with other stronger baselines (i.e., Self-Consistency, SC for short, and Self-
Refine). Specifically, We conducted 30 trials per problem for SC and 3 trials per problem for RCoT
in the zero-shot setting (set temperature to 0.7 Wang et al. (2023)), which uses similar costs. Due to
the extremely high cost, we do not experiment with the few-shot setting and leave it as our future
work. We set max attempt to 5 for Self-Refine. Table 4 has shown the results. RCoT could achieve
comparable performance to SC at nearly one-third of the cost (e.g., AddSub, SingleEq, SV AMP)
and even outperforms SC on the GSM8K dataset. However, the performance significantly drops on
AQuA and Date datasets. That is because there are multiple-choice tasks, making it exceedingly
simple for the model to approximate the answer by employing multiple guesses with incorrect logical
steps. Combining RCoT with SC, our method can further improve the performance and surpass all
baselines, reaching a high accuracy of 84.5% across seven arithmetic datasets. Our experiments
demonstrate the same conclusion as Madaan et al. (2023) that Self-Refine is not good at arithmetic
reasoning. It’s worth noting that Self-Refine achieves the highest accuracy on SingleEq and AddSub
datasets. Nevertheless, the improvement does not come from refinement but the usage of code in the
Self-Refine implementation, reducing a large number of calculation errors. The real improvements
brought by refinement are actually 0.8 and 0.4 in the AddSub and SingleEq datasets, respectively.
Another phenomenon is that self-refine does not bring more token cost even if we give it more
refinement budget. This is because self-refine tends to state that the solution is correct after the second
refinement.
7
Preprint
Table 2: The performance of RCoT using fine-
grained feedback and coarse-grained feedback.
w/o reasons : remove explanations of specific
mistakes from the original fine-grained feedback.
The prompt becomes "Your answer is wrong.
You should double-check your answer.". w/o
judgment+reasons: further remove the high-
level judgment. The prompt becomes "You
should double-check your answer." Red: The per-
formance drops compared with RCoT method.
Method GSM8K AQUA SV AMP
Standard CoT 79.0 51.3 76.7
RCoT(ours) 82.0 55.5 79.6
- w/o reasons 80.0 (-2.0) 52.3 (-3.2) 78.9 (-0.7)
- w/o judgment+reasons 79.3 (-2.7) 42.7 (-12.8) 77.6 (-2.0)Table 3: The performance without question com-
parison and condition comparison, as well as
the performance with coarse-grained comparison.
coarse-grained: We directly ask LLMs to com-
pare the original problem with the reconstructed
problem. Results show that (1) fine-grained com-
parison is important to get fine-grained feedback,
and (2) both question comparison and condi-
tion comparison are important in the fine-grained
comparison. Red: The performance drops com-
pared with the RCoT method.
Method GSM8K AQUA SV AMP
Standard CoT 79.0 51.3 76.7
RCoT 82.0 55.5 79.6
- w/o question comparison 80.9 (-1.1) 54.6 (-0.9) 79.2 (-0.4)
- w/o condition comparison 80.1 (-1.9) 53.5 (-2.0) 78.1 (-1.5)
RCoT (Corase-grained) 74.2 (-7.8) 49.6 (-5.9) 76.1 (-3.5)
Table 4: Average accuracy on seven arithmetic reasoning datasets among Self-Consistency (Wang
et al., 2023), RCoT and Self-Refine (Madaan et al., 2023). Bold denotes the best result.
Method GSM8K AQuA AddSub Date SingleEq ASDiv SV AMP Avg Acc Avg Tokens
SC (30 trials per problem) 81.6 70.8 88.6 80.0 92.9 90.2 80.4 83.5 5615.0
RCoT (1 trial per problem) 82.0 56.3 87.2 71.9 92.4 86.3 79.7 79.4 1831.0
RCoT (3 trials per problem) 83.2 72.8 89.8 78.9 93.8 91.8 81.2 84.5 5453.3
Self-Refineattempt 0 79.1 45.2 90.6 51.3 97.6 83.5 75.2 74.7 190.2
attempt 1 80.7 49.2 91.4 52.7 98.0 84.3 76.8 76.1 3108.4
attempt 2 80.7 49.2 91.4 52.7 98.0 84.3 76.8 76.1 3324.9
attempt 3 80.7 49.2 91.4 52.7 98.0 84.3 76.8 76.1 3359.6
attempt 4 80.7 49.2 91.4 52.7 98.0 84.3 76.8 76.1 3367.7
attempt 5 80.7 49.2 91.4 52.7 98.0 84.3 76.8 76.1 3367.7
4.3 F INE-GRAINED FEEDBACK IS CRITICAL FOR SOLUTION REVISION
The success of our method comes from fine-grained feedback that points out detailed factual incon-
sistency (condition hallucination and overlooking, and question misinterpretation). In this section,
we show that coarse-grained feedback will lead to worse performance to prove the necessity of fine-
grained feedback. We replace our fine-grained feedback with two kinds of coarse-grained feedback:
(1) w/o reasons: we do not tell LLMs the detected factual inconsistency by RCoT and only give
a high-level judgment. Therefore, if RCoT detects no factual inconsistency, we take the original
solution as the final output for evaluation. Otherwise, we use the prompt "Your answer is wrong. You
should double-check your answer" to guide LLMs in revising solutions. (2) w/o judgment+reasons
(i.e., Double-Check): We further remove the high-level judgment from the prompts. Therefore, we
always use "You should double-check your answer" to guide LLMs in revising solutions regardless
of the detection results of RCoT. Table 2 shows the results on SV AMP(easy), GSM8K(medium),
and AQuA (hard) datasets. We can see consistent performance drops when we remove detected
factual inconsistency and only keep a high-level judgment, showing the effectiveness of fine-grained
feedback. Moreover, we can observe that further removing judgment will make the performance even
worse than standard CoT. This is not surprising because LLMs may mistakenly revise the correct
solution to the incorrect one. Appendix B.6 shows an example of RCoT and Double-Check, where
we can see that RCoT could help the LLM correct the solution, but Double-Check couldn’t.
To further show the power of fine-grained feedback, we conduct a human evaluation. Specifically,
we go through the generated solution and write fine-grained feedback on factual inconsistency by
ourselves. Amazingly, the LLM could reach 94.6%accuracy on the GSM8K dataset and could
only reach 86.3%if we remove explanations of factual inconsistencies from our feedback (i.e., the
same setting as "w/o reasons" in Table 2). Appendix B.2 shows examples of manually written and
RCoT-generated feedback. This result shows the same observation of Table 2 and reveals the strong
8
Preprint
power of fine-grained feedback. Since RCoT still has a gap ( 12.6%accuracy gap) compared with
humans, we encourage the community to further explore fine-grained feedback generation.
4.4 F INE-GRAINED COMPARISON LEADS TO FINE -GRAINED FEEDBACK
To get fine-grained feedback, conditions and questions are compared in a fine-grained manner in
RCoT. A simpler way is to ask LLMs to generate fine-grained feedback directly by comparing
the original problems and reconstructed problems. Appendix A has illustrated that LLMs can
reconstruct questions well when the reasoning maintains factual consistency but not vice versa. Table
4 shows that coarse-grained comparison will cause a significant accuracy drop (even worse than
standard CoT), which suggests that it fails to generate high-quality feedback (Figure 5). Therefore,
problem decomposition and fine-grained comparison are essential. We also show that both condition
comparison and question comparison are important. Removing either of them will lead to worse
performance. This is because LLMs may hallucinate/overlook conditions (Figure 3, 1) as well as
misinterpret questions (Figure 2).
Original Problem: Rebecca wants to split a collection of eggs into groups of 3. Rebecca has
99 bananas 9 eggs and 27 marbles. How many groups will be created?
Reconstructed Problem: Rebecca has 9 eggs 99 bananas and 27 marbles. If we can group
items into sets of 3 how many groups can we make in total?
Coarse-Grained Comparison:
Yes, the reconstructed problem does not add new conditions or lack any conditions that were present in
original problem. Original problem and reconstructed problem essentially ask the same question, with
the only difference being the way the information is presented. Both problems ask how many groups of
3 items can be made from a collection of 9 eggs, 99 bananas, and 27 marbles. Therefore, reconstructed
problem does not introduce any new conditions or leave out any conditions from the original problem. ✗
Fine-Grained Comparison:
No,the candidate condition "Group items into sets of 3" cannot be deduced from the given condition
list as it is not applicable to all items in the condition list. The condition only applies to the eggs that
Rebecca wants to split into groups of 3. It does not apply to the bananas or the marbles.
Additionally, the candidate condition implies that all items must be grouped into sets of 3, but there is
no indication in the context that Rebecca wants to group all items into sets of 3. She only wants to split
the eggs into groups of 3. ✔
Figure 5: This is an example showing the failure of the coarse-grained comparison. Red: The
inconsistent condition between the original problem and the reconstructed problem.
4.5 A DDITIONAL QUANTITATIVE ANALYSIS OF RCOT
Table 5: Found/Not Found: RCoT can or cannot
find the reasons for errors. Other errors: such as
computation error, logical error and so on.
Type Found Not Found total
Overlooking 5 1 6
Hallucinating 16 15 31
Misinterpreting 5 3 8
Other errors 0 55 55To explore the effectiveness of RCoT, we con-
struct further quantitative analysis on 100 prob-
lems the ChatGPT answered incorrectly. We
manually divided these problems into four cat-
egories: condition hallucination, condition over-
looking, question misinterpretation and other
errors. The statistical results are shown in Ta-
ble 5. We find that RCoT is better at detecting
overlooking and misinterpretation errors than
hallucination errors.
5 C ONCLUSION
In this paper, we propose RCoT, a method that enables LLMs to detect and rectify factual inconsistency
automatically to improve LLMs’ reasoning abilities. RCoT detects factual inconsistency through
fine-grained comparison between the reconstructed problems and original questions, and then asks
LLMs to rectify inconsistencies through fine-grained feedback. Experimental results on seven
arithmetic reasoning datasets demonstrate the effectiveness of RCoT. Our experiments also show
9
Preprint
encouraging results of LLMs’ reasoning abilities with the help of manually written fine-grained
feedback, encouraging the community to further explore fine-grained feedback generation. RCoT
could, in principle, be applied to other tasks requiring CoT solutions. We discuss the limitations and
future work in Appendix C.
6 R EPRODUCIBILITY STATEMENT
The supplementary material includes the code for all experiments and their corresponding running
scripts. The dataset (GSM8K, AQuA, AddSub, SingleEq, Date, ASDiv and SV AMP) can be eas-
ily accessible on the HuggingFace website or from their official repositories. We explain all the
experimental details (temperature, dataset size and so on ) in Section 4.1.
REFERENCES
Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman,
Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. Abductive commonsense
reasoning. arXiv preprint arXiv:1908.05739 , 2019.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020a.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners, 2020b.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks, 2022.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems, 2021a.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve
math word problems. arXiv preprint arXiv:2110.14168 , 2021b.
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active prompting with chain-of-thought
for large language models, 2023.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence
information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on
computer and communications security , pp. 1322–1333, 2015.
Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart.
Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd
{USENIX }Security Symposium ( {USENIX }Security 14) , pp. 17–32, 2014.
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting
for multi-step reasoning. arXiv preprint arXiv:2210.00720 , 2022.
10
Preprint
Yi R. Fung, Tuhin Chakraborty, Owen Rambow, Smaranda Muresan, and Heng Ji. Normsage:
Multi-lingual multi-cultural norm discovery from conversations on-the-fly. In arxiv , 2022.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and
Graham Neubig. Pal: Program-aided language models, 2023.
Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers
build predictions by promoting concepts in the vocabulary space, 2022.
Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-
Zarandi, and Asli Celikyilmaz. Roscoe: A suite of metrics for scoring step-by-step reasoning,
2022.
Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to
solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pp. 523–533, Doha, Qatar,
October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL
https://aclanthology.org/D14-1058 .
Weizhe Hua, Zhiru Zhang, and G Edward Suh. Reverse engineering convolutional neural networks
through side-channel information leaks. In Proceedings of the 55th Annual Design Automation
Conference , pp. 1–6, 2018.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners, 2023.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS:
A math word problem repository. In Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies , pp.
1152–1157, San Diego, California, June 2016. Association for Computational Linguistics. doi:
10.18653/v1/N16-1136. URL https://aclanthology.org/N16-1136 .
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gen-
eration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pp. 158–167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015 .
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What
makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out
(DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning
Architectures , pp. 100–114, Dublin, Ireland and Online, May 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.
deelio-1.10 .
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,
Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative
refinement with self-feedback, 2023.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing
English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics , pp. 975–984, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.92. URL https://aclanthology.org/
2020.acl-main.92 .
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing
english math word problem solvers, 2021.
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in
context. In Proceedings of the 2022 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies , pp. 2791–2809, Seattle, United
States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.
201. URL https://aclanthology.org/2022.naacl-main.201 .
11
Preprint
Sharan Narang and Aakanksha Chowdhery. Pathways language model (palm): Scaling to 540 billion
parameters for breakthrough performance, 2022.
Seong Joon Oh, Bernt Schiele, and Mario Fritz. Towards reverse-engineering black-box neural
networks. Explainable AI: Interpreting, Explaining and Visualizing Deep Learning , pp. 121–144,
2019.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple
math word problems? In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies , pp. 2080–2094,
Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.
168. URL https://aclanthology.org/2021.naacl-main.168 .
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and
Boi Faltings. Refiner: Reasoning feedback on intermediate representations, 2023.
Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint
arXiv:1608.01413 , 2016.
Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context
learning, 2022.
Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic
memory and self-reflection, 2023.
KaShun Shum, Shizhe Diao, and Tong Zhang. Automatic prompt augmentation and selection with
chain-of-thought from labeled data, 2023.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska,
Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W.
Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda
Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders An-
dreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La,
Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna
Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes,
Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut
Erdem, Ayla Karaka¸ s, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski,
Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk
Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson,
Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta
Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian V oigt, Christopher D. Manning,
Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney
Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth,
Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk,
Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David
Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen,
Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang,
Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth
Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang,
Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice En-
gefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed,
Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard
de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo
Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin,
Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu
Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger,
Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James
Koppel, James Zheng, James Zou, Jan Koco ´n, Jana Thompson, Jared Kaplan, Jarema Radom,
Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Boss-
cher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming
12
Preprint
Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Berant,
Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B.
Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik
Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin
Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell,
Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-
Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt,
Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem ¸ Senel, Maarten Bosma, Maarten Sap,
Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco
Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha
Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna
Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu,
Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Sw˛ edrowski, Michele Bevilacqua,
Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mo Tiwari, Mohit Bansal,
Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Chi,
Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita
Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah
Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy,
Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul
Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon
Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei,
Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker,
Ramón Risco Delgado, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku
Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan
LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee,
Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam
Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoen-
holz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey,
Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan,
Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane
Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak
Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee,
Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Bider-
man, Stephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi,
Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu
Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang,
Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias
Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant
Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu,
Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout
V ossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh,
Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen,
Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang,
Zijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating
the capabilities of language models, 2022.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question
answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pp. 4149–4158, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https:
//aclanthology.org/N19-1421 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand
Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language
models, 2023.
Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart. Stealing machine
learning models via prediction {APIs}. In25th USENIX Security Symposium (USENIX Security
13
Preprint
16), pp. 601–618, 2016.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-
ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,
2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,
and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, and Jun Zhao. Large language
models are reasoners with self-verification, 2023.
Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning,
2022.
Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset
requiring logical reasoning. arXiv preprint arXiv:2002.04326 , 2020.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A large-scale adversarial
dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing , pp. 93–104, Brussels, Belgium, October-November
2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1009. URL https:
//aclanthology.org/D18-1009 .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language
models. ArXiv preprint , abs/2205.01068, 2022a. URL https://arxiv.org/abs/2205.
01068 .
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in
large language models, 2022b.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex
reasoning in large language models, 2023.
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng,
and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual
content in finance. arXiv preprint arXiv:2105.07624 , 2021.
14
Preprint
A T HE QUALITY OF RECONSTRUCTED PROBLEM
We measured the Rouge1, Rouge2, RougeL, RougeSum, and sentence embedding similarity ( using
sentence-transformers/all-mpnet-base-v2 ) between original problems and recon-
structed problems. We can observe from table ??that higher CoT accuracies correspond to higher
similarities between original problems and reconstructed problems. This is expected since cor-
rectly solved problems’ reconstruction should ideally be itself, and incorrectly solved problems’
reconstruction should differ from itself.
Table 6: The metrics between original problem and reconstructed problem
Dateset Rouge1 Rouge2 RougeL RougeSum Similarity Standard CoT Acc
GSM8K 71.4622 49.3915 58.8991 58.8974 93.57 79.0
AQuA 54.2383 33.5828 43.5771 43.7721 84.44 51.3
AddSub 78.0939 57.5594 66.3973 66.4099 94.05 85.2
Date 46.9414 28.7005 39.2268 39.3934 79.12 66.7
SingleEq 72.8212 53.5232 64.8562 64.7956 94.01 90.3
ASDiv 68.1849 46.5488 59.7907 59.7911 92.16 84.3
SV AMP 75.0074 55.3162 65.1539 65.1973 93.71 76.7
B E XAMPLES
B.1 E XAMPLES FOR DIFFERENT KINDS OF FACTUAL INCONSISTENCIES FOUND BY RCOT
Original Solution:
First, we need to find out how many potted plants Mary currently has. She has 2 potted plants on each
of the 40 window ledges, so she has a total of 2 x 40 = 80 potted plants.
Next, we need to subtract the number of potted plants she will give away. She has decided to give away
1 potted plant from each of the 40 window ledges, so she will give away a total of 1 x 40 = 40 potted
plants.
Therefore, Mary will remain with 80 - 40 = 40 potted plants after giving away 1 potted plant from each
of the 40 window ledges. ✗
Original Problem: Mary is an avid gardener. Yesterday, she received 18 new potted
plants from her favorite plant nursery. She already has 2 potted plants on each of the 40 window ledges
of her large country home. Feeling generous, she has decided that she will give 1 potted plant from
each ledge to friends and family tomorrow. How many potted plants will Mary remain with?
Reconstructed Problem: Mary has 40 window ledges in her apartment, and she has 2
potted plants on each ledge. She has decided to give away 1 potted plant from each ledge. How many
potted plants will Mary have left after giving away the plants?
Figure 6: A example for overlooking
15
Preprint
Original Solution:
First, we need to find the date that is one month before the deadline of June 1, 2021. To do this, we can
subtract one month from the deadline date.
June 1, 2021 - 1 month = May 1, 2021
So, the date that is one month before the deadline is May 1, 2021.
Next, we need to write this date in the format MM/DD/YYYY .
May 1, 2021 = 05/01/2021
Therefore, the answer is (C) 05/01/2021. ✗
Original Problem: The deadline is Jun 1, 2021, which is 2 days away from now. What is
the date a month ago in MM/DD/YYYY? Answer Choices: (A) 04/30/2021 (B) 02/19/2021 (C)
05/01/2021 (D) 05/09/2021 (E) 07/06/2021 (F) 08/03/2021
Reconstructed Problem: What is the date that is one month before the deadline of June
1, 2021, written in the format MM/DD/YYYY? Answer Choices: (A) 04/30/2021 (B) 02/19/2021 (C)
05/01/2021 (D) 05/09/2021 (E) 07/06/2021 (F) 08/03/2021
Figure 7: A example for overlooking
Original Solution:
Let’s first calculate how much money Tobias saved up in three months:He gets a $5 allowance per
month, so in three months he has saved up:
$5/month * 3 months = $15
n addition to his allowance, Tobias also earns money by mowing lawns andshoveling driveways.If he
mows 4 lawns, he earns:
4lawns*$15/lawn = $50
Let’s say he shovels x driveways. Then, he earns:
x driveways*$7/driveway = $7x
In total, he has saved up:
$15 (allowance) + $60 (lawn mowing) + $7x (shoveling) = $75 + $7x
He spent $95 on shoes, so he has:
$75 + $7x- $95 = $0
Simplifying this equation:
$7x - $20 = $0
$7x = $20
x= 2.85
Therefore, Tobias shoveled about 2.85 driveways. However since he can’t shovel afraction of a
driveway, we can round this up to 3 driveways. ✗
Original Problem: Tobias is buying a new pair of shoes that costs $95. He has been sav-
ing up his money each month for the past three months. He gets a $5 allowance a month. He also mows
lawns and shovels driveways. He charges $15 to mow a lawn and $7 to shovel. After buying the shoes,
he has $15 in change. If he mows 4 lawns, how many driveways did he shovel?
Reconstructed Problem: How many driveways did Tobias have to shovel to save enough
money to buy $95 shoes, if he gets a $5 allowance per month for three months, mows 4 lawns for $15
each, and earns $7 for every driveway he shovels?
Figure 8: A example for overlooking.
16
Preprint
Original Solution:
First, we need to find out how far John traveled during the first 3 hours at a speed of 60 mph. We can
use the formula distance = speed x time, so:
distance = 60 mph x 3 hours = 180 miles
Next, we need to find out how far John traveled during the next 2 hours in standstill traffic. Since he
didn’t move, the distance is 0 miles.
Then, we need to find out how far John traveled during the next half-hour at a speed of 30 mph. Using
the same formula, we get:
distance = 30 mph x 0.5 hours = 15 miles
Finally, we need to find out how far John traveled during the last 1.5 hours at a speed of 80 mph. Again,
using the same formula, we get:
distance = 80 mph x 1.5 hours = 120 miles
To find out how far John is from home at the end of those 4 hours, we need to add up all the distances
he traveled:
180 miles + 0 miles + 15 miles + 120 miles = 315 miles
Therefore, John is 315 miles away from home at the end of those 4 hours. ✗
Original Problem: John drives for 3 hours at a speed of 60 mph and then turns around
because he realizes he forgot something very important at home. He tries to get home in 4 hours but
spends the first 2 hours in standstill traffic. He spends the next half-hour driving at a speed of 30mph,
before being able to drive the remaining time of the 4 hours going at 80 mph. How far is he from home
at the end of those 4 hours?
Reconstructed Problem: John drove for 4 hours. During the first 3 hours, he drove at a
speed of 60 mph. During the next 2 hours, he was stuck in standstill traffic. During the next half-hour,
he drove at a speed of 30 mph. During the last 1.5 hours, he drove at a speed of 80 mph. How far is
John from home at the end of those 4 hours?
Figure 9: A example for overlooking
Original Solution:
First, we need to find out how many books and movies are left unread/watched.
Books left unread: 11 - 7 = 4
Movies left unwatched: 17 - 21 = -4
Uh oh, we have a problem. We can’t have negative movies left unwatched, so we need to adjust our
calculations.
Let’s assume that the person has actually watched 17 movies, but some of them were repeats. So, we
can say that they have watched 17 unique movies and some of them multiple times.
Now, let’s recalculate:
Books left unread: 11 - 7 = 4
Movies left unwatched: 17 - 17 = 0
So, the person has read 7 books and watched 17 unique movies. To find out how many more movies
than books they have watched, we can subtract:
17 - 7 = 10
Therefore, the person has watched 10 more movies than books they have read. ✗
Original Problem: There are 11 different books and 17 different movies in the ’ crazy
silly school ’ series. If you read 7 of the books and watched 21 of the movies How many more movies
than books have you read?
Reconstructed Problem: A person has 11 books and 21 movies. They have read/watched 7 books
and 17 movies. How many more movies than books have they watched?
Figure 10: A example for hallucination.
17
Preprint
Original Solution:
First, we know that the year is 1969, so we don’t need to worry about that.
Next, we know that it is currently 4/19, but we need to figure out what day it will be tomorrow.
To do this, we add 1 to the day, which gives us 4/20.
Therefore, the answer is (B) 04/20/1969. ✗
Original Problem: It is 4/19/1969 today. What is the date today in MM/DD/YYYY? Answer
Choices: (A) 04/18/1969 (B) 04/20/1969 (C) 04/12/1969 (D) 05/17/1969 (E) 04/19/1969 (F) 07/19/1969
Reconstructed Problem: What is the date tomorrow if today is April 19, 1969? Answer
Choices: (A) 04/18/1969 (B) 04/20/1969 (C) 04/12/1969 (D) 05/17/1969 (E) 04/19/1969 (F)
07/19/1969
Figure 11: A example for hallucination.
Original Solution:
First, we need to calculate the total number of petals on the flowers that Rose picked:
The 3 flowers with 5 petals each have a total of 3 x 5 = 15 petals.
The 4 flowers with 6 petals each have a total of 4 x 6 = 24 petals.
The 5 flowers with 4 petals each have a total of 5 x 4 = 20 petals.
The 6 flowers with 7 petals each have a total of 6 x 7 = 42 petals.
Therefore, the total number of petals on all the flowers that Rose picked is 15 + 24+ 20 + 42 = 101
petals.
Next, we need to subtract one petal from each type of flower that she dropped. Sowe need to subtract:
1 petal from the 3 flowers with 5 petals each, leaving 3 x 4 = 12 petals.
1 petal from the 4 flowers with 6 petals each, leaving 4 x 5 = 20 petals.
1 petal from the 5 flowers with 4 petals each, leaving 5 x 3 = 15 petals.
1 petal from the 6 flowers with 7 petals each, leaving 6 x 6 = 36 petals.
Therefore, the total number of petals on the flowers that Rose puts in the vase is 12+ 20 + 15 + 36 = 83
petals.✗
Original Problem: Rose is out picking flowers for a vase she wants to fill. She starts off
by picking 3 flowers with 5 petals each. She then picks 4 flowers with 6 petals each. She then adds
another 5 flowers with 4 petals each. Lastly she picks 6 flowers with 7 petals each. As she’s carrying
these flowers over to fill the vase, she drops 1 of each and the wind blows them away. She puts the
remnaining flowers in the vase.How many petals in total are on the flowers in the vase?
Reconstructed Problem: Rose went flower-picking and collected different types of flow-
ers with varying numbers of petals. She picks 3 flowers with 5 petals each, 4 flowers with 6 petals each,
5 flowers with 4 petals each and 6 flowers with 7 petals each. She later dropped one petal from each
type of flower and put the remaining flowers in a vase. What is the total number of petals on the flowers
that she put in the vase?
Figure 12: A example for hallucination
18
Preprint
Original Solution:
First, we need to find out how many bottle caps and wrappers Danny had before he found the ones at
the park. To do this, we can add the number he found at the park to the number he has now:
71 + 28 = 99 bottle caps
24 + 16 = 40 wrappers
Next, we need to find the difference between the number of bottle caps and wrappers:
99 - 40 = 59
Therefore, Danny has 59 more bottle caps than wrappers in his collection now. ✗
Original Problem: Danny collects bottle caps and wrappers. He found 71 bottle caps and
24 wrappers at the park. Now he has 28 bottle caps and 16 wrappers in his collection. How many more
bottle caps than wrappers does danny have now?
Reconstructed Problem: Danny found 28 bottle caps and 16 candy bar wrappers at the
park. He now has 71 bottle caps and 24 candy bar wrappers in his collection. How many more bottle
caps than candy bar wrappers does Danny have in his collection now?
Figure 13: A example for hallucination
Original Solution:
One week ago from today would be August 25th, 2021. Therefore, the answer is not (A) or (C) since
they both represent the current date. It is also not (B), (D), or (F) since they are not one week ago from
today. The correct answer is (E) 09/07/2021. ✗
Original Problem: It was Sept.1st, 2021 a week ago. What is the date one week ago
from today in MM/DD/YYYY? Answer Choices: (A) 09/01/2021 (B) 12/06/2021 (C) 09/02/2021 (D)
09/01/2020 (E) 09/07/2021 (F) 09/01/2098
Reconstructed Problem: What is the date that was one week ago from today’s date,
which is September 1st, 2021? Choose the correct answer from the following options: (A) 09/01/2021
(B) 12/06/2021 (C) 09/02/2021 (D) 09/01/2020 (E) 09/07/2021 (F) 09/01/2098
Figure 14: A example for hallucination
Original Solution:
First, we need to figure out what day was yesterday. To do that, we subtract one day from 11/23/1933,
which gives us 11/22/1933.
Next, we need to figure out what day is today. To do that, we subtract one more day from 11/22/1933,
which gives us 11/21/1933.
Therefore, the answer is (E) 08/29/1933. ✗
Original Problem: The day before yesterday was 11/23/1933. What is the date today in
MM/DD/YYYY? Answer Choices: (A) 01/04/1934 (B) 11/04/1933 (C) 11/25/1967 (D) 12/09/1933 (E)
08/29/1933 (F) 11/25/1933
Reconstructed Problem: If today is 11/23/1933, what is the date today? Choose the cor-
rect answer from the following options: (A) 01/04/1934 (B) 11/04/1933 (C) 11/25/1967 (D) 12/09/1933
(E) 08/29/1933 (F) 11/25/1933
Figure 15: A example for hallucinating
19
Preprint
Original Solution:
Martha has been collecting shells since she turned 5 years old, which means she has been col-
lecting shells for 5 years. In one year, Martha collects 12 shells (one per month). So in 5 years,
Martha collects 5 x 12 = 60 shells. By her 10th birthday, Martha will have collected 60 + 12 = 72 shells. ✗
Original Problem: Martha has been collecting shells since she turned 5 years old, every
month she collects one shell. By her 10th birthday, how many shells will Martha have collected?
Figure 16: A example for hallucinating
Original Solution:
First, we need to find the total number of houses in all 81 blocks. To do this, we can multiply the
number of houses in each block (12) by the number of blocks (81):
12 x 81 = 972
So there are 972 houses in total.
Next, we need to find out how many pieces of junk mail the mailman needs to give to each house. We
know that he has to give 4 pieces of junk mail to each house, so we can multiply this by the total
number of houses:
4 x 972 = 3,888
Therefore, the mailman needs to give 3,888 pieces of junk mail in each block. ✗
Original Problem: A mailman has to give 4 pieces of junk mail to each house in each of
the 81 blocks. If there are 12 houses in each block How many pieces of junk mail should he give in
each block?
Reconstructed Problem: There are 81 blocks in a neighborhood, and each block has 12
houses. The mailman needs to deliver 4 pieces of junk mail to each house. How many pieces of junk
mail does the mailman need to deliver in total?
Figure 17: A example for misinterpretation
20
Preprint
B.2 E XAMPLES THAT COMPARE RCOTFEEDBACK WITH HUMAN FEEDBACK
Original Problem: Kelly has 22 nintendo games. How many does she need to buy so
that she will have 140 games left?
Original Solution: To calculate how many Nintendo games Kelly needs to buy so that
she will have 140 games left, we need to subtract the number of games she currently has
from the desired total number of games. The desired total number of games is 140 + 22
= 162 games. Therefore, Kelly needs to buy 162 - 22 = 140 Nintendo games. Therefore,
the final answer is 140.
Reconstructed Problem: Kelly currently has 22 Nintendo games. She wants to have 140
more games than she currently has. How many Nintendo games does Kelly need to buy
to reach her goal?
RCoT
Fine-grained Feedback:
Here are the mistakes and reasons in your answer to the problem.
You use some wrong candidate conditions:
1: She wants to have 140 more games than she currently has.
They all can not be deduced from the true condition list.
Here are detailed reasons:
1: The true problem only provides information about how many games Kelly currently has and she
needs to reach a certain number of games. It does not provide any information that Kelly wants to have
140 more games than she currently has. Therefore, the candidate condition of "She wants to have 140
more games than she currently has" cannot be deduced from your thinking.
The real problem has the conditions:
1: Kelly has 22 nintendo games.
2: She wants to have 140 games left.
You should consider all real conditions in the problem.
Revised Answer:
Thank you for pointing out the mistakes in my previous answer. Here’s a corrected response:
To calculate how many Nintendo games Kelly needs to buy so that she will have 140 games left, we
need to subtract 140 from the total number of games she wants to have. The total number of games she
wants to have is the current number of games she has plus the number of games she wants to buy. So,
we can set up an equation:
22 + x = 140
where x is the number of games Kelly needs to buy.
Solving for x, we get:
x = 140 - 22 = 118
Therefore, Kelly needs to buy 118 Nintendo games so that she will have 140 games left. ✔
Reasons By Humans
Fine-grained Feedback:
Your answer is wrong. The problem mentions that she will have 140 games left, but it doesn’t mean that
she desires 140 more games. You misunderstand it.
Revised Answer:
I apologize for the mistake in my previous response. To calculate how many Nintendo games Kelly
needs to buy so that she will have 140 games left, we need to subtract 140 from the number of games
she currently has.
So, if Kelly currently has 22 Nintendo games and wants to have 140 games left, we can calculate the
number of games she needs to buy as follows:
140 (desired number of games left) - 22 (current number of games) = 118
Therefore, Kelly needs to buy 118 Nintendo games to have 140 games left. ✔
Figure 18: This example shows that feedback generated by RCoT sometimes has the same quality as
the human-written feedback.
21
Preprint
Original Problem: There are 11 different books and 17 different movies in the ’ crazy
silly school ’ series. If you read 7 of the books and watched 21 of the movies How many
more movies than books have you read?
Original Solution: First, we need to find out how many books and movies are left
unread/watched ...... Let’s assume that the person has actually watched 17 movies, but
some of them were repeats. So, we can say that they have watched 17 unique movies
and some of them multiple times. Therefore, the person has watched 10 more movies
than books they have read.
Reconstructed Problem: A person has 11 books and 21 movies. They have read/watched
7 books and 17 movies. How many more movies than books have they watched?
RCoT
Fine-grained Feedback:
Here are the mistakes and reasons in your answer to the problem.
You use some wrong candidate conditions:
1: The person has 11 books and 21 movies.
They all can not be deduced from the true condition list.
Here are detailed reasons:
1: The candidate condition states that the person has 11 books and 21 movies, but the condition list only
tells us that there are 11 books and 17 movies in the series, and that the person has read 7 of the books
and watched 21 of the movies. We cannot deduce that the person has all 11 books just because there are
11 books in the series. Therefore, the candidate condition cannot be deduced from the condition list.
The real problem has the conditions:
1: There are 11 different books and 17 different movies in the ’crazy silly school’ series.
2: You read 7 of the books and watched 21 of the movies.
You should consider all real conditions in the problem.
Revised Solution:
I apologize for the mistakes in my previous answer. Here is the corrected answer: The true problem has
the condition:
There are 11 different books and 17 different movies in the ’crazy silly school’ series.
To find out how many more movies than books you have read ...... Let’s assume that you have actually
watched 17 movies, but some of them were repeats. So, we can say that you have watched 17 unique
movies and some of them multiple times.
Therefore, you have watched 10 more movies than books you have read. ✗
Reasons By Humans
Fine-grained Feedback:
The books and movies in crazy silly school are not relevant to calculate how many more movies than
books you have read. You only need to consider the number of books and movies you have read/watched.
Revised Solution:
You are correct. I apologize for my mistake. To find out how many more movies than books you have
read, we simply need to subtract the number of books you have read from the number of movies you
have watched:
Number of movies watched - Number of books read = 21 - 7 = 14
Therefore, you have watched 14 more movies than books you have read. ✔
Figure 19: This example shows RCoT sometimes cannot perform as well as humans.
22
Preprint
B.3 A CONCRETE EXAMPLE THAT SHOWS THE WHOLE PROCESS OF RCOT
Original Problem: Lana had 72 extra nickels. If she put them into stacks with 8 in
each stack, how many stacks could she make?
Original Solution: To calculate how many stacks Lana could make with 72 extra
nickels, we need to divide the total number of nickels by the number of nickels in each
stack. Since there are 5 nickels in each stack, 72 extra nickels is equal to 72/5 = 14.4
stacks. However, Lana cannot have a fraction of a stack, so we need to round down
to the nearest whole number. Therefore, Lana could make 14 stacks with the 72 extra
nickels. Therefore, the final answer is 14. ✗
Step1: Problem Reconstruction
Input = [instruction, original solution]
Output = [reconstructed problem]
Instruction: Give the concrete prompt (problem) that can generate this answer. The prob-
lem should contain all basic and necessary information and correspond to the answer. The problem can
only ask for one result.
Reconstructed Problem: Lana has 72 extra nickels and wants to stack them in piles of 5. How many
stacks of nickels can she make?
Step2: Fine-grained Comparison
(i)Problem Decomposition:
Input = [instruction, original problem, reconstructed problem]
Output = [original condition list, reconstructed condition list]
Instruction: Please list the conditions of the problem. There may be multiple conditions......
Original condition list:
1: Lana had 72 extra nickels.
2: She put them into stacks with 8 in each stack.
Reconstructed condition list:
1: Lana has 72 extra nickels.
2: She wants to stack them in piles of 5.
(ii)Condition Comparison:
Input = [instruction, sub-condition, condition list]
Output = [yes or no, reason]
Overlooked conditions:
Instruction: Given a candidate condition: "She put them into stacks with 8 in each stack."
Here is a condition list:
"1: Lana has 72 extra nickels. 2: She wants to stack them in piles of 5."
From a mathematical point of view, can this candidate condition be deduced from the condition list?
Please illustrate your reason and answer "yes" or "no".
hallucinated conditions:
Instruction: Given a candidate condition: "She wants to stack them in piles of 5."
Here is a condition list:
"1: Lana had 72 extra nickels. 2: She put them into stacks with 8 in each stack."
From a mathematical point of view, can this candidate condition be deduced from the condition list?
Please illustrate your reason and answer "yes" or "no".
(iii)Question Comparison:
Input = [instruction, original problem, reconstructed problem]
Output = [yes or no, reason]
Q1: Lana had 72 extra nickels. If she put them into stacks with 8 in each stack, how many
stacks could she make?
Q2: Lana has 72 extra nickels and wants to stack them in piles of 5. How many stacks of nickels can
she make?
23
Preprint
Instruction: From a mathematical point of view, are these problems asking the same thing in the end?
Please illustrate your reason and answer "yes" or "no".
Step3: Fine-grained Feedback and Revision
Input = [instruction, original solution]
Output = [revised solution]
Instruction:
Here are the mistakes and reasons in your answer to the problem.
You have ignored some real conditions:
1: She put them into stacks with 8 in each stack.
You should consider all real conditions in the problem.
Here are detailed reasons:
1: There is no information about whether or not She put them into stacks with 8 in each stack. Therefore,
the real condition of ”She put them into stacks with 8 in each stack“ cannot be logically derived or
deduced from your thinking.
You use some wrong candidate conditions:
1: She wants to stack them in piles of 5.
They all can not be deduced from the true condition list.
You should consider all real conditions in the problem.
Here are detailed reasons:
1: The condition list only provides information about the number of extra nickels Lana has and she
wants to put 8 nickels in each stack. There is no mention of her preference or intention to stack them in
piles of 5.
Revised Solution:
I apologize for my mistake in the previous answer. Here is the corrected answer:
To calculate how many stacks Lana could make with 72 extra nickels, we need to divide the total
number of nickels by the number of nickels in each stack. Since there are 8 nickels in each stack, Lana
could make 72/8 = 9 stacks with the 72 extra nickels. Therefore, the final answer is 9. ✔
Figure 20: This is an example showing the whole process of RCoT. Red: The overlooked condition
in the original problem. Blue: The hallucinated condition in the original problem. Green: The correct
solution.
24
Preprint
B.4 A N EXAMPLE COMPARING RCOTAND DOUBLE -CHECK
Original Problem: At the trip to the county-level scavenger hunt competition 90 people
were required to split into groups for the competition to begin. To break people up
into smaller groups with different leaders 9-person groups were formed. If 3/5 of the
number of groups each had members bring back 2 seashells each how many seashells
did they bring? GSM8K
Original Solution: Since there were 90 people, and 9 people were in each group, there were 90/9
= 10 groups. 3/5 of those groups brought back seashells, which is 3/5 * 1 0 = 6 groups. Each group
brought back 2 seashells , so the total number of seashells brought back is 6 group s * 2 seashells per
group = 12 seashells. The ﬁnal answer is 12. 
Reconstructed Problem: There were 90 people on a beach trip, and they split up into groups of 9.
3/5 of the groups brought back seashells, and each group brought back 2 seashells . How many seashells
were brought back in total?
Double-Check
Coarse Feedback:
Y ou should double-check your answer.
Revised Solution:
I apologize for the mistake in my previous answer. Here’s the correct solution:
To form groups of 9 people, divide the total number of people (90) by 9:
90/9 = 10 groupsIf 3/5 of the groups brought back 2 seashells each, that means:
(3/5) x 10 group s = 6 groups
So, 6 groups brought back 2 seashells each:6 group s x 2 seashells/group = 12 seashells
Therefore, the correct answer is 12 seashells. 
RCoT
Fine-grained feedback:
Y ou used some wrong candidate conditions:
1: 3/5 of the groups brought back seashells, each group brought back 2 seashells.
They all can not be deduced from the true conditions.
Y ou should consider all real conditions in the problem.
Revised Solution:
Thank you for pointing out my mistake. Here is the corrected answer:
Given that 90 people were required to split into groups for the competition to begin and
9-person groups were formed, we can ﬁnd the total number of groups as:
90/9 = 10. So, there were 10 groups formed.
Now, we know that 3/5 of the groups had members bring back 2 seashells each. So, the number
of groups that brought back seashells is:3 / 5x1 0=6
Therefore, 6 groups brought back seashells.
Since each member brought back 2 seashells, the total number of seashells brought back is:
6 group s x 9 members per grou p x 2 seashells per member = 108 seashells
Therefore, they brought back a total of 108 seashells. 
Figure 21: This is an example that Double-Check fails to correct the solution, whereas RCoT
succeeded in correcting the solution. Green: The correct solution of the problem. Red: The wrong
intermediate step of the original solution. Brown: The factual inconsistencies found by RCoT.
25
Preprint
B.5 D ATASETS
Table 7: Examples of each reasoning task and detailed description of each dataset.
Dataset Answer Format Train Test Example
GSM8K Number 7473 1319 Joseph had 3 times as many notebooks as Martha.
Martha decided she needed more notebooks and
then bought 5 more for a total of 7 notebooks. How
many more than Joseph does she now have?
AQuA Multiple choice 97467 254 A man spends 70% of his income. If his income
increases by 20%, then what will be his new ex-
penditure? Answer Choices: (A) 58.3% (B) 62.5%
(C) 63.5% (D) 64.5% (E) 65.5%
AddSub Number - 395 Mary is baking a cake . The recipe wants 8 cups of
flour . She already put in 2 cups . How many cups
does she need to add ?
SV AMP Number - 1000 Bobby ate 28 pieces of candy. Then he ate 42
more. He also ate 63 pieces of chocolate. How
many pieces of candy did Bobby eat?
SingleEq Number - 508 There were 28 bales of hay in the barn. Tim stacked
more bales in the barn today. There are now 54
bales of hay in the barn. How many bales did he
store in the barn ?
ASDiv Number - 2096 The following week, they decided to go to Lake
Huron and Lake Michigan. During their stay there,
they caught a total of 30 pikes, 40 sturgeons and
75 herrings. How many fishes did they catch from
the two lakes?
Date Multiple Choices - 370 Today is 9/7. Jane is watching NFL 2003. What
is the date tomorrow in MM/DD/YYYY? An-
swer Choices: (A) 08/18/2003 (B) 09/08/1916
(C) 09/13/2003 (D) 09/15/2003 (E) 09/01/2003
(F) 09/08/2003
26
Preprint
B.6 A N EXAMPLE COMPARING RCOTAND DOUBLE -CHECK
Original Problem: At the trip to the county-level scavenger hunt competition 90 people
were required to split into groups for the competition to begin. To break people up
into smaller groups with different leaders 9-person groups were formed. If 3/5 of the
number of groups each had members bring back 2 seashells each how many seashells
did they bring? GSM8K
Original Solution: Since there were 90 people, and 9 people were in each group, there were 90/9
= 10 groups. 3/5 of those groups brought back seashells, which is 3/5 * 1 0 = 6 groups. Each group
brought back 2 seashells , so the total number of seashells brought back is 6 group s * 2 seashells per
group = 12 seashells. The ﬁnal answer is 12. 
Reconstructed Problem: There were 90 people on a beach trip, and they split up into groups of 9.
3/5 of the groups brought back seashells, and each group brought back 2 seashells . How many seashells
were brought back in total?
Double-Check
Coarse Feedback:
Y ou should double-check your answer.
Revised Solution:
I apologize for the mistake in my previous answer. Here’s the correct solution:
To form groups of 9 people, divide the total number of people (90) by 9:
90/9 = 10 groupsIf 3/5 of the groups brought back 2 seashells each, that means:
(3/5) x 10 group s = 6 groups
So, 6 groups brought back 2 seashells each:6 group s x 2 seashells/group = 12 seashells
Therefore, the correct answer is 12 seashells. 
RCoT
Fine-grained feedback:
Y ou used some wrong candidate conditions:
1: 3/5 of the groups brought back seashells, each group brought back 2 seashells.
They all can not be deduced from the true conditions.
Y ou should consider all real conditions in the problem.
Revised Solution:
Thank you for pointing out my mistake. Here is the corrected answer:
Given that 90 people were required to split into groups for the competition to begin and
9-person groups were formed, we can ﬁnd the total number of groups as:
90/9 = 10. So, there were 10 groups formed.
Now, we know that 3/5 of the groups had members bring back 2 seashells each. So, the number
of groups that brought back seashells is:3 / 5x1 0=6
Therefore, 6 groups brought back seashells.
Since each member brought back 2 seashells, the total number of seashells brought back is:
6 group s x 9 members per grou p x 2 seashells per member = 108 seashells
Therefore, they brought back a total of 108 seashells. 
Figure 22: This is an example that Double-Check fails to correct the solution, whereas RCoT
succeeded in correcting the solution. Green: The correct solution of the problem. Red: The wrong
intermediate step of the original solution. Brown: The factual inconsistencies found by RCoT.
B.7 T EMPLATE
Figure 23 shows the template prompts of RCoT.
C L IMITATIONS AND FUTURE WORK
RCoT can not detect all possible reasoning errors. For example, it is hard for RCoT to detect
computational errors. However, RCoT could be combined with other prompting techniques such as
27
Preprint
Figure 23: All prompts used in RCoT
Program-of-Thought (Chen et al., 2022), a method to reduce computational errors through disen-
tangling reasoning and computations. Besides, there is still a significant gap between revising the
solutions with RCoT-generated feedback and human feedback, which encourages further exploration
in the generation of fine-grained feedback with higher quality. RCoT requires multiple conversations
with LLMs (e.g., ChatGPT in our paper) and may thus slow down the inference speed due to the
low bandwidth of API calls. Nevertheless, a locally deployed model may alleviate such a problem.
28
Preprint
In the future, we will explore other applications of RCoT, such as logical reasoning and symbolic
reasoning.
29
